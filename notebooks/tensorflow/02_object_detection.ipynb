{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objekterkennung auf dem Boneage Datensatz\n",
    "\n",
    "Bei der Objekterkennung geht es darum, Objekte innerhalb eines Bildes zu finden und zu klassifizieren. Anders als bei der Segmentierung werden die Objekte dabei üblicherweise mit Bounding-Boxen umgeben, die alle Pixel des Objekts enthalten sollen, darüber hinaus aber auch Teile des Hintergrunds oder umgebender Objekte enthalten.\n",
    "\n",
    "<img src=\"https://lh6.googleusercontent.com/TLLJ9h4eOKM8loxc8SpKaVPaX0laf_dtsGr7BIWhf36PErnKFweZTGDnhKYsVJ68g9AAiEzyQ67iydfSvVg2TwoKvJxbmcutpPvWmW7yFB7v9CInUtX5r4QtelyaHJCqhGsgzysC\" />\n",
    "Quelle: https://www.clarifai.com/blog/classification-vs-detection-vs-segmentation-models-the-differences-between-them-and-how-each-impact-your-results\n",
    "\n",
    "\n",
    "In diesem Notebook word ein Modell zur Objekterkennung für den Boneage Datensatz mit [M-RCNN](https://github.com/matterport/Mask_RCNN) (welches wiederum Keras und Tensorflow als Backend nutzt) trainiert. In diesem Modell werden zwei getrennte Netze trainiert: eins für die grobe Idenfitikation von relevanten ROIs (Region Proposal Network, RPN) und ein weiteres zur Klassifikation bzw. genaueren Einteilung dieser ROIs.\n",
    "\n",
    "Der Code ist adaptiert von [train_shapes](https://github.com/matterport/Mask_RCNN/blob/master/samples/shapes/train_shapes.ipynb).\n",
    "\n",
    "## Datensatz\n",
    "\n",
    "Der [Boneage](http://rsnachallenges.cloudapp.net/competitions/4) Datensatz besteht aus 12.800 Hand-Röntgenaufnahmen von Kindern und Jugendlichen, die sich in 12.600 Trainings- und 200 Testbilder aufteilen. Ziel der RSNA-Challenge ist die Altersbestimmung des zugehörigen Patienten aufgrund dieser Röntgenbilder.\n",
    "\n",
    "Der hier verwendete Datensatz verwendet ein [Subset](https://doi.org/10.1371/journal.pone.0207496) ([GitHub](https://github.com/razorx89/rsna-boneage-ossification-roi-detection) mit 240 Trainings- und 89 Validierungsbildern für die _Regions Of Interest_ (ROI) von für die Altersbestimmung relevanter Gelenke:\n",
    "\n",
    "- distal interphalangeal joints (DIP, unten in grün)\n",
    "- proximal interphalangeal joints (PIP, unten in türkis)\n",
    "- metacarpophalangeal joints (MCP, unten in blass grün)\n",
    "- Handgelenk\n",
    "- Elle\n",
    "- Speiche\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/razorx89/rsna-boneage-ossification-roi-detection/master/example.png\"  width=\"600\" />\n",
    "\n",
    "Für weitere Informationen sei auf das Paper [Ossification area localization in pediatric hand radiographs using deep neural networks for object detection](https://doi.org/10.1371/journal.pone.0207496) von S. Koitka, A. Demircioglu, M.S. Kim, C.M. Friedrich, F. Nensa verwiesen.\n",
    "\n",
    "Das Ziel unseres Modells ist die Detektion dieser Gelenke und die automatische Bestimmung von entsprechenden ROIs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports und Setup\n",
    "\n",
    "Wie bei der Klassifikation werden hier benötigte Bibliotheken importiert, Funktionen definiert und Konstanten festgelegt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import imgaug\n",
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "\n",
    "\n",
    "# Directories\n",
    "MRCNN_DIR = os.path.abspath(\"/mrcnn\")\n",
    "ROOT_DIR = os.path.abspath(\"/workspace\")\n",
    "PRETRAINED_MODEL_DIR = os.path.abspath(\"/models\")\n",
    "DATA_DIR = os.path.abspath(\"/data\")\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"models\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(PRETRAINED_MODEL_DIR, \"mask_rcnn_coco.h5\")\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Tell TF to not use all GPU RAM\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=6*1024)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "print(\"Tensorflow setup done\")\n",
    "\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "\n",
    "\n",
    "import asyncio\n",
    "import concurrent\n",
    "asyncio.get_event_loop().set_default_executor(concurrent.futures.ThreadPoolExecutor(max_workers=6))\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(MRCNN_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "# Functions\n",
    "\n",
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax\n",
    "\n",
    "def filter_result(r, num_elements_per_class_id):\n",
    "    keep_indices = set()\n",
    "    \n",
    "    ids_to_scores = {}\n",
    "    class_ids = set(r['class_ids'])\n",
    "    \n",
    "    class_counts = np.zeros(len(class_ids)+10)\n",
    "    \n",
    "    for i in range(len(r['class_ids'])):\n",
    "        class_id = r['class_ids'][i]\n",
    "                \n",
    "        if class_counts[class_id] < num_elements_per_class_id[class_id]:\n",
    "            class_counts[class_id] += 1\n",
    "            keep_indices.add(i)\n",
    "    \n",
    "    new_r = { 'rois': np.array([]), 'masks': np.array([]), 'class_ids': np.array([]), 'scores': np.array([])}\n",
    "    for index in keep_indices:\n",
    "        for x in ['rois', 'class_ids', 'scores']:\n",
    "            new_r[x] = np.append(new_r[x], r[x][index])\n",
    "    \n",
    "    # masks\n",
    "    delete_indices = list(set(range(len(r['class_ids']))).symmetric_difference(keep_indices))\n",
    "    new_r['masks'] = np.delete(r['masks'], delete_indices, axis=2)\n",
    "    \n",
    "    new_r['rois'] = new_r['rois'].reshape(len(keep_indices),4)\n",
    "    new_r['class_ids'] = new_r['class_ids'].astype(int)\n",
    "    \n",
    "    return new_r\n",
    "\n",
    "def smooth(values, num_points = 100):\n",
    "    xnew = np.linspace(0,len(values)-1,num_points) #100 represents number of points to make between T.min and T.max\n",
    "    spl = make_interp_spline(list(range(len(values))), values, k=3) #BSpline object\n",
    "    values_smooth = spl(xnew)\n",
    "    \n",
    "    return xnew, values_smooth\n",
    "\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.loss = []\n",
    "        self.rpn_class_loss = []\n",
    "        self.rpn_bbox_loss = []\n",
    "        self.mrcnn_class_loss = []\n",
    "        self.mrcnn_bbox_loss = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.loss.append(logs.get('loss'))\n",
    "        self.rpn_class_loss.append(logs.get('rpn_class_loss'))\n",
    "        self.rpn_bbox_loss.append(logs.get('rpn_bbox_loss'))\n",
    "        self.mrcnn_class_loss.append(logs.get('mrcnn_class_loss'))\n",
    "        self.mrcnn_bbox_loss.append(logs.get('mrcnn_bbox_loss'))\n",
    "\n",
    "print(\"Imports done\")\n",
    "print(\"Directories:\")\n",
    "print(\"Root directory:\", ROOT_DIR)\n",
    "print(\"Model directory:\", MODEL_DIR)\n",
    "print(\"Pre-trained model directory:\", PRETRAINED_MODEL_DIR)\n",
    "print(\"M-RCNN directory:\", MRCNN_DIR)\n",
    "print(\"Datasets directory:\", DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Ordnerstruktur\n",
    "\n",
    "Hier kann mit einfachen Linux-Kommandozeilenbefehlen die Ordnerstruktur untersucht werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l /data\n",
    "!ls -l /data/boneage\n",
    "!ls -l /workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Konfiguration\n",
    "\n",
    "An dieser Stelle wird die Konfiguration des Modells festgelegt. Zur Optimierung werden wir später an diesen Punkt zurück kehren.\n",
    "\n",
    "Die wichtigsten Stellschrauben sind:\n",
    "\n",
    "- `IMAGES_PER_GPU`: dies ist in unserem Fall die Batchgröße, da `GPU_COUNT` immer 1 bleibt. Diese Zahl kann erhöht werden, da wir aber nur 40% des GPU-Speichers zur Verfügung haben, kann dies zur Trainingsabstürzen führen.\n",
    "- `RPN_ANCHOR_SCALES`: Ein \"Anchor\" ist bei dem MRCNN-Modell eine vom ersten Netzwerk (RPN) vorgeschlagene Region, hier kann festgelegt werden, welche Größenordnungen dieser Anchor haben dürfen (bezogen auf das skalierte Bild, nicht das Originalbild). Da wir nach kleinen (DIP/PIP/MCP) und mittelgroßen (Wrist) Objekten suchen, könnte hier z.B. zur Optimierung die 64 entfernt werden.\n",
    "- `TRAIN_ROIS_PER_IMAGE`: Wie viele ROIs sollen pro Bild gesucht werden? Für kurze Trainingsläufe ist es besser hier eine kleine Zahl zu wählen, da ansonsten zu viele Objekte auf den Bildern gefunden werden. Für längere Trainingsläufe sollte die Zahl etwa dem __Dreifachen der erwarteten Objekte__ entsprechen, da der Klassifikator dann die false positives noch herausfiltern kann.\n",
    "- `LEARNING_RATE`: die wohl wichtigste Stellschraube während des Trainingsprozesses. Ist diese Zahl zu klein, dann wird nur langsam gelernt, ist diese Zahl zu groß dann besteht die Gefahr dass das Netzwerk nicht konvergiert, der Netzwerkfehler also über die Zeit konstant bleibt oder fluktuiert. Sinnvolle Werte liegen zwischen 0.001 und 0.00001\n",
    "- `LEARNING_MOMENTUM`: beeinflusst den Einflusst des letzten Gewichtsupdates auf das aktuelle Gewichtsupdate. Kann z.B. auf 0.5 reduziert werden oder deaktiviert werden.\n",
    "- `WEIGHT_DECAY`: beeinflusst das Schrumpfen von Gewichten die nicht verändert wurden. Kann deaktiviert werden, sollte nicht zu viel erhöht werden.\n",
    "- `DETECTION_MIN_CONFIDENCE`: gibt an ab welchen Konfidenzlevel das Modell ein gefundenes Objekt ausgeben soll. Wenn zu wenige Objekte angezeigt kann diese Zahl reduziert werden. Dies kann jedoch dazu führen, dass öfter falsche Objekte angezeigt werden.\n",
    "- `STEPS_PER_EPOCH`: Aus wie vielen Schritten (also Mini-Batches) eine Epoche besteht. Da wir 238 Bilder und eine Batchgröße von 8 haben, brauchen wir `238 / 8 = 29.75`, also 30 Schritte um einmal über alle Bilder zu iterieren.\n",
    "- `VALIDATION_STEPS`: Nach jeder Epoch wird der Validierungsfehler berechnet, wir haben 89 Validierungsbilder, also brauchen wir 11 Schritte um über diese zu iterieren. Um die Trainingszeit zu reduzieren, wird hier nur die Hälfte genommen – dieser Wert hat keinen direkten Einfluss auf das Training, gibt uns nur eine grobe Orientierung wie gut unser Modell generalisiert.\n",
    "\n",
    "Aufgaben:\n",
    "\n",
    "- Wählen Sie eine geeignete Lernrate (`LEARNING_RATE`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoneAgeConfig(Config):\n",
    "    \"\"\"Configuration for training on the bone age dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the bone age dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"bone_age\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    #NUM_CLASSES = 1 + 5 + 4 + 5 + 3  # background + 5xPIP + 4xDIP + 5xMCP + Wrist + Ulna + Radius\n",
    "    NUM_CLASSES = 1 + 6  # background + PIP + DIP + MCP + Wrist + Ulna + Radius\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    #RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "    RPN_ANCHOR_SCALES = (8, 16, 32, 64)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 50\n",
    "    \n",
    "    LEARNING_RATE = ___LERNRATE_EINTRAGEN___\n",
    "    \n",
    "    LEARNING_MOMENTUM = 0.9\n",
    "    \n",
    "    WEIGHT_DECAY = 0.0001\n",
    "    \n",
    "    MINI_MASK_SHAPE=(128,128)\n",
    "    \n",
    "    DETECTION_MIN_CONFIDENCE = 0.75\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 30\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = BoneAgeConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Datensatz\n",
    "\n",
    "Hier wird die Datensatz-Klasse definiert, in der u.a. die folgenden Methoden vorhanden sind:\n",
    "\n",
    "* load_bone_age(): Initialisiert den Datensatz, indem Klassen und Bilder aus der CSV geladen werden\n",
    "* load_image(): Lädt ein einzelnes Bild aus dem Datensatz – diese Funktion wird von der Klasse [`utils.Dataset`](https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/utils.py#L239) geerbt, die aufgrund des in `load_bone_age()` für das Bild übergebenen Pfades das Bild vom Dateisystem lädt.\n",
    "* load_mask(): Lädt die Gelenk-ROIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BoneAgeDataset(utils.Dataset):\n",
    "    \"\"\"Generates the bone age dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_bone_age(self, dataset_dir, annotation_file):\n",
    "        \"\"\"Load the requested subset of the bone age dataset.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"bone_age\", 1, \"PIP\")\n",
    "        self.add_class(\"bone_age\", 2, \"DIP\")\n",
    "        self.add_class(\"bone_age\", 3, \"MCP\")\n",
    "        self.add_class(\"bone_age\", 4, \"Wrist\")\n",
    "        self.add_class(\"bone_age\", 5, \"Ulna\")\n",
    "        self.add_class(\"bone_age\", 6, \"Radius\")\n",
    "\n",
    "        # Add images from CSV\n",
    "        with open(annotation_file, 'r') as input_file:\n",
    "\n",
    "            reader = csv.reader(input_file, delimiter=',')\n",
    "\n",
    "            # Skip header\n",
    "            header = next(reader)\n",
    "\n",
    "            image_dir = dataset_dir\n",
    "            current_filename, current_width, current_height = [None, None, None]\n",
    "            \n",
    "            annotations = []\n",
    "            i = 1\n",
    "            for row in reader:\n",
    "                filename, width, height, class_name, xmin, ymin, xmax, ymax = row\n",
    "                \n",
    "                width, height, xmin, ymin, xmax, ymax = [int(width), int(height), int(xmin), int(ymin), int(xmax), int(ymax)]\n",
    "                \n",
    "                \n",
    "                if filename != current_filename and current_filename != None:\n",
    "                    #annotations.sort(key=lambda x: x[0] + str(x[1]))\n",
    "                    #annotations = list(map(incremental_class_names, annotations))\n",
    "                    \n",
    "                    self.add_image(\n",
    "                        \"bone_age\", image_id=i,\n",
    "                        path=os.path.join(image_dir, current_filename),\n",
    "                        width=current_width,\n",
    "                        height=current_height,\n",
    "                        annotations=annotations)\n",
    "                    i += 1\n",
    "                    annotations = []\n",
    "                    #for c in class_name_counters:\n",
    "                        #class_name_counters[c] = 0\n",
    "                \n",
    "                current_filename = filename\n",
    "                current_width = width\n",
    "                current_height = height\n",
    "                annotations.append([class_name, xmin, ymin, xmax, ymax])\n",
    "                \n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"bone_age\":\n",
    "            return \"https://github.com/razorx89/rsna-boneage-ossification-roi-detection\"\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for instances in the given image.\n",
    "        \"\"\"\n",
    "        instance_masks = []\n",
    "        class_ids = []\n",
    "        image_info = self.image_info[image_id]\n",
    "        annotations = image_info[\"annotations\"]\n",
    "        # Build mask of shape [height, width, instance_count] and list\n",
    "        # of class IDs that correspond to each channel of the mask.\n",
    "        for annotation in annotations:\n",
    "            class_id = self.class_names.index(annotation[0])\n",
    "            if class_id:\n",
    "                m = np.zeros((image_info[\"height\"], image_info[\"width\"]))\n",
    "                m[annotation[2]:annotation[4], annotation[1]:annotation[3]] = 1.\n",
    "                \n",
    "                # Some objects are so small that they're less than 1 pixel area\n",
    "                # and end up rounded out. Skip those objects.\n",
    "                if m.max() < 1:\n",
    "                    continue\n",
    "                \n",
    "                instance_masks.append(m)\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "        # Pack instance masks into an array\n",
    "        masks = np.reshape(np.stack(instance_masks, axis=2).astype(np.bool), (1, image_info[\"height\"], image_info[\"width\"], len(class_ids)))[0]\n",
    "        class_ids = np.array(class_ids, dtype=np.int32)\n",
    "        return masks, class_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier werden nun Trainings- und Validierungsdatensatz initialisiert. Die Bilder liegen gesammelt im Verzeichnis `/data/boneage/train`, in den zugehörigen CSV-Dateien sind die oben erwähnten Subsets für Training und Validierung mit den zugehörigen ROIs definiert.\n",
    "\n",
    "Fehlermeldungen der Art `\"write error: broken pipe\"` können ignoriert werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = !ls -l /data/boneage/train | wc -l\n",
    "print (\"Files in bonage directory:\", files[0])\n",
    "\n",
    "lines = !cat /data/boneage/train-bboxes.csv | wc -l\n",
    "print(\"Files in training dataset:\", int(lines[0]) // 17)\n",
    "\n",
    "lines = !cat /data/boneage/validation-bboxes.csv | wc -l\n",
    "print(\"Files in validation dataset:\", int(lines[0]) // 17)\n",
    "!cat /data/boneage/train-bboxes.csv | head -n 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufgaben:\n",
    "\n",
    "- Tragen Sie die passenden Pfade für die CSV-Dateien ein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = BoneAgeDataset()\n",
    "dataset_train.load_bone_age(\"/data/boneage/train\",\n",
    "                            \"___CSV-PFAD_EINTRAGEN___\")\n",
    "dataset_train.prepare()\n",
    "\n",
    "color_palette = visualize.random_colors(len(dataset_train.class_names))\n",
    "\n",
    "# Validation dataset\n",
    "# training and validation data are subsets from the same directory\n",
    "dataset_val = BoneAgeDataset()\n",
    "dataset_val.load_bone_age(\"/data/boneage/train\",\n",
    "                        \"___CSV-PFAD_EINTRAGEN___\")\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Bilder anzeigen\n",
    "\n",
    "Hier werden nun zufällig ein paar Bilder mit den zugehörigen ROIs geladen und angezeigt. Diese Zelle kann mehrfach ausgeführt werden. Falls die Zelle anfängt zu scrollen kann dies mit `Shift + o` deaktiviert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# change this to show more or fewer images\n",
    "NUM_IMAGES = 2\n",
    "\n",
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, NUM_IMAGES, replace=False)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    masks, class_ids = dataset_train.load_mask(image_id)\n",
    "    bbox_list = list(map(lambda x: [x[2], x[1], x[4], x[3]], dataset_train.image_info[image_id][\"annotations\"]))\n",
    "    bbox = np.array(bbox_list)\n",
    "    print(\"Image shape: \" + str(image.shape))\n",
    "    print(\"Mask shape: \" + str(masks.shape))\n",
    "    print(\"Bbox shape: \" + str(bbox.shape))\n",
    "    colors = [color_palette[id] for id in class_ids]\n",
    "    visualize.display_instances(image, bbox, masks, class_ids,\n",
    "                            dataset_train.class_names, show_mask=False, show_polygon=False, figsize=(8, 8), colors=colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Erstellung des Modells\n",
    "\n",
    "Hier wird das Modell erstellt. Initial werden drei Parameter übergeben:\n",
    "\n",
    "- der Modell-Modus (`training` oder `inference`)\n",
    "- die oben definierte Konfiguration\n",
    "- das Verzeichnis indem das Modell gespeichert / geladen werden soll\n",
    "\n",
    "Anschließend werden die Gewichte des Modells initialisiert, mögliche Werte für `init_with` sind\n",
    "\n",
    "- `imagenet`: initialisiert das Modell mit Gewichten die auf dem Imagenet Datensatz trainiert wurden\n",
    "- `coco`: initialisiert das Modell mit Gewichten die auf dem MS COCO Datensatz trainiert wurden\n",
    "- `last`: versucht das zuletzt trainierte Modell zu laden. Nur sinnvoll wenn ein solches Modell im angegebenen Ordner existiert\n",
    "- `manual`: versucht das Modell aus der in `MANUAL_MODEL_PATH` angegebenen Datei zu laden\n",
    "\n",
    "Aufgaben:\n",
    "\n",
    "- Tragen Sie den passenden `mode` Parameter für den Modell-Modus ein\n",
    "- Wählen Sie den passenden Wert für die `init_with` Variable um das Modell mit auf dem MS COCO Datensatz trainierten Gewichten zu initialisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Which weights to start with?\n",
    "init_with = \"___WERT_EINTRAGEN___\"  # imagenet, coco, manual or last\n",
    "MANUAL_MODEL_PATH = \"/models/mask_rcnn_bone_age_0100.h5\"\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last(), by_name=True)\n",
    "elif init_with == \"manual\":\n",
    "    model.load_weights(MANUAL_MODEL_PATH, by_name=True)\n",
    "    \n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Training (Schritt 1)\n",
    "\n",
    "Nun findet das Training des Modells statt.\n",
    "\n",
    "Der Funktion [`model.train`](https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/model.py#L2276) werden hierzu\n",
    "\n",
    "- die Trainings- und Validierungsdatensätze,\n",
    "- die Lernrate (die oben in der Konfiguration festgelegt wurde),\n",
    "- die Anzahl Epochen (hier auf 1 gesetzt da es sonst zu lange dauert) und\n",
    "- die zu trainierenden Schichten\n",
    "\n",
    "übergeben.\n",
    "\n",
    "Das Training geschieht in zwei Schritten:\n",
    "\n",
    "1. nur die obersten, neu hinzugefügten Schichten. Dazu werden die Gewichte in den anderen Schichten eingefroren und beim Training nicht verändert. Um das zu erreichen, wird dem `layers` Parameter der Wert `heads` übergeben.\n",
    "\n",
    "2. anschließendes Fine-tuning aller Schichten mit `layers=\"all\"`\n",
    "\n",
    "Abschließend kann das Modell bei Bedarf manuell gespeichert werden, dies ist jedoch üblicherweise nicht nötig, da das Modell automatisch nach jeder Epoche gespeichert wird.\n",
    "\n",
    "Auf eine intelligente Vorverarbeitung der Bilder wurde an dieser Stelle verzichtet, dies ist jedoch in vielen Fällen ebenso wichtig oder noch wichtiger als eine gute Netzwerkachitektur und die Wahl geeigneter Lernparameter und würde in diesem Fall das Training deutlich leistungsstärkere Modelle in kürzerer Zeit erlauben.\n",
    "\n",
    "Aufgaben:\n",
    "\n",
    "- Wählen Sie die passenden Werte für die `layers`-Variablen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = LossHistory()\n",
    "\n",
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=2,\n",
    "            custom_callbacks=[history],\n",
    "            layers='___WERT_EINTRAGEN___')\n",
    "\n",
    "print(\"Training finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Ausgabe des Trainingsscripts enthält folgende Informationen:\n",
    "\n",
    "- `8/30`: Das aktuelle Minibatch. Wie oben beschrieben ergibt sich die Zahl 30 aus der Anzahl Trainingsbilder durch die Batch-Größe: `238 / 8 = 29.75`\n",
    "- `413s 180ms/step`: Gesamtdauer für die Trainingsepoche sowie durchschnittliche Dauer eines Minibatches\n",
    "- `ETA: 1:55`: Erwartete Restzeit\n",
    "- `loss: 1.3854`: Wert der zu minimierenden Verlustfunktion auf dem letzten Minibatch\n",
    "- `rpn_class_loss` und `rpn_bbox_loss`: Werte der Verlustfunktion für das Netzwerk welches die ROIs vorschlägt, einerseits für die Objektklassen und andererseits für die Bounding Boxes\n",
    "- `mrcnn_class_loss`, `mrcnn_bbox_loss` und `mrcnn_mask_loss` sind die entsprechenden Werte der Verlustfunktion für die Klassifikation, die Bounding Boxen und die Segmentierungsmasken (M-RCNN arbeitet intern immer mit diesen Masken)\n",
    "- `val_*:`: Die entsprechenden Werte auf dem gesamten Validierungsdatensatz. Wird nur einmal pro Epoche generiert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 Visualisierung (Schritt 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(*smooth(history.loss))\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('step')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(*smooth(history.rpn_class_loss))\n",
    "plt.title('model RPN class loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('step')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(*smooth(history.rpn_bbox_loss))\n",
    "plt.title('model RPN bbox loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('step')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(*smooth(history.mrcnn_class_loss))\n",
    "plt.title('model MRCNN class loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('step')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(*smooth(history.mrcnn_bbox_loss))\n",
    "plt.title('model MRCNN bbox loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('step')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Detektion (Schritt 1)\n",
    "\n",
    "Nach dem Training schauen wir uns jetzt an wie gut unser Modell funktioniert. Dazu wird die Batch-Größe auf 1 gesetzt und ein Modell mit dem Modus `inference` erstellt und anschließend die Gewichte vom Training geladen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(BoneAgeConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model_inference = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "model_path = model_inference.find_last()\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model_inference.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier wird nun ein je ein zufälliges Bild aus dem Trainings- und Validierungs-Datensatz genommen und zunächst das Original mit den zugehörigen ROIs und dann das Bild mit den vom Modell vorausgesagten ROIs und zugehörigen Konfidenzwerten angezeigt.\n",
    "\n",
    "Um die Ausgabe auf die X \"sichersten\" ROIs für jede Klasse zu reduzieren kann die `filter_result`-Zeile einkommentiert werden.\n",
    "\n",
    "Diese Zelle kann mehrfach ausgeführt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_train.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_train, inference_config, \n",
    "                           image_id )\n",
    "\n",
    "colors = [color_palette[id] for id in gt_class_id]\n",
    "\n",
    "print(\"Original (training):\")\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, show_mask=False, show_polygon=False, colors=colors, figsize=(8, 8))\n",
    "\n",
    "results = model_inference.detect([original_image], verbose=1)\n",
    "        \n",
    "print(\"Prediction:\")\n",
    "\n",
    "r = results[0]\n",
    "r = filter_result(r, {0: 0, 1: 5, 2: 5, 3: 5, 4: 1, 5: 1, 6: 1,})\n",
    "print(r['rois'].shape)\n",
    "print(r['masks'].shape)\n",
    "print(r['class_ids'].shape)\n",
    "print(r['scores'].shape)\n",
    "colors = [color_palette[id] for id in r['class_ids']]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax(), colors=colors, show_mask=False, show_polygon=False)\n",
    "\n",
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id)\n",
    "\n",
    "colors = [color_palette[id] for id in gt_class_id]\n",
    "plt.show()\n",
    "print(\"\\n\\nOriginal (validation):\")\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, show_mask=False, show_polygon=False, colors=colors, figsize=(8, 8))\n",
    "\n",
    "results = model_inference.detect([original_image], verbose=1)\n",
    "        \n",
    "print(\"Prediction:\")\n",
    "\n",
    "r = results[0]\n",
    "r = filter_result(r, {0: 0, 1: 5, 2: 5, 3: 5, 4: 1, 5: 1, 6: 1,})\n",
    "print(r['rois'].shape)\n",
    "print(r['masks'].shape)\n",
    "print(r['class_ids'].shape)\n",
    "print(r['scores'].shape)\n",
    "colors = [color_palette[id] for id in r['class_ids']]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax(), colors=colors, show_mask=False, show_polygon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Training (Schritt 2)\n",
    "\n",
    "Nun wird das komplette Modell für eine Epoche mit reduzierter Lernrate trainiert.\n",
    "\n",
    "Der `epochs`-Parameter besagt hier wie viele Epochen insgesamt trainiert werden soll. Da bereits für zwei Epochen trainiert wurde, wird durch die Angabe 4 hier nur noch zwei weitere Epoche trainiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=4,\n",
    "            custom_callbacks=[history],\n",
    "            layers=\"all\")\n",
    "\n",
    "print(\"Training finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Visualisierung (Schritt 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(*smooth(history.loss))\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('step')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(*smooth(history.rpn_class_loss))\n",
    "plt.title('model RPN class loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('step')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(*smooth(history.rpn_bbox_loss))\n",
    "plt.title('model RPN bbox loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('step')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(*smooth(history.mrcnn_class_loss))\n",
    "plt.title('model MRCNN class loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('step')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(*smooth(history.mrcnn_bbox_loss))\n",
    "plt.title('model MRCNN bbox loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('step')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Detektion (Schritt 2)\n",
    "\n",
    "Hier erneut die Detektion nach dem vollständigen Training über 4 Epochen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(BoneAgeConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model_inference = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "#model_path = os.path.join(PRETRAINED_MODEL_DIR, \"mask_rcnn_bone_age_0100.h5\")\n",
    "model_path = model_inference.find_last()\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model_inference.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_train.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_train, inference_config, \n",
    "                           image_id)\n",
    "\n",
    "colors = [color_palette[id] for id in gt_class_id]\n",
    "\n",
    "print(\"Original (training):\")\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, show_mask=False, show_polygon=False, colors=colors, figsize=(8, 8))\n",
    "\n",
    "results = model_inference.detect([original_image], verbose=1)\n",
    "        \n",
    "print(\"Prediction:\")\n",
    "\n",
    "r = results[0]\n",
    "r = filter_result(r, {0: 0, 1: 5, 2: 5, 3: 5, 4: 1, 5: 1, 6: 1,})\n",
    "print(r['rois'].shape)\n",
    "print(r['masks'].shape)\n",
    "print(r['class_ids'].shape)\n",
    "print(r['scores'].shape)\n",
    "colors = [color_palette[id] for id in r['class_ids']]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax(), colors=colors, show_mask=False, show_polygon=False)\n",
    "\n",
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id)\n",
    "\n",
    "colors = [color_palette[id] for id in gt_class_id]\n",
    "plt.show()\n",
    "print(\"\\n\\nOriginal (validation):\")\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, show_mask=False, show_polygon=False, colors=colors, figsize=(8, 8))\n",
    "\n",
    "results = model_inference.detect([original_image], verbose=1)\n",
    "        \n",
    "print(\"Prediction:\")\n",
    "\n",
    "r = results[0]\n",
    "r = filter_result(r, {0: 0, 1: 5, 2: 5, 3: 5, 4: 1, 5: 1, 6: 1,})\n",
    "print(r['rois'].shape)\n",
    "print(r['masks'].shape)\n",
    "print(r['class_ids'].shape)\n",
    "print(r['scores'].shape)\n",
    "colors = [color_palette[id] for id in r['class_ids']]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax(), colors=colors, show_mask=False, show_polygon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Pre-trained model\n",
    "\n",
    "Da die Performance unseres Modells nach wenigen Epochen Training noch zu wünschen übrig lässt, laden wir hier ein Modell welches für 100 Epochen trainiert wurde und vergleichen die Performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(BoneAgeConfig):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model_inference = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "model_path = os.path.join(PRETRAINED_MODEL_DIR, \"mask_rcnn_bone_age_0100.h5\")\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model_inference.load_weights(model_path, by_name=True)\n",
    "print(\"Model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Führen Sie nun selbst die Visualisierung durch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Evaluation\n",
    "\n",
    "Zur Evaluierung werden verschiedene Fehlermaße berechnet:\n",
    "\n",
    "**Precision:** $P = \\frac{TP}{TP + FP}$, wobei TP die _true positives_ (hier: es wurde eine Segmentierungsmaske zu einer Lesion gefunden) und FP die _false positives_ sind (hier: es wurde eine Segmentierungsmakse gefunden, wo tatsächlich keine Lesion ist), d.h. die Precision gibt an wie viele der gefundenen Segmentierungsmasken wirklich zu Lesions gehören.\n",
    "\n",
    "**Recall:** $R = \\frac{TP}{TP + FN}$, wobei FN die _false negatives_ sind (hier: Lesions zu denen keine Segmentierungsmasken gefunden wurden), d.h. Recall gibt an, zu wie vielen der tatsächlichen Lesions Segmentierungsmaksen gefunden wurden.\n",
    "\n",
    "Bei der Bildklassifikation gibt es vorgegebene Kategorien über die einfach bestimmt werden kann, ob eine Dateninstanz korrekt klassifiziert wurde, doch wie sieht das bei der Erkennung mehrerer Objekte in einem aus? Wann gilt eine gefundene ROI als _true positive_?\n",
    "\n",
    "## 6.1 IoU (Intersection over union)\n",
    "\n",
    "Mit der IoU wird die Überlappung zweier Bildbereiche bestimmt:\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1600/1*FrmKLxCtkokDC3Yr1wc70w.png\" />\n",
    "Quelle: https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173\n",
    "\n",
    "Diese Zahl liegt zwischen 0 und 1, wobei 0 auf unseren Datensatz bezogen besagt dass die vorausgesagte ROI keine Überlappung mit einer tatsächlichen ROI dieser Klasse hat, und 1 besagt dass die ROI eine tatsächliche ROI der gleichen Klasse exakt abbildet.\n",
    "\n",
    "Ob eine ROI nun für die oben beschriebenen Maße als korrekt angesehen wird hängt von der Wahl des IoU ab, im Folgenden wird mit einem IoU von 0.5 gearbeitet, d.h. sobald die Hälfte der gefundenen Segmentierungsmaske über einer tatsächlichen ROI liegt, wird sie als korrekt angesehen.\n",
    "\n",
    "Der IoU-Grenzwert kann über die Variable `IOU_THRESHOLD` angepasst werden, die Anzahl der zu untersuchenden Bilder über die Variable `NUM_IMAGES`.\n",
    "\n",
    "In der Funktion `print_condition` kann angepasst werden, wann ein untersuchtes Bild ausgegeben werden soll.\n",
    "\n",
    "Ausgegeben werden nun folgende Werte:\n",
    "\n",
    "- mean Average Precision (mAP) für das aktuelle Bild, dieser Wert berechnet sich aus der [Fläche unter der Precision-Recall-Kurve](https://medium.com/@jonathan_hui/map-mean-average-precision-for-object-detection-45c121a31173)\n",
    "- IoU für jede der nummertierten, vorausgesagten ROIs\n",
    "- Precision, Recall und die zugehörigen Variablen (Formel siehe oben)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "IOU_THRESHOLD = 0.5\n",
    "NUM_IMAGES = 10\n",
    "\n",
    "# change this if you want to print only certain images\n",
    "def print_condition(AP, precisions, recalls, overlaps):\n",
    "    # Print images below IoU threshold\n",
    "    #return overlaps[0] < IOU_THRESHOLD\n",
    "    # Uncomment to print all\n",
    "    return True\n",
    "    # Uncomment to print none\n",
    "    #return False\n",
    "\n",
    "# Compute VOC-Style mAP @ IoU=0.5\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, NUM_IMAGES, replace=False)\n",
    "APs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_ids, gt_bbox, gt_masks =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    \n",
    "    # Run object detection\n",
    "    results = model_inference.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    \n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_ids, gt_masks,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'],\n",
    "                        iou_threshold=IOU_THRESHOLD)\n",
    "    APs.append(AP)\n",
    "    \n",
    "    if print_condition(AP, precisions, recalls, overlaps):\n",
    "        print(\"Original:\")\n",
    "        colors = [color_palette[id] for id in gt_class_id]\n",
    "        visualize.display_instances(image, gt_bbox, gt_masks, gt_class_ids, \n",
    "                                    dataset_train.class_names, figsize=(8, 8),\n",
    "                                    colors=colors, show_polygon=False, show_mask=False)\n",
    "        print(\"Prediction:\")\n",
    "        print(\"mAP:\", AP)\n",
    "        ious_raw = list(map(lambda x: max(x), overlaps))\n",
    "        ious = list(map(lambda x, n: str(n) + \": \" + str(x), ious_raw, list(range(1, len(r['class_ids'])+1))))\n",
    "        print(\"IoU:\", ious)\n",
    "        \n",
    "        true_positives = len(list(filter(lambda x: x > IOU_THRESHOLD, ious_raw)))\n",
    "        false_positives = len(ious_raw) - true_positives\n",
    "        false_negatives = len(gt_class_ids) - true_positives\n",
    "        print(\"True positives (predicted objects where overlap >=\", IOU_THRESHOLD, \"):\", true_positives)\n",
    "        print(\"False positives (predicted objects where overlap <\", IOU_THRESHOLD, \"):\", false_positives)\n",
    "        print(\"False negatives (ground truth objects for which there was no predicted object with overlap >=\", IOU_THRESHOLD, \"):\", false_negatives)\n",
    "        \n",
    "        print(\"Precision: \", true_positives, \" / (\", true_positives, \" + \", false_positives, \") = \", precisions[-2])\n",
    "        print(\"Recall: \", true_positives, \" / (\", true_positives, \" + \", false_negatives, \") = \", recalls[-2])\n",
    "        colors = [color_palette[id] for id in r['class_ids']]\n",
    "        visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \n",
    "                                    list(range(len(r['masks']))), r['scores'], ax=get_ax(),\n",
    "                                    colors=colors, show_polygon=False, show_mask=False,\n",
    "                                   captions=list(range(1, len(r['class_ids'])+1)))\n",
    "        plt.show()\n",
    "    \n",
    "print(\"Overall mAP: \", np.mean(APs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
