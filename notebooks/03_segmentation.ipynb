{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Melanomsegmentierung auf dem ISIC 2018 Datensatz\n",
    "\n",
    "Während es bei der Objektdetektion primär darum geht, Objekte auf einem Bild zu finden und zu unterscheiden, geht es bei der Segmentierung darum, jedem Pixel eines Bildes eine Klasse zuzuweisen und das Bild so in verschiedene Bereiche zu segmentieren. Das Ergebnis von Objektdetektion sind überlicherweise Bounding Boxes, während bei der Segmentierung die genauen Umrisse von Objekten ausgegeben werden.\n",
    "\n",
    "Der Code ist adaptiert von [train_shapes](https://github.com/matterport/Mask_RCNN/blob/master/samples/shapes/train_shapes.ipynb).\n",
    "\n",
    "## Datensatz\n",
    "\n",
    "Die [ISIC 2018](https://challenge2018.isic-archive.com/task1/) Datensatz umfasst 2596 Hautbilder mit genau einer prmären Lesion (Hautveränderung) und 2596 zugehörige Segmentierungsmasken.\n",
    "\n",
    "<img src=\"https://challenge2018.isic-archive.com/wp-content/uploads/2018/04/task1.png\" />\n",
    "Quelle: https://challenge2018.isic-archive.com/task1/\n",
    "\n",
    "Für unser Training wurde ein Subset von 556 Bilder der größe 1024x768 ausgewählt, um das Einlesen des Datensatzes zu beschleunigen. Diese werden in einen Trainingsdatensatz von 400 und einen Validierungsdatensatz von 156 aufgeteilt.\n",
    "\n",
    "Das Ziel unseres Modells besteht in der automatischen Erstellung der Segmentierungmasken für Hautbilder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports und Setup\n",
    "\n",
    "Wie bei der Klassifikation werden hier benötigte Bibliotheken importiert, Funktionen definiert und Konstanten festgelegt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import sys\n",
    "import random\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from PIL import Image\n",
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "\n",
    "# Root directory of the project\n",
    "MRCNN_DIR = os.path.abspath(\"/mrcnn/\")\n",
    "ROOT_DIR = os.path.abspath(\"/workspace\")\n",
    "PRETRAINED_MODEL_DIR = os.path.abspath(\"/models\")\n",
    "DATA_DIR = os.path.abspath(\"/data\")\n",
    "\n",
    "# Directory to save logs and trained model\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"logs\")\n",
    "\n",
    "# Local path to trained weights file\n",
    "COCO_MODEL_PATH = os.path.join(PRETRAINED_MODEL_DIR, \"mask_rcnn_coco.h5\")\n",
    "\n",
    "\n",
    "# Make sure TF does not print confusing warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "# Tell TF to not use all GPU RAM\n",
    "from tensorflow import ConfigProto\n",
    "config = ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = .40\n",
    "session = tf.Session(config=config)\n",
    "\n",
    "from tensorflow.python.keras.callbacks import Callback\n",
    "\n",
    "import asyncio\n",
    "import concurrent\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(MRCNN_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "import mrcnn.model as modellib\n",
    "from mrcnn import visualize\n",
    "from mrcnn.model import log\n",
    "\n",
    "%matplotlib inline \n",
    "\n",
    "def get_ax(rows=1, cols=1, size=8):\n",
    "    \"\"\"Return a Matplotlib Axes array to be used in\n",
    "    all visualizations in the notebook. Provide a\n",
    "    central point to control graph sizes.\n",
    "    \n",
    "    Change the default size attribute to control the size\n",
    "    of rendered images\n",
    "    \"\"\"\n",
    "    _, ax = plt.subplots(rows, cols, figsize=(size*cols, size*rows))\n",
    "    return ax\n",
    "\n",
    "def smooth(values, num_points = 100):\n",
    "    xnew = np.linspace(0,len(values)-1,num_points) #100 represents number of points to make between T.min and T.max\n",
    "    spl = make_interp_spline(list(range(len(values))), values, k=3) #BSpline object\n",
    "    values_smooth = spl(xnew)\n",
    "    \n",
    "    return xnew, values_smooth\n",
    "\n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.loss = []\n",
    "        self.rpn_class_loss = []\n",
    "        self.rpn_bbox_loss = []\n",
    "        self.mrcnn_class_loss = []\n",
    "        self.mrcnn_bbox_loss = []\n",
    "        self.mrcnn_mask_loss = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.loss.append(logs.get('loss'))\n",
    "        self.rpn_class_loss.append(logs.get('rpn_class_loss'))\n",
    "        self.rpn_bbox_loss.append(logs.get('rpn_bbox_loss'))\n",
    "        self.mrcnn_class_loss.append(logs.get('mrcnn_class_loss'))\n",
    "        self.mrcnn_bbox_loss.append(logs.get('mrcnn_bbox_loss'))\n",
    "        self.mrcnn_mask_loss.append(logs.get('mrcnn_mask_loss'))\n",
    "\n",
    "print(\"Setup done\")\n",
    "print(\"Directories:\")\n",
    "print(\"Root directory:\", ROOT_DIR)\n",
    "print(\"Model directory:\", MODEL_DIR)\n",
    "print(\"Pre-trained model directory:\", PRETRAINED_MODEL_DIR)\n",
    "print(\"M-RCNN directory:\", MRCNN_DIR)\n",
    "print(\"Datasets directory:\", DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Ordnerstruktur\n",
    "\n",
    "Im folgenden können die oben aufgelisteten Verzeichnisse inspiziert werden, um sich mit der Ordnerstruktur vertraut zu machen.\n",
    "\n",
    "Fehlermeldungen der Art `\"write error: broken pipe\"` können ignoriert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l /workspace\n",
    "!ls -l /data/ISIC_2018\n",
    "!ls -l /workspace/logs\n",
    "!ls -l /models\n",
    "!ls -l /mrcnn\n",
    "!ls -l /data/ISIC_2018/train | head -n10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Konfiguration\n",
    "\n",
    "An dieser Stelle wird die Konfiguration des Modells festgelegt. Zur Optimierung werden wir später an diesen Punkt zurück kehren.\n",
    "\n",
    "Die wichtigsten Stellschrauben sind:\n",
    "\n",
    "- `IMAGES_PER_GPU`: dies ist in unserem Fall die Batchgröße, da `GPU_COUNT` immer 1 bleibt. Diese Zahl kann erhöht werden, da wir aber nur 40% des GPU-Speichers zur Verfügung haben, kann dies zur Trainingsabstürzen führen.\n",
    "- `RPN_ANCHOR_SCALES`: Ein \"Anchor\" ist bei dem MRCNN-Modell eine vom ersten Netzwerk (RPN) vorgeschlagene Region, hier kann festgelegt werden, welche Größenordnungen dieser Anchor haben dürfen (bezogen auf das skalierte Bild, nicht das Originalbild). Da die Lesions teilweise relativ klein, an anderen Stellen aber auch das komplette Bild einnehmen, kann man hier eher am unteren Ende des Spektrums noch etwas entfernen als am oberen.\n",
    "- `TRAIN_ROIS_PER_IMAGE`: Wie viele ROIs sollen pro Bild gesucht werden? Für kurze Trainingsläufe ist es besser hier eine kleine Zahl zu wählen, da ansonsten zu viele Objekte auf den Bildern gefunden werden. Für längere Trainingsläufe sollte die Zahl etwa dem Dreifachen der erwarteten Objekte entsprechen, da der Klassifikator dann die false positives noch herausfiltern kann. Wählt man die Zahl zu klein, kann es sein dass das Modell nicht konvergiert.\n",
    "- `LEARNING_RATE`: die wohl wichtigste Stellschraube während des Trainingsprozesses. Ist diese Zahl zu klein, dann wird nur langsam gelernt, ist diese Zahl zu groß dann besteht die Gefahr dass das Netzwerk nicht konvergiert, der Netzwerkfehler also über die Zeit konstant bleibt oder fluktuiert. Sinnvolle Startwerte sind z.B. 0.0001 oder 0.001\n",
    "- `LEARNING_MOMENTUM`: beeinflusst den Einflusst des letzten Gewichtsupdates auf das aktuelle Gewichtsupdate. Kann z.B. auf 0.5 reduziert werden oder deaktiviert werden.\n",
    "- `WEIGHT_DECAY`: beeinflusst das Schrumpfen von Gewichten die nicht verändert wurden. Kann deaktiviert werden, sollte nicht zu viel erhöht werden.\n",
    "- `DETECTION_MIN_CONFIDENCE`: gibt an ab welchen Konfidenzlevel das Modell ein gefundenes Objekt ausgeben soll. Wenn zu wenige Objekte angezeigt kann diese Zahl reduziert werden. Dies kann jedoch dazu führen, dass öfter falsche Objekte angezeigt werden.\n",
    "- `STEPS_PER_EPOCH`: Aus wie vielen Schritten (also Mini-Batches) eine Epoche besteht. Da wir 400 Bilder und eine Batchgröße von 8 haben, brauchen wir 50 Schritte um einmal über alle Bilder zu iterieren.\n",
    "- `VALIDATION_STEPS`: Nach jeder Epoch wird der Validierungsfehler berechnet, wir haben 156 Validierungsbilder, also bräuchten wir 20 Schritte um über diese zu iterieren. Um die Trainingszeit zu reduzieren, wird auch nur 5 genommen\n",
    "\n",
    "Aufgaben:\n",
    "\n",
    "- Wählen Sie eine geeignete Lernrate (`LEARNING_RATE`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISIC2018Config(Config):\n",
    "    \"\"\"Configuration for training on the bone age dataset.\n",
    "    Derives from the base Config class and overrides values specific\n",
    "    to the bone age dataset.\n",
    "    \"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"isic\"\n",
    "\n",
    "    # Train on 1 GPU and 8 images per GPU. We can put multiple images on each\n",
    "    # GPU because the images are small. Batch size is 8 (GPUs * images/GPU).\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 8\n",
    "\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 1  # background + lesion\n",
    "\n",
    "    # Use small images for faster training. Set the limits of the small side\n",
    "    # the large side, and that determines the image shape.\n",
    "    IMAGE_MIN_DIM = 128\n",
    "    IMAGE_MAX_DIM = 128\n",
    "\n",
    "    # Use smaller anchors because our image and objects are small\n",
    "    #RPN_ANCHOR_SCALES = (8, 16, 32, 64, 128)  # anchor side in pixels\n",
    "    RPN_ANCHOR_SCALES = (16, 32, 64, 128)  # anchor side in pixels\n",
    "\n",
    "    # Reduce training ROIs per image because the images are small and have\n",
    "    # few objects. Aim to allow ROI sampling to pick 33% positive ROIs.\n",
    "    TRAIN_ROIS_PER_IMAGE = 5\n",
    "    \n",
    "    LEARNING_RATE = ___LERNRATE_EINTRAGEN___\n",
    "    \n",
    "    LEARNING_MOMENTUM = 0.9\n",
    "    \n",
    "    WEIGHT_DECAY = 0.0001\n",
    "    \n",
    "    DETECTION_MIN_CONFIDENCE = 0.75\n",
    "\n",
    "    # Use a small epoch since the data is simple\n",
    "    STEPS_PER_EPOCH = 50\n",
    "\n",
    "    # use small validation steps since the epoch is small\n",
    "    VALIDATION_STEPS = 5\n",
    "    \n",
    "config = ISIC2018Config()\n",
    "config.display()\n",
    "\n",
    "print(\"Configuration done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Datensatz\n",
    "\n",
    "Hier wird die Datensatz-Klasse definiert, in der u.a. die folgenden Methoden vorhanden sind:\n",
    "\n",
    "* load_isic(): Initialisiert den Datensatz, indem die Bilder aus dem Trainingsverzeichnis geladen werden (bzw. in diesem Fall werden nur die Dateinamen geladen)\n",
    "* load_image(): Lädt ein einzelnes Bild aus dem Datensatz – diese Funktion wird von der Klasse [`utils.Dataset`](https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/utils.py#L239) geerbt, die aufgrund des in `load_isic()` für das Bild übergebenen Pfades das Bild vom Dateisystem lädt.\n",
    "* load_mask(): Lädt die zugehörige Segmentierungsmaske"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ISIC2018Dataset(utils.Dataset):\n",
    "    \"\"\"Generates the bone age dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def load_isic(self, image_dir, mask_dir, image_range):\n",
    "        \"\"\"Load the requested subset of the ISIC dataset.\n",
    "        \"\"\"\n",
    "        # Add classes\n",
    "        self.add_class(\"isic\", 1, \"lesion\")\n",
    "\n",
    "        directory = os.fsencode(image_dir)\n",
    "\n",
    "        i = 0\n",
    "        for file in os.listdir(directory):\n",
    "            i+=1\n",
    "            filename = os.fsdecode(file)\n",
    "            if filename.endswith(\".jpg\"):\n",
    "                if not i in image_range:\n",
    "                    continue\n",
    "                    \n",
    "                image_id = filename.split(\".\")[0].split(\"_\")[1]\n",
    "                image_path = os.path.join(image_dir, filename)\n",
    "                #height, width, _ = cv2.imread(image_path).shape\n",
    "                # print(os.path.join(directory, filename))\n",
    "                self.add_image(\n",
    "                    \"isic\", image_id=image_id,\n",
    "                    path=image_path,\n",
    "                    mask_path=os.path.join(mask_dir, \"ISIC_\" + image_id + \"_segmentation.png\"),\n",
    "                    width=1024,\n",
    "                    height=768)\n",
    "                \n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"Return the shapes data of the image.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"isic\":\n",
    "            return \"https://challenge2018.isic-archive.com/task1/\"\n",
    "        else:\n",
    "            super(self.__class__).image_reference(self, image_id)\n",
    "\n",
    "    def load_mask(self, image_id):\n",
    "        \"\"\"Generate instance masks for instances in the given image.\n",
    "        \"\"\"\n",
    "        instance_masks = []\n",
    "        class_ids = []\n",
    "        image_info = self.image_info[image_id]\n",
    "        mask_path = image_info[\"mask_path\"]\n",
    "        \n",
    "        # Build mask of shape [height, width, instance_count] and list\n",
    "        # of class IDs that correspond to each channel of the mask.\n",
    "        img = Image.open(mask_path)\n",
    "        mask = np.array(img.getdata(), np.uint8).reshape(image_info[\"height\"], image_info[\"width\"])\n",
    "            #.reshape(image_info[\"height\"], image_info[\"width\"])/255\n",
    "        instance_masks.append(mask)\n",
    "        class_ids.append(1)\n",
    "\n",
    "        # Pack instance masks into an array\n",
    "        masks = np.reshape(np.stack(instance_masks, axis=2).astype(np.bool), (1, image_info[\"height\"], image_info[\"width\"], len(class_ids)))[0]\n",
    "        class_ids = np.array(class_ids, dtype=np.int32)\n",
    "        return masks, class_ids\n",
    "\n",
    "print(\"Dataset created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier werden nun Trainings- und Validierungsdatensatz initialisiert. Die Bilder liegen gesammelt im Unterverzeichnis `train`, die Segmentierungsmasken im Unterverzeichnis `ISIC2018_Task1_Training_GroundTruth`. Über die im Anschluss angegebene `range()` wird bestimmt welche Bilder zum Training und zur Validierung genommen werden.\n",
    "\n",
    "Fehlermeldungen der Art `\"write error: broken pipe\"` können ignoriert werden.\n",
    "\n",
    "Aufgaben:\n",
    "\n",
    "- Geben Sie die korrekten Pfade für Trainings- und Validierungsdatensatz an"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l /data/ISIC_2018\n",
    "!ls /data/ISIC_2018/train | wc -l\n",
    "!ls -l /data/ISIC_2018/train | head -n10\n",
    "!ls -l /data/ISIC_2018/ISIC2018_Task1_Training_GroundTruth | head -n10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training dataset\n",
    "dataset_train = ISIC2018Dataset()\n",
    "dataset_train.load_isic(\"___PFAD_EINTRAGEN___\",\n",
    "                        \"___PFAD_EINTRAGEN___\", range(1,400))\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Validation dataset\n",
    "dataset_val = ISIC2018Dataset()\n",
    "# training and evaluation data are taken from the same directory\n",
    "dataset_val.load_isic(\"___PFAD_EINTRAGEN___\", \n",
    "                        \"___PFAD_EINTRAGEN___\", range(401,556))\n",
    "dataset_val.prepare()\n",
    "\n",
    "print(\"Dataset prepared\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Beispielbilder\n",
    "\n",
    "Hier werden nun zufällig ein paar Bilder mit den zugehörigen Masken geladen und angezeigt. Diese Zelle kann mehrfach ausgeführt werden. Falls die Zelle anfängt zu scrollen kann dies mit `Shift + o` deaktiviert werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_IMAGES = 4\n",
    "\n",
    "# Load and display random samples\n",
    "image_ids = np.random.choice(dataset_train.image_ids, NUM_IMAGES, replace=False)\n",
    "for image_id in image_ids:\n",
    "    image = dataset_train.load_image(image_id)\n",
    "    mask, class_ids = dataset_train.load_mask(image_id)\n",
    "    print(\"Image shape: \" + str(image.shape))\n",
    "    print(\"Mask shape: \" + str(mask.shape))\n",
    "    visualize.display_top_masks(image, mask, class_ids, dataset_train.class_names, limit=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Modellerstellung\n",
    "\n",
    "Hier wird das Modell erstellt. Initial werden drei Parameter übergeben:\n",
    "\n",
    "- der Modell-Modus (`training` oder `inference`)\n",
    "- die oben definierte Konfiguration\n",
    "- das Verzeichnis indem das Modell gespeichert / geladen werden soll\n",
    "\n",
    "Anschließend werden die Gewichte des Modells initialisiert, mögliche Werte für `init_with` sind\n",
    "\n",
    "- `imagenet`: initialisiert das Modell mit Gewichten die auf dem Imagenet Datensatz trainiert wurden\n",
    "- `coco`: initialisiert das Modell mit Gewichten die auf dem MS COCO Datensatz trainiert wurden\n",
    "- `last`: versucht das zuletzt trainierte Modell zu laden. Nur sinnvoll wenn ein solches Modell im angegebenen Ordner existiert\n",
    "- `manual`: versucht das Modell aus der in `MANUAL_MODEL_PATH` angegebenen Datei zu laden\n",
    "\n",
    "Aufgaben:\n",
    "\n",
    "- Tragen Sie den passenden `mode` Parameter für den Modell-Modus ein\n",
    "- Wählen Sie den passenden Wert für die `init_with` Variable um das Modell mit auf dem MS COCO Datensatz trainierten Gewichten zu initialisieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"___MODELL-MODUS_EINTRAGEN___\", config=config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Which weights to start with?\n",
    "init_with = \"___WERT_EINTRAGEN___\"  # imagenet, coco, manual, or last\n",
    "MANUAL_MODEL_PATH = \"/models/mask_rcnn_bone_age_0100.h5\"\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last(), by_name=True)\n",
    "elif init_with == \"manual\":\n",
    "    model.load_weights(MANUAL_MODEL_PATH, by_name=True)\n",
    "    \n",
    "print(\"Model created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Training (Schritt 1)\n",
    "\n",
    "Nun findet das Training des Modells statt.\n",
    "\n",
    "Der Funktion [`model.train`](https://github.com/matterport/Mask_RCNN/blob/master/mrcnn/model.py#L2276) werden hierzu\n",
    "\n",
    "- die Trainings- und Validierungsdatensätze,\n",
    "- die Lernrate (die oben in der Konfiguration festgelegt wurde),\n",
    "- die Anzahl Epochen (hier auf 2 gesetzt da es sonst zu lange dauert) und\n",
    "- die zu trainierenden Schichten\n",
    "\n",
    "übergeben.\n",
    "\n",
    "Das Training geschieht in zwei Schritten:\n",
    "\n",
    "1. nur die obersten, neu hinzugefügten Schichten. Dazu werden die Gewichte in den anderen Schichten eingefroren und beim Training nicht verändert. Um das zu erreichen, wird dem `layers` Parameter der Wert `heads` übergeben.\n",
    "\n",
    "2. anschließendes Fine-tuning aller Schichten mit `layers=\"all\"`\n",
    "\n",
    "In beiden Schritten wird für 2 Epochen trainiert, was zusammen weniger als 10 Minuten dauern sollte.\n",
    "\n",
    "Aufgaben:\n",
    "\n",
    "- Wählen Sie den passenden Wert für die `layers`-Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "history = LossHistory()\n",
    "\n",
    "# Train the head branches\n",
    "# Passing layers=\"heads\" freezes all layers except the head\n",
    "# layers. You can also pass a regular expression to select\n",
    "# which layers to train by name pattern.\n",
    "model.train(dataset_train, dataset_val,\n",
    "            learning_rate=config.LEARNING_RATE, \n",
    "            epochs=2,\n",
    "            custom_callbacks=[history],\n",
    "            layers='___WERT_EINTRAGEN___')\n",
    "print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Ausgabe des Trainingsscripts enthält folgende Informationen:\n",
    "\n",
    "- `8/50`: Das aktuelle Minibatch. Wie oben beschrieben ergibt sich die Zahl 50 aus der Anzahl Trainingsbilder durch die Batch-Größe: `400 / 8 = 50`\n",
    "- `ETA: 1:55`: Erwartete Restzeit\n",
    "- `413s 180ms/step`: Gesamtdauer für die Trainingsepoche sowie durchschnittliche Dauer eines Minibatches\n",
    "- `loss: 1.3854`: Wert der zu minimierenden Verlustfunktion auf dem letzten Minibatch\n",
    "- `rpn_class_loss` und `rpn_bbox_loss`: Werte der Verlustfunktion für das Netzwerk welches die ROIs vorschlägt, einerseits für die Objektklassen und andererseits für die Bounding Boxes, jeweils für das letzte Trainings-Minibatch\n",
    "- `mrcnn_class_loss`, `mrcnn_bbox_loss` und `mrcnn_mask_loss` sind die entsprechenden Werte der Verlustfunktion für die Klassifikation, die Bounding Boxen und die Segmentierungsmasken (M-RCNN arbeitet intern immer mit diesen Masken), jeweils für das letzte Trainings-Minibatch\n",
    "- `val_*:`: Die entsprechenden Werte auf dem gesamten Validierungsdatensatz. Wird nur einmal pro Epoche generiert\n",
    "\n",
    "Die `*_class` Fehler sind hier erwartungsgemäß sehr gering, da es nur eine Klasse (bzw. zwei wenn man die Klasse \"Hintergrund\" dazu zählt) gibt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  4.1.1 Visualisierung (Schritt 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(*smooth(history.loss))\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('step')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(*smooth(history.rpn_class_loss))\n",
    "plt.title('model RPN class loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('step')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(*smooth(history.rpn_bbox_loss))\n",
    "plt.title('model RPN bbox loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('step')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(*smooth(history.mrcnn_class_loss))\n",
    "plt.title('model MRCNN class loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('step')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(*smooth(history.mrcnn_bbox_loss))\n",
    "plt.title('model MRCNN bbox loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('step')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(*smooth(history.mrcnn_mask_loss))\n",
    "plt.title('model MRCNN mask loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('step')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.2 Detection (Schritt 1)\n",
    "\n",
    "Nach dem Training schauen wir uns jetzt an wie gut unser Modell funktioniert. Dazu wird die Batch-Größe auf 1 gesetzt und ein Modell mit dem Modus `inference` erstellt und anschließend die Gewichte vom Training geladen.\n",
    "\n",
    "Aufgaben:\n",
    "\n",
    "- Setzen Sie den korrekten Modell-Modus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(ISIC2018Config):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"___MODELL-MODUS_EINTRAGEN___\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()\n",
    "#model_path = os.path.join(PRETRAINED_MODEL_DIR, \"mask_rcnn_isic_0050.h5\")\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier wird nun ein zufälliges Bild aus dem Datensatz genommen und zunächst das Original mit der zugehörigen Segmentierungsmakse und dann das Bild mit der vom Modell vorausgesagten Maske und zugehörigem Konfidenzwert angezeigt.\n",
    "\n",
    "Diese Zelle kann mehrfach ausgeführt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8), show_bbox=False, show_mask=False)\n",
    "\n",
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax(), show_bbox=False, show_mask=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Training (Schritt 2)\n",
    "\n",
    "Nun wird das gesamte Netzwerk mit einer reduzierten Lernrate für zwei weitere Epoche trainiert.\n",
    "\n",
    "Aufgaben:\n",
    "\n",
    "- Wählen Sie den passenden Wert für die `layers`-Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create model in training mode\n",
    "model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Which weights to start with?\n",
    "init_with = \"last\"  # imagenet, coco, manual, or last\n",
    "MANUAL_MODEL_PATH = \"/models/mask_rcnn_bone_age_0100.h5\"\n",
    "\n",
    "if init_with == \"imagenet\":\n",
    "    model.load_weights(model.get_imagenet_weights(), by_name=True)\n",
    "elif init_with == \"coco\":\n",
    "    # Load weights trained on MS COCO, but skip layers that\n",
    "    # are different due to the different number of classes\n",
    "    # See README for instructions to download the COCO weights\n",
    "    model.load_weights(COCO_MODEL_PATH, by_name=True,\n",
    "                       exclude=[\"mrcnn_class_logits\", \"mrcnn_bbox_fc\", \n",
    "                                \"mrcnn_bbox\", \"mrcnn_mask\"])\n",
    "elif init_with == \"last\":\n",
    "    # Load the last model you trained and continue training\n",
    "    model.load_weights(model.find_last(), by_name=True)\n",
    "elif init_with == \"manual\":\n",
    "    model.load_weights(MANUAL_MODEL_PATH, by_name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Fine tune all layers\n",
    "# Passing layers=\"all\" trains all layers. You can also \n",
    "# pass a regular expression to select which layers to\n",
    "# train by name pattern.\n",
    "model.train(dataset_train, dataset_val, \n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=4,\n",
    "            custom_callbacks=[history],\n",
    "            layers=\"___WERT_EINTRAGEN___\")\n",
    "\n",
    "print(\"Training finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Visualisierung (Schritt 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(*smooth(history.loss))\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('step')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(*smooth(history.rpn_class_loss))\n",
    "plt.title('model RPN class loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('step')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(*smooth(history.rpn_bbox_loss))\n",
    "plt.title('model RPN bbox loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('step')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(*smooth(history.mrcnn_class_loss))\n",
    "plt.title('model MRCNN class loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('step')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(*smooth(history.mrcnn_bbox_loss))\n",
    "plt.title('model MRCNN bbox loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('step')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# summarize history for loss\n",
    "plt.plot(*smooth(history.mrcnn_mask_loss))\n",
    "plt.title('model MRCNN mask loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('step')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.2 Detection (Schritt 2)\n",
    "\n",
    "Nach dem Training schauen wir uns jetzt an wie gut unser Modell funktioniert. Dazu wird die Batch-Größe auf 1 gesetzt und ein Modell mit dem Modus `inference` erstellt und anschließend die Gewichte vom Training geladen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(ISIC2018Config):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = model.find_last()\n",
    "#model_path = os.path.join(PRETRAINED_MODEL_DIR, \"mask_rcnn_isic_0050.h5\")\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier wird nun ein zufälliges Bild aus dem Datensatz genommen und zunächst das Original mit der zugehörigen Segmentierungsmakse und dann das Bild mit der vom Modell vorausgesagten Maske und zugehörigem Konfidenzwert angezeigt.\n",
    "\n",
    "Diese Zelle kann mehrfach ausgeführt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on a random image\n",
    "image_id = random.choice(dataset_val.image_ids)\n",
    "original_image, image_meta, gt_class_id, gt_bbox, gt_mask =\\\n",
    "    modellib.load_image_gt(dataset_val, inference_config, \n",
    "                           image_id, use_mini_mask=False)\n",
    "\n",
    "log(\"original_image\", original_image)\n",
    "log(\"image_meta\", image_meta)\n",
    "log(\"gt_class_id\", gt_class_id)\n",
    "log(\"gt_bbox\", gt_bbox)\n",
    "log(\"gt_mask\", gt_mask)\n",
    "\n",
    "visualize.display_instances(original_image, gt_bbox, gt_mask, gt_class_id, \n",
    "                            dataset_train.class_names, figsize=(8, 8), show_bbox=False, show_mask=False)\n",
    "\n",
    "results = model.detect([original_image], verbose=1)\n",
    "\n",
    "r = results[0]\n",
    "visualize.display_instances(original_image, r['rois'], r['masks'], r['class_ids'], \n",
    "                            dataset_val.class_names, r['scores'], ax=get_ax(), show_bbox=False, show_mask=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Pre-trained model\n",
    "\n",
    "Zum Vergleich mit unserem trainiertem Modell schauen wir uns jetzt noch die Performance eines 50 Epochen trainierten Modells an.\n",
    "\n",
    "Aufgabe:\n",
    "\n",
    "* Fügen Sie den Pfad zur vortrainierten Modelldatei ein"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PRETRAINED_MODEL_DIR)\n",
    "!ls -l /models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceConfig(ISIC2018Config):\n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "\n",
    "inference_config = InferenceConfig()\n",
    "\n",
    "# Recreate the model in inference mode\n",
    "model = modellib.MaskRCNN(mode=\"inference\", \n",
    "                          config=inference_config,\n",
    "                          model_dir=MODEL_DIR)\n",
    "\n",
    "# Get path to saved weights\n",
    "# Either set a specific path or find last trained weights\n",
    "# model_path = os.path.join(ROOT_DIR, \".h5 file name here\")\n",
    "model_path = \"___PFAD_EINTRAGEN___\"\n",
    "\n",
    "# Load trained weights\n",
    "print(\"Loading weights from \", model_path)\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aufgabe:\n",
    "\n",
    "- Testen Sie das vortrainierte Netz mit einem Bild Ihrer Wahl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Evaluation\n",
    "\n",
    "Ähnlich wie bei der Object Detection wird hier der IoU-Wert berechnet. Diese Zahl liegt zwischen 0 und 1, wobei 0 auf unseren Datensatz bezogen besagt dass die vorausgesagte Maske komplett außerhalb der Lesion liegt, und 1 besagt dass die Maske die Lesion exakt abbildet.\n",
    "\n",
    "Precision und Recall sind hier weniger interessant, da nur eine Maske pro Bild generiert wird (und es nur eine Klasse gibt), und Precision und Recall entsprechend nur 0 oder 1 sein können.\n",
    "\n",
    "Der mAP besagt entsprechend eher bei wie vielen untersuchten Bildern die Maske über dem IoU-Threshold gelegen hat, wo also die Lesion korrekt segmentiert wurde.\n",
    "\n",
    "Ob eine Segmentierungsmaske nun für die oben beschriebenen Maße als korrekt angesehen wird hängt von der Wahl des IoU ab, im Folgenden wird mit einem IoU von 0.75 gearbeitet, d.h. sobald drei Viertel der gefundenen Segmentierungsmaske über der Lesion liegt, wird sie als korrekt angesehen.\n",
    "\n",
    "Standardmäßig werden 20 Bilder evaluiert und nur diejenigen ausgegeben, bei denen der IoU unterhalb des Threshold liegt, wo also die Lesion nicht korrekt identifiziert wurde.\n",
    "\n",
    "Aufgabe:\n",
    "\n",
    "- Testen Sie mit verschiedenen IoU-Thresholds und `print-condition`-Werten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "IOU_THRESHOLD = 0.75\n",
    "NUM_IMAGES = 20\n",
    "\n",
    "# change this if you want to print only certain images\n",
    "def print_condition(AP, precisions, recalls, overlaps):\n",
    "    # Print images below IoU threshold\n",
    "    return overlaps[0] < IOU_THRESHOLD\n",
    "    # Uncomment to print all\n",
    "    #return True\n",
    "    # Uncomment to print none\n",
    "    #return False\n",
    "\n",
    "# Compute VOC-Style mAP @ IoU=0.75\n",
    "# Running on 10 images. Increase for better accuracy.\n",
    "image_ids = np.random.choice(dataset_val.image_ids, NUM_IMAGES, replace=False)\n",
    "APs = []\n",
    "IoUs = []\n",
    "for image_id in image_ids:\n",
    "    # Load image and ground truth data\n",
    "    image, image_meta, gt_class_ids, gt_bbox, gt_masks =\\\n",
    "        modellib.load_image_gt(dataset_val, inference_config,\n",
    "                               image_id, use_mini_mask=False)\n",
    "    molded_images = np.expand_dims(modellib.mold_image(image, inference_config), 0)\n",
    "    \n",
    "    # Run object detection\n",
    "    results = model.detect([image], verbose=0)\n",
    "    r = results[0]\n",
    "    \n",
    "    # Compute AP\n",
    "    AP, precisions, recalls, overlaps =\\\n",
    "        utils.compute_ap(gt_bbox, gt_class_ids, gt_masks,\n",
    "                         r[\"rois\"], r[\"class_ids\"], r[\"scores\"], r['masks'],\n",
    "                        iou_threshold=IOU_THRESHOLD)\n",
    "    APs.append(AP)\n",
    "    IoUs.append(overlaps[0][0])\n",
    "    \n",
    "    if print_condition(AP, precisions, recalls, overlaps):\n",
    "        print(\"Original:\")\n",
    "        visualize.display_instances(image, gt_bbox, gt_masks, gt_class_ids, \n",
    "                                dataset_train.class_names, figsize=(8, 8), show_bbox=False, show_mask=False)\n",
    "        print(\"Prediction:\")\n",
    "        print(\"IoU: \", overlaps[0][0])\n",
    "        visualize.display_instances(image, r['rois'], r['masks'], r['class_ids'], \n",
    "                                dataset_val.class_names, r['scores'], ax=get_ax(), show_bbox=False, show_mask=False)\n",
    "        plt.show()\n",
    "    \n",
    "print(\"mAP: \", np.mean(APs))\n",
    "print(\"mean IoU:\", np.mean(IoUs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
