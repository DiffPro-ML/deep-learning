{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bildklassifikation mit dem MURA-Datensatz\n",
    "\n",
    "Bei der Klassifikation geht es darum, einem Bild ein Label aus einer vorgegebenen Menge an möglichen Labels zuzuweisen. Im Vergleich zur Segmentierung und Objekt Detection ist diese Aufgabe die einfachste.\n",
    "\n",
    "In diesem Notebook wird ein Klassifikationsmodell für den MURA-Datensatz mit Keras und Tensorflow als Backend trainiert.\n",
    "\n",
    "## Datensatz\n",
    "\n",
    "Der [MURA](https://stanfordmlgroup.github.io/competitions/mura/) (**mu**sculoskeletal **ra**diographs) Datensatz besteht aus 40.561 Röntgenbildern von Oberkörperextremitäten der Kategorien: Ellbogen, Finger, Unterarm, Hand, Oberarm, Schulter, und Handgelenk. Zusätzlich ist jedem Bild das Label normal oder abnormal zugeordnet.\n",
    "\n",
    "![XR_HAND](http://s1.saviola.de/screens/mura.png)\n",
    "\n",
    "In diesem Jupyter-Notebook soll ein Model trainiert werden welches Bilder des MURA-Datensatzes in eine der sieben Extremitätsklassen einordnen kann.\n",
    "\n",
    "## Jupyter-Notebook\n",
    "\n",
    "Jupyter-Notebooks sind interaktive Python-Scripte, in denen Markdown und sogar Latex zur Dokumentation verwendet werden kann. In diesem Abschnitt sollen einerseits allgemeine Informationen wie Tastenkombinationen und übliche Workflows, andererseits aber auch für diesen Workshop und die verwendete Hardware spezifische Informationen im Umgang mit Python und Jupyter-Notebooks geliefert werden.\n",
    "\n",
    "### Wichtiges auf einen Blick (a.k.a. TL;DR)\n",
    "\n",
    "- **beim Wechseln auf ein anderes Notebook immer den Kernel beenden (Kernel -> Shutdown)**\n",
    "- **Shift + Enter** zum Ausführen der aktiven Zelle\n",
    "- **Strg + Shift + p** zum Öffnen der Kommandopalette\n",
    "- **Shift + o** zum Togglen des Zell-Scrollings\n",
    "- bei Fehlermeldungen im Zweifel **Kernel neustarten (Kernel -> Restart)**\n",
    "- \"Hilfe, ich sehe den Markdown-Code\" -> **Shift + Enter** in der entsprechenden Zelle\n",
    "- \"Hilfe, ich bekommen ResourceExhaustion / OutOfMemory (OOM) Fehler\" -> **Kernel neustarten, Kernel von anderen noch laufenden Notebooks herunterfahren** (oben links auf das Jupyter-Logo klicken, dann auf den Tab \"Running\")\n",
    "- \"Hilfe, mein Notebook ist kaputt\" -> siehe **Notebook \"Wiederherstellung\"**\n",
    "- \"Passiert da noch was?\" -> ist der **Kreis oben rechts neben \"Python 3\", ausgefüllt und dunkel** dann ist der Kernel noch am Arbeiten, ist er nicht gefüllt dann ist der Kernel untätig. Die aktuell laufende Zelle ist die von oben gesehen erste bei der auf der linken Seite \"In[*]\" anstelle von z.B. \"In[5]\" steht. Es kann aber passieren dass sich der Kernel aufhängt, in dem Fall einfach oben auf __Kernel -> Interrupt__ und die Zelle erneut ausführen\n",
    "\n",
    "\n",
    "### Überblick & Workflow\n",
    "\n",
    "Die einzigen beiden Shortcuts die man sich eigentlich nur merken muss sind\n",
    "\n",
    "- **Shift + Enter** zum Ausführen einer Zelle, und\n",
    "- **Strg + Shift + p** zum Öffnen der Kommandopalette, von der aus man dann direkt Zugriff auf alle möglichen Befehle hat, inklusive entsprechender Shortcuts\n",
    "\n",
    "Ein weiterer nützlicher Shortcut ist **Shift + o**, welcher das **Scrolling für Zellenoutput** umschaltet.\n",
    "\n",
    "Zum **Editieren einer Zelle** genügt ein Doppelklick in die Zelle, bei Code-Zellen reicht es zum Beenden des Editiermodus einfach außerhalb der Zelle zu klicken, bei Dokumentationszellen (wie dieser hier) ist eine Ausführung der Zelle nötig um die Code-Ansicht zu verlassen.\n",
    "\n",
    "Während eine Zelle ausgeführt wird, wechselt der Kernel-Indikator oben rechts neben \"Python 3\" von einem hellen Kreis mit dunklem Rand zu einem ausgefüllten dunklen Kreis und springt wieder zurück sobald die Ausführung beendet ist. Wenn mehrere Zellen gleichzeitig ausgeführt wurden, kann man an dem Label in der linken Spalte ablesen, ob die Zelle fertig ausgeführt wurde (**In [ZAHL]:**) oder ob sie gerade ausgeführt wird bzw. auf Ausführung wartet (**In [*]:**). Zusätzlich wird nach Ausführung einer Zelle die Zellenausgabe unterhalb der Zelle angezeigt.\n",
    "\n",
    "Um den Überblick zu behalten kann es manchmal sinnvoll sein, die **Zellenausgabe zu löschen**. Dies kann u.a. auf diesen beiden Wegen erfolgen:\n",
    "\n",
    "- oben auf Cell -> Current Outputs / All outputs -> Clear\n",
    "- oben auf Kernel -> Restart & Clear Output\n",
    "\n",
    "Der Kernel ist für die Ausführung des Python-Codes zuständig und behält den Kontext (also belegte Variablen, definierte Funktionen und belegter Speicher) seit dem letzten Kernel-(Neu)start. Dies kann zu Problemen führen wenn Zellen in anderer Reihenfolge ausgeführt werden oder Zellen übersprungen werden in denen Variablen oder Funktionen definiert werden die im weiteren Verlauf des Scripts benötigt werden, aber auch wenn **ein anderes Jupyter-Notebook gestartet wird**, da dafür ein weiterer Kernel gestartet wird.\n",
    "\n",
    "Deswegen beim **Wechseln auf ein anderes Notebook** immer den **Kernel herunterfahren oder neustarten** (oben Kernel -> Restart/Shutdown)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports und Tensorflow-Setup\n",
    "\n",
    "Neben den benötigten Imports wird wird hier Tensorflow angewiesen, nur maximal 6GB des GPU-RAMs zu belegen, damit mehrere Trainingsprozesse gleichzeitig auf jeder GPU ausgeführt werden können. Überspringt man diesen Schritt, dann reserviert Tensorflow den gesamten Grafikspeicher. Ein wichtiger Faktor für den beim Training benötigen Grafikspeicher ist die gewählte Batch-Größe, auf die später im Script noch genauer eingangen wird.\n",
    "\n",
    "Bitte diese Zahl nicht ändern, da mehrere Trainingsprozesse auf einer GPU ausgeführt werden und es zu Crashes kommen kann wenn der GPU-Speicher voll läuft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image as Image\n",
    "\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "\n",
    "\n",
    "# Tell TF to not use all GPU RAM\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=6*1024)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "    # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "\n",
    "from tensorflow.keras import models\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import Callback\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "import efficientnet.tfkeras as efn \n",
    "\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "print(\"Tensorflow setup done\")\n",
    "\n",
    "# Functions for visualization\n",
    "# See https://www.tensorflow.org/tutorials/images/hub_with_keras\n",
    "def image_to_network_input(filename):\n",
    "    im = Image.open(filename)\n",
    "    plt.imshow(np.asarray(im))\n",
    "\n",
    "    # Convert to RGB and resize\n",
    "    im_converted = im.convert(mode='RGB').resize((IMAGE_SIZE,IMAGE_SIZE))\n",
    "\n",
    "    # Convert to array and add batch dimension\n",
    "    X_test = np.array(im_converted)/255.0\n",
    "    X_test = X_test[np.newaxis, ...]\n",
    "    \n",
    "    return X_test\n",
    "\n",
    "def print_prediction_result(result):\n",
    "    print('Predictions:')\n",
    "    prediction_sum = np.sum(result)\n",
    "    i = 0\n",
    "    for x in np.nditer(result):\n",
    "        print('{}: {:.3f}%'.format(CLASSES[i], x/prediction_sum*100))\n",
    "        i+=1\n",
    "        \n",
    "    predicted_class = CLASSES[np.argmax(result[0], axis=-1)]\n",
    "    print('Predicted class: ' + predicted_class)\n",
    "\n",
    "def smooth(values, num_points = 100):\n",
    "    xnew = np.linspace(0,len(values)-1,num_points) #100 represents number of points to make between T.min and T.max\n",
    "    spl = make_interp_spline(list(range(len(values))), values, k=3) #BSpline object\n",
    "    values_smooth = spl(xnew)\n",
    "    \n",
    "    return xnew, values_smooth\n",
    "    \n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.loss = []\n",
    "        self.acc = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.loss.append(logs.get('loss'))\n",
    "        self.acc.append(logs.get('acc'))\n",
    "\n",
    "def find_last(model_directory):\n",
    "        \"\"\"Finds the last checkpoint file of the last trained model in the\n",
    "        model directory.\n",
    "        Returns:\n",
    "            The path of the last checkpoint file\n",
    "        \"\"\"\n",
    "        \n",
    "        checkpoints = next(os.walk(model_directory))[2]\n",
    "        checkpoints = filter(lambda f: f.endswith(\".hdf5\"), checkpoints)\n",
    "        checkpoints = sorted(checkpoints)\n",
    "        if not checkpoints:\n",
    "            import errno\n",
    "            raise FileNotFoundError(\n",
    "                errno.ENOENT, \"Could not find weight files in {}\".format(model_directory))\n",
    "        checkpoint = os.path.join(model_directory, checkpoints[-1])\n",
    "        return checkpoint\n",
    "\n",
    "    \n",
    "def show_example_predictions(dataset,IMAGES_TO_SHOW = 4, NUM_WRONG = 2):\n",
    "    i = 0\n",
    "    j = 0\n",
    "    for image_batch, label_batch in (iter(dataset)):\n",
    "        for n in range(len(image_batch)):\n",
    "            if(j>=NUM_WRONG):\n",
    "                if(i>=IMAGES_TO_SHOW):\n",
    "                    return\n",
    "            \n",
    "            result=model.predict(image_batch[n][np.newaxis, ...])\n",
    "            \n",
    "            predicted_label=CLASSES[np.argmax(result)]\n",
    "            label=class_names[label_batch[n].numpy()==1][0].title()\n",
    "            \n",
    "            if predicted_label.lower()!=label.lower():\n",
    "                j+=1\n",
    "                plt.figure(figsize=(10,10))\n",
    "                ax = plt.subplot()\n",
    "                plt.imshow(image_batch[n].numpy())\n",
    "                plt.title(\"Label: \"+label+\"      Prediction: \"+predicted_label)\n",
    "                plt.axis('off')\n",
    "                i+=1\n",
    "            elif j>=NUM_WRONG:\n",
    "                plt.figure(figsize=(10,10))\n",
    "                ax = plt.subplot()\n",
    "                plt.imshow(image_batch[n].numpy())\n",
    "                plt.title(\"Label: \"+label+\"      Prediction: \"+predicted_label)\n",
    "                plt.axis('off')\n",
    "                i+=1\n",
    "            \n",
    "            \n",
    "print(\"Imports and setup done\")\n",
    "\n",
    "ROOT_DIR = os.path.abspath(\"/workspace\")\n",
    "DATA_DIR = os.path.abspath(\"/data\")\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"models/classification\")\n",
    "PRETRAINED_MODEL_DIR = os.path.abspath(\"/models\")\n",
    "\n",
    "# Solutions\n",
    "#NUM_CLASSES =\n",
    "#LEARN_RATE = \n",
    "#CONV_BASE_TRAINABLE = \n",
    "\n",
    "print(\"Directories:\")\n",
    "print(\"Root directory:\", ROOT_DIR)\n",
    "print(\"Model directory:\", MODEL_DIR)\n",
    "print(\"Pre-trained model directory:\", PRETRAINED_MODEL_DIR)\n",
    "print(\"Datasets directory:\", DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die wichtigsten Ordner sind:\n",
    "\n",
    "- `ROOT_DIR`: Das Hauptverzeichnis in dem sich die Jupyter-Notebooks befinden\n",
    "- `MODEL_DIR`: Verzeichnis in dem trainierte Modelle abgespeichert werden\n",
    "- `PRETRAINED_MODEL_DIR`: Verzeichnis für vortrainierte Modelle (read-only)\n",
    "- `DATA_DIR`: Verzeichnis in dem sich die Datensätze befinden\n",
    "\n",
    "Hier kann mit einfachen Linux-Kommandozeilenbefehlen die Ordnerstruktur untersucht werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l /data\n",
    "!ls -l /data/MURA-v1.1\n",
    "!ls -l /data/MURA-v1.1/train\n",
    "!ls -l /workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Datensatz\n",
    "\n",
    "Hier wird der Datensatz definiert. Es gibt in Keras und Tensorflow viele verschiedene Wege Daten einzulesen, in diesem Beispiel wird ein [tf.data.Dataset](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) verwendet. Die verschiedenen Optionen zur Data Augmentation können unter dem o.g. Link nachgelesen werden und können später zur Optimierung genutzt werden.\n",
    "\n",
    "In unserem Beispiel ist der Name des Datensatz-Ordners `MURA-v1.1` und die möglichen __Splits__ sind `train` und `valid`, die Klassen entsprechen den Ordnernamen innerhalb der Unterverzeichnisse `train/` und `valid/`.\n",
    "\n",
    "Eine wichtige Variable hier ist `BATCH_SIZE`, welche bestimmt wie viele Bilder gleichzeitig in den Grafikspeicher geladen werden müssen. Wählt man diesen Wert zu hoch, so wird der reservierte Grafikspeicher nicht ausreichen, wählt man ihn zu niedrig so wird das Training sehr langsam. Für unser Beispiel soll dieser Wert auf 42 gesetzt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 42 # 16 with ResNet50\n",
    "IMAGE_SIZE = 224 #128 with ResNet50\n",
    "PRE_CROPPED_SIZE = 250 #150 with ResNet50\n",
    "\n",
    "# Dataset directory\n",
    "DATASET_DIR = '/data/MURA-v1.1/'\n",
    "\n",
    "CLASSES = ['XR_ELBOW', 'XR_FINGER', 'XR_FOREARM', 'XR_HAND', 'XR_HUMERUS', 'XR_SHOULDER', 'XR_WRIST']\n",
    "class_names=np.array(CLASSES)\n",
    "\n",
    "#loading image paths from csv\n",
    "train_paths=[]\n",
    "valid_paths=[]\n",
    "\n",
    "with open (DATASET_DIR+\"train_image_paths.csv\") as file:\n",
    "    train_paths=file.read().splitlines()\n",
    "    \n",
    "with open (DATASET_DIR+\"valid_image_paths.csv\") as file:\n",
    "    valid_paths=file.read().splitlines()  \n",
    "    \n",
    "train_paths=[\"/data/\"+s for s in train_paths]\n",
    "valid_paths=[\"/data/\"+s for s in valid_paths]\n",
    "\n",
    "print(len(train_paths),\"train images\")\n",
    "print(len(valid_paths),\"valid images\")\n",
    "\n",
    "#tensor with classnames for comparison on GPU\n",
    "class_tensor=tf.constant(class_names)\n",
    "\n",
    "#returns a label[] for given file\n",
    "def get_label(file_path):\n",
    "    return tf.strings.split(file_path, os.sep)[-4]==class_tensor\n",
    "#prints a batch\n",
    "def show_batch(image_batch, label_batch):\n",
    "    plt.figure(figsize=(10,10))\n",
    "    for n in range(BATCH_SIZE):\n",
    "        ax = plt.subplot(5,5,n+1)\n",
    "        plt.imshow(image_batch[n])\n",
    "        plt.title(class_names[label_batch[n]==1][0].title())\n",
    "        plt.axis('off')\n",
    "        \n",
    "#image to tensor\n",
    "def decode_img(img):\n",
    "  # convert the compressed string to a 3D uint8 tensor\n",
    "    img = tf.image.decode_image(img, channels=3,expand_animations=False)\n",
    "  # Use `convert_image_dtype` to convert to floats in the [0,1] range.\n",
    "    img = tf.image.convert_image_dtype(img, tf.float32)\n",
    "  # resize the image to the desired size.\n",
    "    #return img\n",
    "    return tf.image.resize(img, [PRE_CROPPED_SIZE, PRE_CROPPED_SIZE])\n",
    "\n",
    "#loads, decodes and augments images and labels\n",
    "def process_path_train(file_path):\n",
    "    label = get_label(file_path)\n",
    "    \n",
    "  # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    \n",
    "    #data augmentation\n",
    "    img=tf.image.random_crop(img, [IMAGE_SIZE, IMAGE_SIZE, 3],seed=42)\n",
    "    img=tfa.image.rotate(img,tf.random.uniform(shape=[],dtype=tf.float32,minval=-90, maxval=90,seed=42))\n",
    "    img=tf.image.random_flip_left_right(img, seed=42)\n",
    "    img=tf.image.random_flip_up_down(img, seed=42)\n",
    "    img=tf.image.random_brightness(img,0.4,seed=42)\n",
    "    img=tf.image.random_saturation(img,0.4,2.5,seed=42)\n",
    "        \n",
    "    return img, label \n",
    "\n",
    "#no augmentation on validation set\n",
    "def process_path_validation(file_path):\n",
    "    label = get_label(file_path)\n",
    "  # load the raw data from the file as a string\n",
    "    img = tf.io.read_file(file_path)\n",
    "    img = decode_img(img)\n",
    "    \n",
    "    img = tf.image.resize(img,[IMAGE_SIZE,IMAGE_SIZE])\n",
    "        \n",
    "    return img, label \n",
    "\n",
    "#handles image IO for GPU\n",
    "def prepare_for_training(ds, cache=False, shuffle_buffer_size=1000,shuffle=True):\n",
    "    if cache:\n",
    "        if isinstance(cache, str):\n",
    "            ds = ds.cache(cache)\n",
    "        else:\n",
    "            ds = ds.cache()\n",
    "\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=shuffle_buffer_size)\n",
    "          # Repeat forever\n",
    "        #ds = ds.repeat()\n",
    "\n",
    "\n",
    "    ds = ds.batch(BATCH_SIZE)\n",
    "\n",
    "  # `prefetch` lets the dataset fetch batches in the background while the model\n",
    "  # is training.\n",
    "    #prefetch(int)->number of images\n",
    "    ds = ds.prefetch(AUTOTUNE)\n",
    "\n",
    "    return ds\n",
    "\n",
    "\n",
    "#make a dataset from files\n",
    "list_train_ds = tf.data.Dataset.list_files(train_paths)\n",
    "list_valid_ds = tf.data.Dataset.list_files(valid_paths)\n",
    "\n",
    "#define operations on images\n",
    "labeled_train_ds = list_train_ds.map(process_path_train,num_parallel_calls=AUTOTUNE)\n",
    "labeled_valid_ds = list_valid_ds.map(process_path_validation,num_parallel_calls=AUTOTUNE)\n",
    "\n",
    "#generate final dataset, only shuffle training data\n",
    "train_ds = prepare_for_training(labeled_train_ds,shuffle=True)\n",
    "valid_ds = prepare_for_training(labeled_valid_ds,shuffle=False)\n",
    "\n",
    "\n",
    "print(\"Datensatz generiert.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Bilder anzeigen lassen\n",
    "\n",
    "Im Folgedenen werden Bilder aus dem (Validierungs-)Datensatz in Originalauflösung extrahiert. Um stattdessen Bilder aus dem Trainingsdatensatz anzuzeigen, einfach das `'valid_ds'` in der ersten Zeile zu `'train_ds'` ändern.\n",
    "\n",
    "Die Bilder aus dem Trainingsdatensatz sind augmentiert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_batch, label_batch = next(iter(valid_ds))\n",
    "show_batch(image_batch.numpy(), label_batch.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='from_scratch'></a>\n",
    "\n",
    "# 3. Modell-Erstellung (from scratch)\n",
    "\n",
    "Falls Sie das Training from Scratch bereits durchgeführt haben, klicken Sie [hier](#pre_trained) um zum nächsten Schritt zu springen.\n",
    "\n",
    "Als nächstes wird das Modell erstellt. \n",
    "`conv_base` enthält das Basis-Modell, in diesem Fall ein [EfficientNet B0](https://github.com/qubvel/efficientnet), andere Möglichkeiten sind auf der [keras.io](https://keras.io/applications/)-Webseite aufgelistet (dafür müssen aber auch oben die Imports angepasst werden).\n",
    "\n",
    "Auf das Basis-Modell wird eine neue fully-connected Schicht mit 1000 Knoten gesetzt, und darüber eine weitere fully-connected Ausgabeschicht deren Anzahl Knoten der Anzahl Klassen entspricht.\n",
    "\n",
    "Abschließend wird das Modell kompiliert, wobei die Loss-Funktion, ein Optimizer (hier kann die Lernrate angepasst werden, s.u.), und die zu generierenden Metriken übergeben werden.\n",
    "\n",
    "Hier wird direkt das komplette Modell trainiert, da alle Schichten zufällig initialisiert wurden und ein zweiphasiges Training daher nicht sinnvoll ist.\n",
    "\n",
    "### Optimizer und Lernrate\n",
    "\n",
    "Keras unterstützt [eine Reihe](https://keras.io/optimizers/) von Optimizern mit verschiedenen Parametern. Optimizer sind für die Anpassung der Gewichte zuständig. Wir werden für unser Training zwei verschiedene Optimizer nutzen:\n",
    "\n",
    "- [Adam](https://arxiv.org/abs/1412.6980v8)\n",
    "- [Stochastic Gradient Descent (SGD)](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)\n",
    "\n",
    "Die wichtigsten Parameter die von beiden Optimizern unterstützt werden:\n",
    "\n",
    "- `lr`, die Lernrate\n",
    "- `decay`, learning rate decay\n",
    "\n",
    "Für die Lernrate liegen übliche zwischen `0.001` und `0.00001`. Größere Lernrate ermöglichen schnelleres Training, können aber auch zu problematischen Sprüngen im Lösungsraum und Divergenz (Fehler gleichbleibend schlecht) führen. Kleinere Lernraten eignen sich für das Fine-Tuning.\n",
    "\n",
    "Der learning rate decay sorgt dafür, dass die Lernrate im Laufe des Trainings immer geringer wird. Die Idee ist mit einer großen Lernrate zu starten um so schnell in die Nähe eines lokalen Minimums zu gelangen, und dann mit einer geringeren Lernrate diesem lokalen Minimum möglichst nahe zu kommen. Der Wert sollte hier abhängig von der Anzahl Epochen und Batches gewählt werden. Die Formel für die Berechnung der aktuellen Lernrate ist\n",
    "\n",
    "\\begin{equation*}\n",
    "l = l_{init} * \\frac{1}{1 + d * i}\n",
    "\\end{equation*}\n",
    "\n",
    "mit $l$ = Lernrate, $l_{init}$ = initiale Lernrate, $d$ = decay, $i$ = Iterationen (Anzahl verarbeiteter Batches).\n",
    "\n",
    "Hier ein paar Beispiele mit verschiedenen Lernraten und $d = 0.00025$:\n",
    "\n",
    "<style type=\"text/css\" rel=\"stylesheet\">\n",
    ".foo table { width: 300px; }\n",
    "</style>\n",
    "<div class=\"foo\">\n",
    "    \n",
    "| Epoche | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Formel &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | $l$ bei $l_{init} = 0.001$ | $l$ bei $l_{init} = 0.0005$ |\n",
    "| ---------- | ---------- | ---------- | ---------- |\n",
    "| 0 | $l_{init} * \\frac{1}{1 + 0.00025 * 0*2300}$ | &nbsp; &nbsp; 0.001 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | &nbsp; &nbsp; &nbsp; 0.0005 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |\n",
    "| 1 | $l_{init} * \\frac{1}{1 + 0.00025 * 1*2300}$ | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.000634921 | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00031746 &nbsp; |\n",
    "| 2 | $l_{init} * \\frac{1}{1 + 0.00025 * 2*2300}$ | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.000465116 | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  0.000232558 |\n",
    "| 3 | $l_{init} * \\frac{1}{1 + 0.00025 * 3*2300}$ | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.000366972 | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  0.000183486 |\n",
    "| 4 | $l_{init} * \\frac{1}{1 + 0.00025 * 4*2300}$ | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.000303030 | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  0.000151515 |\n",
    "| 5 | $l_{init} * \\frac{1}{1 + 0.00025 * 5*2300}$ | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.000258065 | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  0.000129032 |\n",
    "\n",
    "</div>\n",
    "\n",
    "Aufgaben:\n",
    "\n",
    "- Ersetzen Sie die Variable `NUM_CLASSES` durch die Anzahl Klassen\n",
    "- Ersetzen Sie die Variable `LEARN_RATE` durch eine geeignete Lernrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Create model\n",
    "##\n",
    "\n",
    "print(\"Creating model\")\n",
    "\n",
    "os.makedirs(MODEL_DIR, 0o777, exist_ok=True)\n",
    "print(\"model_dir: \", os.path.realpath(MODEL_DIR))\n",
    "\n",
    "model_file = os.path.join(MODEL_DIR, \"model_from_scratch.h5\")\n",
    "\n",
    "# Create Keras model\n",
    "# see https://keras.io/applications/\n",
    "conv_base = efn.EfficientNetB0(\n",
    "                weights=None,\n",
    "                include_top=False,\n",
    "                input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3)\n",
    "            )\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "# Add more layers here if needed\n",
    "model.add(layers.Dense(1000, activation='relu'))\n",
    "# The number of nodes in the last layer should be equal to the number of classes\n",
    "model.add(layers.Dense(NUM_CLASSES, activation='softmax'))\n",
    "# Enable training for the base model\n",
    "conv_base.trainable = True\n",
    "\n",
    "# Here you can adjust the learning rate\n",
    "# see https://keras.io/optimizers/\n",
    "optimizer = tf.keras.optimizers.Adam(lr=LEARN_RATE, decay=0.0025)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"Model successfully created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Training (from scratch)\n",
    "\n",
    "Das eigentliche Training wird über die Funktion `fit` gestartet, welchem die zuvor erstellten Datensätze übergeben werden.\n",
    "\n",
    "Dies kann einige Zeit dauern. Ob der Kernel im Hintergrund noch aktiv ist lässt sich in dem Kreis oben rechts erkennen – solange der Kreis ausgefüllt ist, läuft das Script noch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start training from scratch\")\n",
    "#filepath = os.path.join(MODEL_DIR, \"from_scratch_{epoch:02d}-{val_loss:.2f}.hdf5\")\n",
    "#model_checkpoint = ModelCheckpoint(filepath)\n",
    "history=model.fit(\n",
    "        train_ds, \n",
    "        epochs=2,\n",
    "        validation_data=valid_ds\n",
    "        )\n",
    "\n",
    "#models.save_model(model, filepath = model_file)\n",
    "\n",
    "#print(\"Finished training, model saved to: \" + MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Ausgabe des Trainingsscripts enthält folgende Informationen:\n",
    "\n",
    "- `8/2300`: Das aktuelle Minibatch. Wie oben beschrieben ergibt sich die Zahl 30 aus der Anzahl Trainingsbilder durch die Batch-Größe: `36800 / 16 = 2300`\n",
    "- `ETA: 1:55`: Erwartete Restzeit\n",
    "- `413s 180ms/step`: Gesamtdauer für die Trainingsepoche sowie durchschnittliche Dauer eines Minibatches\n",
    "- `loss: 1.3854`: Wert der zu minimierenden Verlustfunction (in diesem Fall [Cross Entropy](https://en.wikipedia.org/wiki/Cross_entropy)) auf dem letzten Minibatch\n",
    "- `acc: 0.4926`: Accuracy auf dem aktuellen Trainings-Minibatch, also prozentualer Anteil korrekt klassifizierter Bilder\n",
    "- `val_loss: 1.5387`: Wert der Verlustfunction auf den gesamten Validierungsdaten, wird nur einmal pro Epoche generiert\n",
    "- `val_acc: 0.5148`: Accuracy auf den gesamten Validierungsdaten, wird nur einmal pro Epoche generiert\n",
    "\n",
    "Wichtig ist hier ein Vergleich zwischen den Werten für die Trainings- und Validierungsdaten. Wenn diese Werte weit auseinander gehen dann liegt Overfitting vor, die Trainingsdaten werden also auswendig gelernt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='visualization'></a>\n",
    "## 3.2 Visualisierung (from scratch)\n",
    "\n",
    "Nach dem Training kann mit der `LossHistory`-Instanz eine Visualisierung der Klassifikationsergebnisse stattfinden.\n",
    "\n",
    "Es werden hier nur die Loss und Accuracy der Trainingsbatches angezeigt, da der Validierungsfehler nur einmal am Ende jeder Epoche (in diesem Fall also insgesmat nur ein mal) berechnet wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#insert values if history contains only 1 epoch to plot graph\n",
    "if len(history.history['accuracy'])==1:\n",
    "    for i in history.history:\n",
    "        history.history[i].insert(0,history.history[i][0])\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Einzelinferenz (Trainingsbilder, from scratch)\n",
    "\n",
    "Spannender hingegen ist die anschließende Einzelinferenz: was genau macht unser Netz aus bestimmten Bildern der Trainingsmenge?\n",
    "\n",
    "Es werden hier vier (Variable `IMAGES_TO_SHOW`) zufällige Bilder aus der Trainingsmenge genommen.\n",
    "\n",
    "Über die Variable `NUM_WRONG` kann festgelegt werden, wie viele falsch klassifizierte Bilder mindestens ausgewählt werden sollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_example_predictions(train_ds,IMAGES_TO_SHOW = 4, NUM_WRONG = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier wird das gleiche für Bilder aus dem Validierungsdatensatz durchgeführt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_example_predictions(valid_ds,IMAGES_TO_SHOW = 4, NUM_WRONG = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pre_trained'></a>\n",
    "# 4. Modell-Erstellung (pre-trained)\n",
    "\n",
    "Nachdem wir uns das Training from scratch angesehen haben soll nun ein Training basierend auf einem vortrainierten Modell stattfinden. Die in Keras mitgelieferten Modelle sind mit dem ImageNet-Datensatz vortrainiert. Um ein vortrainiertes Netz zu nutzen muss bei der Erstellung der `conv_base` der `weights` Parameter auf `imagenet` gesetzt werden.\n",
    "\n",
    "Wichtig ist hierbei `conv_base.trainable` zunächst auf `False` zu setzen, wodurch nur die neu hinzugefügten Schichten für eine Epoche trainierten werden.\n",
    "\n",
    "Falls der Validierungsfehler nach dem ersten Trainingsschritt sehr schlecht ist, dann ist das kein Grund zur Sorge – dieser wird spätestens nach dem zweiten Trainingsschritt dem Trainingsfehler sehr ähnlich sein.\n",
    "\n",
    "Falls ein `ResourceExhaustion` bzw. `OOM`-Error auftritt, muss der Kernel neustartet werden `\"restart the kernel (with dialog)\"`. Anschließend alle Zellen einschließlich der Datensatzerstellung (aber nicht das Training from scratch) ausführen und dann hier fortfahren. Klicken Sie dazu [hier](#from_scratch), dann oben auf `Cell -> Run all above`, und anschließend auf den Link zum Überspringen des Schrittes, um wieder hierhin zu gelangen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Creating pre-trained model, this may take a while...')\n",
    "\n",
    "# Clear session memory to avoid OOM\n",
    "K.clear_session()\n",
    "\n",
    "# Create Keras model\n",
    "# see https://keras.io/applications/\n",
    "conv_base = efn.EfficientNetB0(weights='noisy-student',\n",
    "                  include_top=False,\n",
    "                  input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "# Add more layers here if needed\n",
    "model.add(layers.Dense(1000, activation='relu'))\n",
    "# The number of nodes in the last layer should be equal to the number of classes\n",
    "model.add(layers.Dense(7, activation='softmax'))\n",
    "\n",
    "print(\"Model created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Training neue Schichten (pre-trained)\n",
    "\n",
    "\n",
    "Aufgaben:\n",
    "\n",
    "- Frieren Sie die Gewichte der vortrainierten Schichten über den Parameter `conv_base.trainable` ein, indem Sie die Variable `CONV_BASE_TRAINABLE` durch `True` oder `False` ersetzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable training for the pre-trained model\n",
    "conv_base.trainable = False\n",
    "\n",
    "# Here you can adjust the learning rate\n",
    "# see https://keras.io/optimizers/\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.0001, decay=0.00025, clipnorm=1.)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Training new model')\n",
    "\n",
    "\n",
    "print(\"Start training: only new layers\")\n",
    "# First train only top layers\n",
    "history=model.fit(\n",
    "        train_ds, \n",
    "        epochs=1,\n",
    "        validation_data=valid_ds\n",
    "        )\n",
    "\n",
    "#models.save_model(model, filepath = model_file)\n",
    "#evaluation = model.evaluate_generator(train_generator, NUM_TRAINING_SAMPLES // BATCH_SIZE)\n",
    "#print(\"Training loss:\", evaluation[0], \"\\nTraining accuracy: {:.2f}%\".format(evaluation[1]*100))\n",
    "#evaluation = model.evaluate_generator(validation_generator, NUM_VALIDATION_SAMPLES // BATCH_SIZE)\n",
    "#print(\"Validation loss:\", evaluation[0], \"\\nValidation accuracy: {:.2f}%\".format(evaluation[1]*100))\n",
    "#print(\"Finished training, model saved to: \" + MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='visualization'></a>\n",
    "## 4.2 Visualisierung nach Training neuer Schichten (pre-trained)\n",
    "\n",
    "Nach dem Training kann mit der `LossHistory`-Instanz eine Visualisierung der Klassifikationsergebnisse stattfinden.\n",
    "\n",
    "Diese Visualisierung zeigt jederzeit den Verlauf des zuletzt durchgeführten Trainings an, Sie können also nach jedem Training hierhin zurück kehren und diesen Code-Block erneut ausführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#insert values if history contains only 1 epoch to plot graph\n",
    "if len(history.history['accuracy'])==1:\n",
    "    for i in history.history:\n",
    "        history.history[i].insert(0,history.history[i][0])\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Einzelinferenz nach Training neuer Schichten (pre-trained)\n",
    "\n",
    "Spannender hingegen ist die anschließende Einzelinferenz: was genau macht unser Netz aus bestimmten Bildern der Trainingsmenge?\n",
    "\n",
    "Es werden hier vier (Variable `IMAGES_TO_SHOW`) zufällige Bilder aus der Trainingsmenge genommen.\n",
    "\n",
    "Über die Variable `NUM_WRONG` kann festgelegt werden, wie viele falsch klassifizierte Bilder mindestens ausgewählt werden sollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_example_predictions(train_ds,IMAGES_TO_SHOW = 4, NUM_WRONG = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier wird das gleiche für Bilder aus dem Validierungsdatensatz durchgeführt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_example_predictions(valid_ds,IMAGES_TO_SHOW = 5, NUM_WRONG = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Fine-Tuning (pre-trained)\n",
    "\n",
    "\n",
    "Im zweiten Schritt wird nun das gesamte Modell für eine Epoche trainiert (`conv_base.trainable` wird auf `True` gesetzt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.trainable = True\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.00001, decay=0.00025, clipnorm=1.)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "try: history\n",
    "except NameError: history = LossHistory()\n",
    "\n",
    "\n",
    "print(\"Start training from pre-trained: fine-tuning all layers\")\n",
    "history=model.fit(\n",
    "        train_ds, \n",
    "        epochs=20,\n",
    "        validation_data=valid_ds\n",
    "        )\n",
    "\n",
    "#evaluation = model.evaluate_generator(train_generator, NUM_TRAINING_SAMPLES // BATCH_SIZE)\n",
    "#print(\"Training loss:\", evaluation[0], \"\\nTraining accuracy: {:.2f}%\".format(evaluation[1]*100))\n",
    "#evaluation = model.evaluate_generator(validation_generator, NUM_VALIDATION_SAMPLES // BATCH_SIZE)\n",
    "#print(\"Validation loss:\", evaluation[0], \"\\nValidation accuracy: {:.2f}%\".format(evaluation[1]*100))\n",
    "#print(\"Finished training, model saved to: \" + MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Visualisierung Fine-Tuning (pre-trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert values if history contains only 1 epoch to plot graph\n",
    "if len(history.history['accuracy'])==1:\n",
    "    for i in history.history:\n",
    "        history.history[i].insert(0,history.history[i][0])\n",
    "\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Führen Sie nun die finale Einzelinferenz auf den Trainings-/Validierungsdaten selbst durch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out of Class Bilder\n",
    "\n",
    "Zum Test wie unser Modell mit Bildern umgeht die es noch nie gesehen hat, geben wir ihm nun das folgende Schädel-CT-Bild:\n",
    "\n",
    "<img src=\"https://prod-images.static.radiopaedia.org/images/17058746/a60ee77e6b44da2d72a25cea08dfd2_big_gallery.jpeg\" />\n",
    "Quelle: radiopaedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_generator = ImageDataGenerator().flow_from_directory(DATA_DIR + '/ooc', batch_size=1, target_size=(IMAGE_SIZE,IMAGE_SIZE), class_mode='categorical')\n",
    "i=0\n",
    "for x_batch, y_batch in example_generator:\n",
    "    \n",
    "    x_input = x_batch[0]/255.\n",
    "    x_input = x_input[np.newaxis, ...]\n",
    "    result = model.predict(x_input)\n",
    "    \n",
    "    predicted_class = CLASSES[np.argmax(result[0], axis=-1)]\n",
    "    correct_class = CLASSES[np.where(y_batch[0] == 1.)[0][0]]\n",
    "    \n",
    "    print(\"\\n\\nImage\", (i+1))\n",
    "    plt.figure()\n",
    "    plt.imshow(x_batch[0].astype(int))\n",
    "    \n",
    "    \n",
    "    print_prediction_result(result)\n",
    "    plt.show()\n",
    "    i+=1\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "## Run 1\n",
    "Eine Epoche neue Schichten trainiert:<br/>\n",
    "optimizer = tf.keras.optimizers.Adam(lr=0.001, decay=0.0025, clipnorm=1.)<br/>\n",
    "``loss: 1.9823 - accuracy: 0.7639 - val_loss: 0.6412 - val_accuracy: 0.7879``\n",
    "\n",
    "10 Epochen das gesamte Netzwerk trainiert:<br/>\n",
    "optimizer = tf.keras.optimizers.SGD(lr=0.0001, decay=0.00025, clipnorm=1.)\n",
    "``loss: 0.2861 - accuracy: 0.9077 - val_loss: 0.3449 - val_accuracy: 0.8943``\n",
    "\n",
    "## Run 2\n",
    "Es wurde das Efficientnet B4 mit einer Bildgröße von 380x380 verwendet. Anschließend wurden 20 Epochen trainiert.<br/>\n",
    "Pretraining:<br/>\n",
    "``loss: 0.8445 - accuracy: 0.7323 - val_loss: 0.7250 - val_accuracy: 0.7482``\n",
    "\n",
    "Finetuning:<br/>\n",
    "``loss: 0.1384 - accuracy: 0.9584 - val_loss: 0.1925 - val_accuracy: 0.9537``<br/>\n",
    "Das Training dauerte nur zwei Stunden."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
