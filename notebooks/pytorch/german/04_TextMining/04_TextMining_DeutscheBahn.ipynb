{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stimmungsanalyse mit dem DB-Social-Media Datensatz\n",
    "\n",
    "Der Datensatz wurde zur Stimmungsanalyse und Erkennung von relevanten Feedback erstellt.\n",
    "Weitere Dokumentation und Downloads sind unter https://sites.google.com/view/germeval2017-absa/data?authuser=0 zu finden.\n",
    "\n",
    "\n",
    "Der Datensatz wird mittels Logistischer Regression, FastText und Bert bearbeitet.\n",
    "# Jupyter-Notebook\n",
    "\n",
    "Jupyter-Notebooks sind interaktive Python-Scripte, in denen Markdown und sogar Latex zur Dokumentation verwendet werden kann. In diesem Abschnitt sollen einerseits allgemeine Informationen wie Tastenkombinationen und übliche Workflows, andererseits aber auch für diesen Workshop und die verwendete Hardware spezifische Informationen im Umgang mit Python und Jupyter-Notebooks geliefert werden.\n",
    "\n",
    "### Wichtiges auf einen Blick (a.k.a. TL;DR)\n",
    "\n",
    "- **beim Wechseln auf ein anderes Notebook immer den Kernel beenden (Kernel -> Shutdown)**\n",
    "- **Shift + Enter** zum Ausführen der aktiven Zelle\n",
    "- **Strg + Shift + p** zum Öffnen der Kommandopalette\n",
    "- **Shift + o** zum Togglen des Zell-Scrollings\n",
    "- bei Fehlermeldungen im Zweifel **Kernel neustarten (Kernel -> Restart)**\n",
    "- \"Hilfe, ich sehe den Markdown-Code\" -> **Shift + Enter** in der entsprechenden Zelle\n",
    "- \"Hilfe, ich bekommen ResourceExhaustion / OutOfMemory (OOM) Fehler\" -> **Kernel neustarten, Kernel von anderen noch laufenden Notebooks herunterfahren** (oben links auf das Jupyter-Logo klicken, dann auf den Tab \"Running\")\n",
    "- \"Hilfe, mein Notebook ist kaputt\" -> siehe **Notebook \"Wiederherstellung\"**\n",
    "- \"Passiert da noch was?\" -> ist der **Kreis oben rechts neben \"Python 3\", ausgefüllt und dunkel** dann ist der Kernel noch am Arbeiten, ist er nicht gefüllt dann ist der Kernel untätig. Die aktuell laufende Zelle ist die von oben gesehen erste bei der auf der linken Seite \"In[*]\" anstelle von z.B. \"In[5]\" steht. Es kann aber passieren dass sich der Kernel aufhängt, in dem Fall einfach oben auf __Kernel -> Interrupt__ und die Zelle erneut ausführen\n",
    "\n",
    "\n",
    "### Überblick & Workflow\n",
    "\n",
    "Die einzigen beiden Shortcuts die man sich eigentlich nur merken muss sind\n",
    "\n",
    "- **Shift + Enter** zum Ausführen einer Zelle, und\n",
    "- **Strg + Shift + p** zum Öffnen der Kommandopalette, von der aus man dann direkt Zugriff auf alle möglichen Befehle hat, inklusive entsprechender Shortcuts\n",
    "\n",
    "Ein weiterer nützlicher Shortcut ist **Shift + o**, welcher das **Scrolling für Zellenoutput** umschaltet.\n",
    "\n",
    "Zum **Editieren einer Zelle** genügt ein Doppelklick in die Zelle, bei Code-Zellen reicht es zum Beenden des Editiermodus einfach außerhalb der Zelle zu klicken, bei Dokumentationszellen (wie dieser hier) ist eine Ausführung der Zelle nötig um die Code-Ansicht zu verlassen.\n",
    "\n",
    "Während eine Zelle ausgeführt wird, wechselt der Kernel-Indikator oben rechts neben \"Python 3\" von einem hellen Kreis mit dunklem Rand zu einem ausgefüllten dunklen Kreis und springt wieder zurück sobald die Ausführung beendet ist. Wenn mehrere Zellen gleichzeitig ausgeführt wurden, kann man an dem Label in der linken Spalte ablesen, ob die Zelle fertig ausgeführt wurde (**In [ZAHL]:**) oder ob sie gerade ausgeführt wird bzw. auf Ausführung wartet (**In [*]:**). Zusätzlich wird nach Ausführung einer Zelle die Zellenausgabe unterhalb der Zelle angezeigt.\n",
    "\n",
    "Um den Überblick zu behalten kann es manchmal sinnvoll sein, die **Zellenausgabe zu löschen**. Dies kann u.a. auf diesen beiden Wegen erfolgen:\n",
    "\n",
    "- oben auf Cell -> Current Outputs / All outputs -> Clear\n",
    "- oben auf Kernel -> Restart & Clear Output\n",
    "\n",
    "Der Kernel ist für die Ausführung des Python-Codes zuständig und behält den Kontext (also belegte Variablen, definierte Funktionen und belegter Speicher) seit dem letzten Kernel-(Neu)start. Dies kann zu Problemen führen wenn Zellen in anderer Reihenfolge ausgeführt werden oder Zellen übersprungen werden in denen Variablen oder Funktionen definiert werden die im weiteren Verlauf des Scripts benötigt werden, aber auch wenn **ein anderes Jupyter-Notebook gestartet wird**, da dafür ein weiterer Kernel gestartet wird.\n",
    "\n",
    "Deswegen beim **Wechseln auf ein anderes Notebook** immer den **Kernel herunterfahren oder neustarten** (oben Kernel -> Restart/Shutdown)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, datetime, random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import fasttext\n",
    "import gensim\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import Adam\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "import transformers\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import data_cleaner # pyscript data_cleaner.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.abspath(\"./\")\n",
    "DATA_DIR = os.path.abspath(f\"./dataset/\")\n",
    "\n",
    "TRAIN_DF= f\"{DATA_DIR}/db_twitter/train_v1.4.tsv\"\n",
    "TEST_DF= f\"{DATA_DIR}/db_twitter/dev_v1.4.tsv\"\n",
    "CLEAN_TRAIN_DF= f\"{DATA_DIR}/db_twitter/train_clean.tsv\"\n",
    "CLEAN_TEST_DF= f\"{DATA_DIR}/db_twitter/test_clean.tsv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Datenanalyse\n",
    "Zur Analyse laden wir den Testdatensatz ein und schauen uns die ersten fünf Zeilen an."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die ersten 5 Zeilen in der Trainingsmenge:\n",
      "                    0                   1      2         3                   4\n",
      "0  http://twitter....  @DB_Bahn ja, we...   True   neutral  Allgemein#Haupt...\n",
      "1  http://twitter....  @nordschaf theo...   True  positive  Zugfahrt#Streck...\n",
      "2  http://twitter....  Bahn verspätet ...   True  negative  Zugfahrt#Pünktl...\n",
      "3  http://wirtscha...  Ihre Anfragen b...  False   neutral                 NaN\n",
      "4  http://communit...  Kann ich mit de...   True   neutral  Allgemein#Haupt... \n",
      "\n",
      "Die ersten 5 Zeilen in der Testmenge:\n",
      "                    0                   1      2         3                   4\n",
      "0  http://www.face...  Re: Das Erste \"...   True  negative  Allgemein#Haupt...\n",
      "1  http://www.luft...  Effektiver Part...  False   neutral                 NaN\n",
      "2  http://twitter....  #TelMi #telmi L...   True  negative  Sicherheit#Haup...\n",
      "3  http://twitter....  @KuttnerSarah @...   True  negative  Gepäck#Haupt:ne...\n",
      "4  http://twitter....  Probleme bei de...   True  negative  Allgemein#Haupt... \n",
      "\n",
      "Nachricht der ersten Zeile der Trainingsmenge:\n",
      "@DB_Bahn ja, weil in Wuppertal Bauarbeiten sind, soweit bin ich auch, aber wieso nur am Wochenende und grade jetzt?\n",
      "\n",
      "Nachricht der ersten Zeile der Testmenge:\n",
      "Re: Das Erste \"Ich fahre nicht mit der Bahn. Nach Erzählungen von Freunden, scheint es ein zentrales Problem zu sein. Es kann nicht sei, dass jemand in Hannover umsteigen muss, um in einen Zug , auch ohne Klimaanlage nach Berlin zu fahren. --- Es war aber bei dem letzten \"\"Hitzehoch\"\". --- Das liegt am Hersteller.\"\n"
     ]
    }
   ],
   "source": [
    "analyse_train = pd.read_csv(TRAIN_DF, delimiter='\\t', header=None)\n",
    "analyse_test = pd.read_csv(TEST_DF, delimiter='\\t', header=None)\n",
    "\n",
    "with pd.option_context('display.max_colwidth', 19):\n",
    "    print(\"Die ersten 5 Zeilen in der Trainingsmenge:\")\n",
    "    print(analyse_train[:5],\"\\n\")\n",
    "    print(\"Die ersten 5 Zeilen in der Testmenge:\")\n",
    "    print(analyse_test[:5], \"\\n\")\n",
    "    \n",
    "print(f\"Nachricht der ersten Zeile der Trainingsmenge:\\n{analyse_train[1].iloc[0]}\\n\")\n",
    "print(f\"Nachricht der ersten Zeile der Testmenge:\\n{analyse_test[1].iloc[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Klassenverteilung\n",
    "Die Verteilung der Klassen sieht wie folgt aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainingsmenge:\n",
      "[['negative' 5228]\n",
      " ['neutral' 14497]\n",
      " ['positive' 1216]]\n",
      "Testmenge:\n",
      "[['negative' 617]\n",
      " ['neutral' 1812]\n",
      " ['positive' 155]]\n"
     ]
    }
   ],
   "source": [
    "unique_train, counts_train = np.unique(analyse_train[3], return_counts=True)\n",
    "unique_test, counts_test = np.unique(analyse_test[3], return_counts=True)\n",
    "\n",
    "print(f\"Trainingsmenge:\\n{np.asarray((unique_train, counts_train)).T}\")\n",
    "print(f\"Testmenge:\\n{np.asarray((unique_test, counts_test)).T}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Daten bereinigen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für unsere Aufgabe benötigen wir nur den Text der Posts und die Labels, daher nutzen wir nur die Spalten 1 und 3.\n",
    "\n",
    "Um Erwähnungen zu vereinfachen, wird jeder Twitterusername durch den Text 'twitterusername' ersetzt. @DB_Bahn wird besonders gekennzeichnet, da er hier von besonderer Bedeutung ist.\n",
    "\n",
    "Zudem werden Satzzeichen entfernt und Emojis durch festgelegte Wörter ersetzt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0         1\n",
      "0  @DB_Bahn ja, weil in Wuppertal Bauarbeiten sin...   neutral\n",
      "1  @nordschaf theoretisch kannste dir überall im ...  positive\n",
      "2  Bahn verspätet sich..gleich kommt noch jemand ...  negative\n",
      "3  Ihre Anfragen brachten uns zu neuen Leistungen...   neutral\n",
      "4  Kann ich mit dem DB Geschenk Ticket den ICE Sp...   neutral\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(TRAIN_DF, delimiter='\\t', usecols=[1,3], header=None).rename(columns={1:0,3:1})\n",
    "test_data = pd.read_csv(TEST_DF, delimiter='\\t', usecols=[1,3], header=None).rename(columns={1:0,3:1})\n",
    "\n",
    "print(train_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datensätze werden bereinigt, dies kann einige Zeit in Anspruch nehmen...\n",
      "Datensätze bereinigt.\n",
      "                                                   0         1\n",
      "0  dbusername ja weil in wuppertal bauarbeiten si...   neutral\n",
      "1  twitterusername theoretisch kannste dir uebera...  positive\n",
      "2  bahn verspaetet sich annoyeddots gleich kommt ...  negative\n",
      "3  ihre anfragen brachten uns zu neuen leistungen...   neutral\n",
      "4  kann ich mit dem db geschenk ticket den ice sp...   neutral\n"
     ]
    }
   ],
   "source": [
    "print(\"Datensätze werden bereinigt, dies kann einige Zeit in Anspruch nehmen...\")\n",
    "clean_train_data = train_data.apply(data_cleaner.clean_text)\n",
    "clean_test_data = test_data.apply(data_cleaner.clean_text)\n",
    "print(\"Datensätze bereinigt.\")\n",
    "\n",
    "print(clean_train_data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die bereinigten Datensätze sind bereits gespeichert und können im Folgenden geladen werden.\n",
    "Ein Vergleich der originalen und bereinigten Daten kann hier angesehen werden. Über das Argument in `.iloc[]`kann der Eintrag gewählt werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: Bahn verspätet sich..gleich kommt noch jemand und drückt mir ein riesiges Kreuz in die Hand, das ich rumschleppen muss\n",
      "bereinigt: bahn verspaetet sich annoyeddots gleich kommt noch jemand und drueckt mir ein riesiges kreuz in die hand das ich rumschleppen muss\n"
     ]
    }
   ],
   "source": [
    "print(f\"original: {train_data[0].iloc[2]}\")\n",
    "print(f\"bereinigt: {clean_train_data[0].iloc[2]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bereinigte Trainings und Testdaten laden\n",
    "**Wird verwendet, falls das Bereinigen übersprungen werden soll**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_data = pd.read_csv(CLEAN_TRAIN_DF, delimiter='\\t', header=None)\n",
    "clean_test_data = pd.read_csv(CLEAN_TEST_DF, delimiter='\\t', header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Logistische-Regression mit Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Trainings und Testdatensatz wird mit Gensim zum vectorisieren vorverarbeitet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainingsdaten:\n",
      "[TaggedDocument(words=['dbusername', 'ja', 'weil', 'in', 'wuppertal', 'bauarbeiten', 'sind', 'soweit', 'bin', 'ich', 'auch', 'aber', 'wieso', 'nur', 'am', 'wochenende', 'und', 'grade', 'jetzt'], tags='neutral')]\n",
      "\n",
      "Testdaten:\n",
      "[TaggedDocument(words=['re', 'emote', 'das', 'erste', 'ich', 'fahre', 'nicht', 'mit', 'der', 'bahn', 'nach', 'erzaehlungen', 'von', 'freunden', 'scheint', 'es', 'ein', 'zentrales', 'problem', 'zu', 'sein', 'es', 'kann', 'nicht', 'sei', 'dass', 'jemand', 'in', 'hannover', 'umsteigen', 'muss', 'um', 'in', 'einen', 'zug', 'auch', 'ohne', 'klimaanlage', 'nach', 'berlin', 'zu', 'fahren', 'es', 'war', 'aber', 'bei', 'dem', 'letzten', 'hitzehoch', 'das', 'liegt', 'am', 'hersteller'], tags='negative')]\n"
     ]
    }
   ],
   "source": [
    "def read_corpus(fname):\n",
    "    for i in range(0,fname.shape[0]):\n",
    "        tokens = gensim.utils.simple_preprocess(str(fname[0].iloc[i]))\n",
    "        yield gensim.models.doc2vec.TaggedDocument(tokens, str(fname[1].iloc[i]))\n",
    "                \n",
    "train_corpus = list(read_corpus(clean_train_data))\n",
    "test_corpus = list(read_corpus(clean_test_data))       \n",
    "\n",
    "print(\"Trainingsdaten:\")\n",
    "print(train_corpus[:1])\n",
    "print(\"\\nTestdaten:\")\n",
    "print(test_corpus[:1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Doc2Vec-Modell erzeugen\n",
    "Gensim wird für ganze Sätze konfiguriert(docs) und das Wörterbuch wird erstellt. \"min_count\" ignoriert alle Wörter, die seltener als definiert vorkommen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=100, min_count=2, epochs=15)\n",
    "model.build_vocab(train_corpus)\n",
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zunächst werden Text und Labels für einfache Weiterverwendung extrahiert. Danach werden die Trainingssätze vom Doc2Vec-Modell in Vektoren umgewandelt und den Traininglabels eindeutige Zahlen zugeordnet. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'negative': 0, 'neutral': 1, 'positive': 2}\n"
     ]
    }
   ],
   "source": [
    "train_regressors, train_targets = zip(*[(doc.words, doc.tags) for doc in train_corpus])\n",
    "\n",
    "# converting training sentences into vectors\n",
    "train_x = np.asarray([model.infer_vector(regressor) for regressor in train_regressors])\n",
    "\n",
    "# converting training labels to unique numbers \n",
    "target_name_idx = {target:i for i, target in enumerate(np.unique(train_targets))}\n",
    "train_y = np.vectorize(target_name_idx.get)(train_targets)\n",
    "print(f\"{target_name_idx}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Logistische Regression\n",
    "Die erwarteten Labels werden an die Logistische regression gegeben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=2000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=2000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=2000)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(max_iter=2000)\n",
    "logreg.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Genauigkeit der Testmenge\n",
    "Hier wird die Testmenge von dem Modell analysiert. Die Zusammenfassung wird von SKLearn angezeigt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.32      0.39       617\n",
      "     neutral       0.75      0.87      0.81      1812\n",
      "    positive       0.23      0.10      0.14       155\n",
      "\n",
      "    accuracy                           0.70      2584\n",
      "   macro avg       0.49      0.43      0.45      2584\n",
      "weighted avg       0.66      0.70      0.67      2584\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_regressors, test_targets = zip(*[(doc.words, doc.tags) for doc in test_corpus])\n",
    "\n",
    "# converting test sentences into vectors\n",
    "test_x = np.asarray([model.infer_vector(regressor) for regressor in test_regressors])\n",
    "# converting test labels to unique numbers \n",
    "test_y = np.vectorize(target_name_idx.get)(test_targets)\n",
    "\n",
    "# predict labels of test sentences\n",
    "preds = logreg.predict(test_x)\n",
    "\n",
    "print(classification_report(test_y, preds, target_names=target_name_idx.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 FastText\n",
    "\n",
    "[FastText](https://fasttext.cc/) ist ein Framework von Facebook zum Klassifizieren von Texten. Es ist effizient und kann sogar auf Mobilgeräten verwendet werden.\n",
    "\n",
    "\n",
    "## 4.1 Vorverarbeitung für FastText\n",
    "Zuerst werden die Kommentare mit Labels versehen. Sie werden aus dem Datensatz in diese Form gebracht: <br>\n",
    "\"\\__label\\__<positive/neutral/negative> >Kommentar<\".\n",
    "    \n",
    "Die ersten zwei Einträge werden angezeigt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__label__neutral dbusername ja weil in wuppertal bauarbeiten sind soweit bin ich auch aber wieso nur am wochenende und grade jetzt', '__label__positive twitterusername theoretisch kannste dir ueberall im koelner stadtbereich was suchen mit der kvb sbahn kommt man ueberall fix hin']\n"
     ]
    }
   ],
   "source": [
    "labeledList_train = [f\"__label__{row[1]} {row[0]}\" for _, row in clean_train_data.iterrows()]\n",
    "labeledList_test = [f\"__label__{row[1]} {row[0]}\" for _, row in clean_test_data.iterrows()]\n",
    "print(labeledList_train[:2])\n",
    "\n",
    "with open(f\"{ROOT_DIR}/fasttext_train.txt\", 'w') as f:\n",
    "      f.write(\"\\n\".join(labeledList_train))\n",
    "\n",
    "with open(f\"{ROOT_DIR}/fasttext_test.txt\", 'w') as f:\n",
    "      f.write(\"\\n\".join(labeledList_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Training\n",
    "Anschließend wird mit FastText der Text trainiert. Die Anzahl der Epochen, Lernrate ([0.1,1]) und NGram-länge ([1,5]) kann angepasst werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 1M words\n",
      "Number of words:  96976\n",
      "Number of labels: 3\n",
      "Progress: 100.0% words/sec/thread:  119111 lr:  0.000000 avg.loss:  0.183315 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.train_supervised(input=f\"{ROOT_DIR}/fasttext_train.txt\", epoch=20, lr=0.2, wordNgrams=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Test des Modells\n",
    "Um das Modell zu testen, wird der Testdatensatz übergeben.\n",
    "\n",
    "Die Ausgabe ist wie folgt Formatiert: (Samples, Precision, Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2584, 0.7801857585139319, 0.7801857585139319)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test(f\"{ROOT_DIR}/fasttext_test.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Einzelinferenz\n",
    "Hier können eigene Sätze mit dem Modell getestet werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'dbusername heute mal wieder viel zu spaet sadsmiley'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(('__label__negative',), array([1.00000966]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_predict=\"@DB_Bahn Heute mal wieder viel zu spät :(\"\n",
    "\n",
    "#preprocess input\n",
    "df = pd.DataFrame([to_predict])\n",
    "df = df.apply(data_cleaner.clean_text)\n",
    "to_predict = df[0].item()\n",
    "print(f\"'{to_predict}'\")\n",
    "\n",
    "model.predict(to_predict, k=-1, threshold=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# 5 Bert\n",
    "[Bert](https://huggingface.co/transformers/model_doc/bert.html) (Bidirectional Encoder Representations from Transformers) ist ein Modell, das nicht nur die Wörter in einem Text, sondern auch den Kontext analysiert.\n",
    "\n",
    "In diesem Notebook wird Pytorch als Backend verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0         1\n",
      "0  dbusername ja weil in wuppertal bauarbeiten si...   neutral\n",
      "1  twitterusername theoretisch kannste dir uebera...  positive\n"
     ]
    }
   ],
   "source": [
    "clean_train_data = pd.read_csv(CLEAN_TRAIN_DF, delimiter='\\t', header=None)\n",
    "clean_test_data = pd.read_csv(CLEAN_TEST_DF, delimiter='\\t', header=None)\n",
    "print(clean_train_data[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Preprocessing\n",
    "Das Modell erwartet die Labels als Ordinale, daher werden sie umgewandelt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ordinale labels: {'negative': 0, 'neutral': 1, 'positive': 2}\n"
     ]
    }
   ],
   "source": [
    "# convert labels to ordinals\n",
    "labels_to_ordinal = {target:i for i, target in enumerate(np.unique(clean_train_data[1]))}\n",
    "print(f\"ordinale labels: {labels_to_ordinal}\")\n",
    "        \n",
    "bert_train = pd.DataFrame({'label': np.vectorize(labels_to_ordinal.get)(clean_train_data[1]),\n",
    "                         'text':clean_train_data[0]})\n",
    "\n",
    "bert_test = pd.DataFrame({'label': np.vectorize(labels_to_ordinal.get)(clean_test_data[1]),\n",
    "                         'text':clean_test_data[0]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Text wird mit Hilfe von BERT in Tokens mit Start- und Stopp-Symbolen umgewandelt. Es werden nur Eingaben von 512 Zeichen unterstützt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc1871db4f36454283885e8d31406c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d265dc357c44cfb155920d36d850c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66029492e614a3a980cd03cffaf0456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/249k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d91bb1d06b645839d59dd57b3d72d3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/474k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token werden erstellt, dies kann einige Zeit in anspruch nehmen..\n",
      "Token erstellt!\n",
      "\n",
      "Text: dbusername ja weil in wuppertal bauarbeiten sind soweit bin ich auch aber wieso nur am wochenende und grade jetzt\n",
      "\n",
      "Als Token: [3, 9, 5655, 212, 1431, 3278, 982, 50, 25, 6221, 14779, 3703, 3085, 287, 4133, 4058, 1169, 194, 386, 6724, 26910, 356, 235, 743, 280, 765, 42, 5086, 57, 1868, 4]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-german-cased\", do_lower_case=True)\n",
    "print(\"Token werden erstellt, dies kann einige Zeit in anspruch nehmen..\")\n",
    "train_tokenized = bert_train['text'].astype(str).apply((lambda x: tokenizer.encode(x[:512], add_special_tokens=True)))\n",
    "test_tokenized = bert_test['text'].astype(str).apply((lambda x: tokenizer.encode(x[:512], add_special_tokens=True)))\n",
    "print(\"Token erstellt!\")\n",
    "\n",
    "print(f\"\\nText: {clean_train_data[0].iloc[0]}\")\n",
    "print(f\"\\nAls Token: {train_tokenized[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Padding der Daten\n",
    "Da auf die Daten möglichst performant zugegriffen werden soll, werden sie als Arrays gespeichert. Die längste Zeichenkette gibt daher die größe der Arrays vor. Alle anderen werden am ende mit 0 aufgefült. Die `Attention Mask` filtert die aufgefüllten Werte später raus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for i in train_tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "        \n",
    "for i in test_tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)        \n",
    "\n",
    "\n",
    "train_padded = np.array([i + [0]*(max_len-len(i)) for i in train_tokenized.values])\n",
    "train_attention_mask = np.where(train_padded != 0, 1, 0)\n",
    "\n",
    "test_padded = np.array([i + [0]*(max_len-len(i)) for i in test_tokenized.values])\n",
    "test_attention_mask = np.where(test_padded != 0, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Daten für GPU vorbereiten\n",
    "Hier werden Datensätze zum Training und Testen erzeugt. Übergeben werden die Token-Sätze, Masken und Label. Die `Dataloader` laden die Datensätze mit der Größe `BATCH_SIZE` wenn benötigt auf die GPU, um den GPU-Speicher nicht durch den Datensatz auszulasten. In diesem Fall werden gleichzeitig 50 Einträge verarbeitet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.tensor(train_padded),\n",
    "                             torch.tensor(train_attention_mask),\n",
    "                             torch.tensor(list(bert_train['label'])))\n",
    "\n",
    "test_dataset = TensorDataset(torch.tensor(test_padded),\n",
    "                            torch.tensor(test_attention_mask),\n",
    "                            torch.tensor(list(bert_test['label'])))\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler = RandomSampler(train_dataset), \n",
    "            batch_size = BATCH_SIZE\n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            test_dataset,\n",
    "            sampler = SequentialSampler(test_dataset),\n",
    "            batch_size = BATCH_SIZE\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Modell erstellen\n",
    "Es wird ein BERT Modell mit 12 Layern erzeugt, die Anzahl an Labels muss mit dem Datensatz übereinstimmen (hier \"positive\", \"neutral\", \"negative\").\n",
    "\n",
    "Der \"cuda()\" Aufruf verschiebt das Modell auf die GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-german-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-german-cased\",\n",
    "    num_labels = 3,\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,\n",
    ")\n",
    "model.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Optimizer und Lernrate\n",
    "Als Optimizer verwenden wir Adam, da dieser sehr schnell konvergiert.\n",
    "Um möglichst viele Daten gleichzeitig zu verarbeiten, verwenden wir `mixed-precision`, d.h. es werden wenn möglich Float16 Werte anstelle von Float32 Werten verwendet, um Ressourcen einzusparen. Details zu `mixed-precision` mit PyTorch: https://pytorch.org/docs/stable/notes/amp_examples.html "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(),\n",
    "                  lr = 1e-5, # args.learning_rate - default is 5e-5,\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "scaler = GradScaler() # necessary for mixed precision training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Training vorbereiten\n",
    "Die Anzahl an Epochen kann über `NUM_EPOCHS` eingestellt werden. Die Lernrate wird über einen scheduler gesteuert.\n",
    "Die definierten Funktionen dienen zur Darstellung des Fortschritts im Training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "\n",
    "total_steps = len(train_dataloader) * NUM_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Modell Training\n",
    "Der Trainingsprozess bestet aus der Trainingsphase mit der Trainingsmenge und Validierungsphase mit der Validierungsmenge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1/5 ========\n",
      "Training...\n",
      "  Batch    40  of    655.    Elapsed: 0:00:07.\n",
      "  Batch    80  of    655.    Elapsed: 0:00:14.\n",
      "  Batch   120  of    655.    Elapsed: 0:00:21.\n",
      "  Batch   160  of    655.    Elapsed: 0:00:28.\n",
      "  Batch   200  of    655.    Elapsed: 0:00:34.\n",
      "  Batch   240  of    655.    Elapsed: 0:00:41.\n",
      "  Batch   280  of    655.    Elapsed: 0:00:48.\n",
      "  Batch   320  of    655.    Elapsed: 0:00:54.\n",
      "  Batch   360  of    655.    Elapsed: 0:01:01.\n",
      "  Batch   400  of    655.    Elapsed: 0:01:08.\n",
      "  Batch   440  of    655.    Elapsed: 0:01:14.\n",
      "  Batch   480  of    655.    Elapsed: 0:01:21.\n",
      "  Batch   520  of    655.    Elapsed: 0:01:27.\n",
      "  Batch   560  of    655.    Elapsed: 0:01:34.\n",
      "  Batch   600  of    655.    Elapsed: 0:01:41.\n",
      "  Batch   640  of    655.    Elapsed: 0:01:47.\n",
      "  Average training loss: 0.59\n",
      "  Training epoch took: 0:01:50\n",
      "\n",
      " Running Validation...\n",
      " Validation Accuracy: 0.80\n",
      "  Validation Loss: 0.50\n",
      "  Validation Time: 0:00:10\n",
      "\n",
      "======== Epoch 2/5 ========\n",
      "Training...\n",
      "  Batch    40  of    655.    Elapsed: 0:00:07.\n",
      "  Batch    80  of    655.    Elapsed: 0:00:13.\n",
      "  Batch   120  of    655.    Elapsed: 0:00:20.\n",
      "  Batch   160  of    655.    Elapsed: 0:00:26.\n",
      "  Batch   200  of    655.    Elapsed: 0:00:33.\n",
      "  Batch   240  of    655.    Elapsed: 0:00:39.\n",
      "  Batch   280  of    655.    Elapsed: 0:00:46.\n",
      "  Batch   320  of    655.    Elapsed: 0:00:52.\n",
      "  Batch   360  of    655.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    655.    Elapsed: 0:01:05.\n",
      "  Batch   440  of    655.    Elapsed: 0:01:12.\n",
      "  Batch   480  of    655.    Elapsed: 0:01:18.\n",
      "  Batch   520  of    655.    Elapsed: 0:01:25.\n",
      "  Batch   560  of    655.    Elapsed: 0:01:32.\n",
      "  Batch   600  of    655.    Elapsed: 0:01:38.\n",
      "  Batch   640  of    655.    Elapsed: 0:01:45.\n",
      "  Average training loss: 0.44\n",
      "  Training epoch took: 0:01:47\n",
      "\n",
      " Running Validation...\n",
      " Validation Accuracy: 0.82\n",
      "  Validation Loss: 0.47\n",
      "  Validation Time: 0:00:10\n",
      "\n",
      "======== Epoch 3/5 ========\n",
      "Training...\n",
      "  Batch    40  of    655.    Elapsed: 0:00:07.\n",
      "  Batch    80  of    655.    Elapsed: 0:00:13.\n",
      "  Batch   120  of    655.    Elapsed: 0:00:20.\n",
      "  Batch   160  of    655.    Elapsed: 0:00:26.\n",
      "  Batch   200  of    655.    Elapsed: 0:00:33.\n",
      "  Batch   240  of    655.    Elapsed: 0:00:39.\n",
      "  Batch   280  of    655.    Elapsed: 0:00:46.\n",
      "  Batch   320  of    655.    Elapsed: 0:00:52.\n",
      "  Batch   360  of    655.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    655.    Elapsed: 0:01:05.\n",
      "  Batch   440  of    655.    Elapsed: 0:01:12.\n",
      "  Batch   480  of    655.    Elapsed: 0:01:18.\n",
      "  Batch   520  of    655.    Elapsed: 0:01:25.\n",
      "  Batch   560  of    655.    Elapsed: 0:01:31.\n",
      "  Batch   600  of    655.    Elapsed: 0:01:38.\n",
      "  Batch   640  of    655.    Elapsed: 0:01:44.\n",
      "  Average training loss: 0.35\n",
      "  Training epoch took: 0:01:47\n",
      "\n",
      " Running Validation...\n",
      " Validation Accuracy: 0.81\n",
      "  Validation Loss: 0.49\n",
      "  Validation Time: 0:00:10\n",
      "\n",
      "======== Epoch 4/5 ========\n",
      "Training...\n",
      "  Batch    40  of    655.    Elapsed: 0:00:07.\n",
      "  Batch    80  of    655.    Elapsed: 0:00:14.\n",
      "  Batch   120  of    655.    Elapsed: 0:00:20.\n",
      "  Batch   160  of    655.    Elapsed: 0:00:27.\n",
      "  Batch   200  of    655.    Elapsed: 0:00:34.\n",
      "  Batch   240  of    655.    Elapsed: 0:00:40.\n",
      "  Batch   280  of    655.    Elapsed: 0:00:47.\n",
      "  Batch   320  of    655.    Elapsed: 0:00:53.\n",
      "  Batch   360  of    655.    Elapsed: 0:01:00.\n",
      "  Batch   400  of    655.    Elapsed: 0:01:06.\n",
      "  Batch   440  of    655.    Elapsed: 0:01:13.\n",
      "  Batch   480  of    655.    Elapsed: 0:01:19.\n",
      "  Batch   520  of    655.    Elapsed: 0:01:26.\n",
      "  Batch   560  of    655.    Elapsed: 0:01:32.\n",
      "  Batch   600  of    655.    Elapsed: 0:01:39.\n",
      "  Batch   640  of    655.    Elapsed: 0:01:45.\n",
      "  Average training loss: 0.29\n",
      "  Training epoch took: 0:01:48\n",
      "\n",
      " Running Validation...\n",
      " Validation Accuracy: 0.81\n",
      "  Validation Loss: 0.52\n",
      "  Validation Time: 0:00:10\n",
      "\n",
      "======== Epoch 5/5 ========\n",
      "Training...\n",
      "  Batch    40  of    655.    Elapsed: 0:00:07.\n",
      "  Batch    80  of    655.    Elapsed: 0:00:13.\n",
      "  Batch   120  of    655.    Elapsed: 0:00:20.\n",
      "  Batch   160  of    655.    Elapsed: 0:00:26.\n",
      "  Batch   200  of    655.    Elapsed: 0:00:33.\n",
      "  Batch   240  of    655.    Elapsed: 0:00:39.\n",
      "  Batch   280  of    655.    Elapsed: 0:00:46.\n",
      "  Batch   320  of    655.    Elapsed: 0:00:53.\n",
      "  Batch   360  of    655.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    655.    Elapsed: 0:01:06.\n",
      "  Batch   440  of    655.    Elapsed: 0:01:12.\n",
      "  Batch   480  of    655.    Elapsed: 0:01:19.\n",
      "  Batch   520  of    655.    Elapsed: 0:01:26.\n",
      "  Batch   560  of    655.    Elapsed: 0:01:32.\n",
      "  Batch   600  of    655.    Elapsed: 0:01:39.\n",
      "  Batch   640  of    655.    Elapsed: 0:01:46.\n",
      "  Average training loss: 0.24\n",
      "  Training epoch took: 0:01:48\n",
      "\n",
      " Running Validation...\n",
      " Validation Accuracy: 0.80\n",
      "  Validation Loss: 0.55\n",
      "  Validation Time: 0:00:10\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:09:51 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "# use cuda device\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# same seed for reproducibility\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# training statistics \n",
    "training_stats = []\n",
    "# time measure\n",
    "total_t0 = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # ========================================\n",
    "    # Training phase\n",
    "    # ========================================\n",
    "    print(f'\\n======== Epoch {epoch+1}/{NUM_EPOCHS} ========\\nTraining...')\n",
    "\n",
    "    #training time for epoch\n",
    "    t0 = time.time()\n",
    "\n",
    "    # reset total loss\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # change model mode to training\n",
    "    model.train()\n",
    "\n",
    "    for step, (tokens, att_mask, labels) in enumerate(train_dataloader):\n",
    "        # Progress update every 20 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0) # elapsed time\n",
    "            print(f'  Batch {step:>5,}  of  {len(train_dataloader):>5,}.    Elapsed: {elapsed}.')\n",
    "\n",
    "        tokens = tokens.to(device)\n",
    "        att_mask = att_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # clear calculated gradiends\n",
    "        optimizer.zero_grad()    \n",
    "        \n",
    "        # calculate gradients\n",
    "        with autocast():\n",
    "            # train model with current batch\n",
    "            outputs = model(tokens, token_type_ids=None, attention_mask=att_mask, labels=labels)\n",
    "            loss, logits = outputs.loss, outputs.logits\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        \n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # training loss of all batches for statistics\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        # Update the learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "    print(f\"  Average training loss: {avg_train_loss:.2f}\\n  Training epoch took: {training_time}\")\n",
    "        \n",
    "        \n",
    "    # ========================================\n",
    "    #  Validation phase\n",
    "    # ========================================\n",
    "    print(\"\\n Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    # put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    for (tokens, att_mask, labels) in validation_dataloader:\n",
    "        tokens = tokens.to(device)\n",
    "        att_mask = att_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # constructing the compute graph is only needed for training\n",
    "        with torch.no_grad():        \n",
    "            # the documentation for this model function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "            # values prior to applying an activation function like the softmax.\n",
    "            outputs = model(tokens, token_type_ids=None, attention_mask=att_mask, labels=labels)\n",
    "            loss, logits = outputs.loss, outputs.logits\n",
    "            \n",
    "        # accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        labels = labels.to('cpu').numpy()\n",
    "\n",
    "        # calculate the accuracy \n",
    "        total_eval_accuracy += flat_accuracy(logits, labels)\n",
    "        \n",
    "    # report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    # calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    # measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(f\" Validation Accuracy: {avg_val_accuracy:.2f}\")\n",
    "    print(f\"  Validation Loss: {avg_val_loss:.2f}\")\n",
    "    print(f\"  Validation Time: {validation_time}\")\n",
    "\n",
    "    # record all statistics from this epoch.\n",
    "    training_stats.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "print(f\"Total training took {format_time(time.time()-total_t0)} (h:mm:ss)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 Einzelinferenz\n",
    "Hier können eigene Sätze mit dem Modell getestet werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vorverarbeiteter Text: 'dbusername heute wieder spaeter'\n",
      "Tokenized: [3, 9, 5655, 212, 1431, 1138, 525, 338, 4173, 60, 4]\n",
      "Ergebnis: negative\n"
     ]
    }
   ],
   "source": [
    "to_predict=\"@DB_Bahn heute wieder später\"\n",
    "ordinal_to_labels = {v: k for k, v in labels_to_ordinal.items()}\n",
    "\n",
    "#preprocess input\n",
    "df=pd.DataFrame([to_predict])\n",
    "df=df.apply(data_cleaner.clean_text)\n",
    "to_predict=(df[0].item())\n",
    "\n",
    "print(f\"Vorverarbeiteter Text: '{to_predict}'\")\n",
    "\n",
    "tokenized=tokenizer.encode(to_predict[:512])\n",
    "print(f\"Tokenized: {tokenized}\")\n",
    "\n",
    "#predict input\n",
    "tokens = torch.tensor([tokenized]).cuda()\n",
    "with torch.no_grad(): \n",
    "    model.eval()\n",
    "    output = model(tokens, token_type_ids=None)\n",
    "    logits = output.logits\n",
    "\n",
    "prediction = np.argmax(logits.detach().cpu().numpy(), axis=1)[0]\n",
    "print(f\"Ergebnis: {ordinal_to_labels[prediction]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    " torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
