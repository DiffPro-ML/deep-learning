{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with the DB Social Media Dataset\n",
    "\n",
    "The dataset was created for sentiment analysis and detection of relevant feedback.\n",
    "Additional documentation and downloads can be found at https://sites.google.com/view/germeval2017-absa/data?authuser=0.\n",
    "\n",
    "The dataset will be processed using Logistic Regression, FastText and BERT.\n",
    "\n",
    "## Jupyter notebook\n",
    "Jupyter notebooks are interactive Python scripts where Markdown and even Latex can be used for documentation. The purpose of this section is to provide general information such as keyboard shortcuts and common workflows, as well as information specific to this workshop and the hardware used when dealing with Python and Jupyter notebooks.\n",
    "\n",
    "### Important things in a nutshell (a.k.a. TL;DR)\n",
    "- **when switching to another notebook, always exit the kernel (Kernel -> Shutdown)**\n",
    "- **Shift + Enter** to execute the active cell\n",
    "- **Ctrl + Shift + p** to open the command palette (**Ctrl + Shift + f** in Firefox)\n",
    "- **Shift + o** to toggle cell scrolling\n",
    "- in case of error messages **restart the kernel (Kernel -> Restart)**\n",
    "- \"Help, I see the Markdown code\" -> **Shift + Enter** in the corresponding cell\n",
    "- \"Help, I'm getting ResourceExhaustion / OutOfMemory (OOM) error\" -> **Restart kernel, shutdown kernel from other still running notebooks** (click on the Jupyter logo in the upper left, then on the \"Running\" tab)\n",
    "- \"Help, my notebook is broken\" -> see **Notebook \"Recovery \"**.\n",
    "- \" Is something happening?\" -> if the **circle at the top right of \"Python 3\" is filled and dark** then the kernel is still working, if it is not filled then the kernel is idle. The currently running cell is the first one seen from the top, where \"In[*]\" is written on the left side instead of e.g. \"In[5]\". But it can happen that the kernel freezes, in this case just click on __Kernel -> Interrupt__ and execute the cell again.\n",
    "\n",
    "\n",
    "### Overview & Workflow\n",
    "The only two shortcuts you really have to remember are\n",
    "- **Shift + Enter** to execute a cell, and\n",
    "- **Ctrl + Shift + p** to open the command palette, from which you then have direct access to all possible commands, including the corresponding shortcuts.\n",
    "\n",
    "Another useful shortcut is **Shift + o**, which toggles **Scrolling for cell output**.\n",
    "\n",
    "To **edit a cell** it is sufficient to double-click inside the cell, for code cells it is sufficient to simply click outside the cell to exit edit mode, for documentation cells (like this one) it is necessary to execute the cell to exit code view.\n",
    "\n",
    "While a cell is executing, the kernel indicator in the upper right corner next to \"Python 3\" changes from a light circle with a dark border to a filled dark circle and jumps back as soon as the execution is finished. If multiple cells were executed at the same time, the label in the left column tells you whether the cell has finished executing (**In [NUMBER]:**) or is currently executing or waiting to execute (**In [*]:**). Additionally, after a cell has been executed, the cell output is displayed below the cell.\n",
    "\n",
    "To keep the overview it can sometimes be useful to delete the **cell output**. This can be done in these two ways, among others:\n",
    "- top of Cell -> Current Outputs / All outputs -> Clear\n",
    "- on top of Kernel -> Restart & Clear Output\n",
    "\n",
    "The kernel is responsible for the execution of the Python code and keeps the context (i.e. used variables, defined functions and used memory) available since the last kernel (re)start. This can lead to problems if cells are executed in a different order or if cells are skipped in which variables or functions are defined that are needed in the further course of the script, but also if **an other Jupyter notebook is started**, since another kernel is started for this.\n",
    "\n",
    "Therefore when **changing to another notebook** always shutdown or restart the **kernel** (above Kernel -> Restart/Shutdown)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, datetime, random\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import fasttext\n",
    "import gensim\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.optim import Adam\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "\n",
    "import transformers\n",
    "from transformers import BertForSequenceClassification\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import data_cleaner # pyscript data_cleaner.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DIR = os.path.abspath(\"./\")\n",
    "DATA_DIR = os.path.abspath(f\"./dataset/\")\n",
    "\n",
    "TRAIN_DF= f\"{DATA_DIR}/db_twitter/train_v1.4.tsv\"\n",
    "TEST_DF= f\"{DATA_DIR}/db_twitter/dev_v1.4.tsv\"\n",
    "CLEAN_TRAIN_DF= f\"{DATA_DIR}/db_twitter/train_clean.tsv\"\n",
    "CLEAN_TEST_DF= f\"{DATA_DIR}/db_twitter/test_clean.tsv\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data Analysis\n",
    "For analysis, we load the test dataset and look at the first five rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die ersten 5 Zeilen in der Trainingsmenge:\n",
      "                    0                   1      2         3                   4\n",
      "0  http://twitter....  @DB_Bahn ja, we...   True   neutral  Allgemein#Haupt...\n",
      "1  http://twitter....  @nordschaf theo...   True  positive  Zugfahrt#Streck...\n",
      "2  http://twitter....  Bahn verspätet ...   True  negative  Zugfahrt#Pünktl...\n",
      "3  http://wirtscha...  Ihre Anfragen b...  False   neutral                 NaN\n",
      "4  http://communit...  Kann ich mit de...   True   neutral  Allgemein#Haupt... \n",
      "\n",
      "Die ersten 5 Zeilen in der Testmenge:\n",
      "                    0                   1      2         3                   4\n",
      "0  http://www.face...  Re: Das Erste \"...   True  negative  Allgemein#Haupt...\n",
      "1  http://www.luft...  Effektiver Part...  False   neutral                 NaN\n",
      "2  http://twitter....  #TelMi #telmi L...   True  negative  Sicherheit#Haup...\n",
      "3  http://twitter....  @KuttnerSarah @...   True  negative  Gepäck#Haupt:ne...\n",
      "4  http://twitter....  Probleme bei de...   True  negative  Allgemein#Haupt... \n",
      "\n",
      "Nachricht der ersten Zeile der Trainingsmenge:\n",
      "@DB_Bahn ja, weil in Wuppertal Bauarbeiten sind, soweit bin ich auch, aber wieso nur am Wochenende und grade jetzt?\n",
      "\n",
      "Nachricht der ersten Zeile der Testmenge:\n",
      "Re: Das Erste \"Ich fahre nicht mit der Bahn. Nach Erzählungen von Freunden, scheint es ein zentrales Problem zu sein. Es kann nicht sei, dass jemand in Hannover umsteigen muss, um in einen Zug , auch ohne Klimaanlage nach Berlin zu fahren. --- Es war aber bei dem letzten \"\"Hitzehoch\"\". --- Das liegt am Hersteller.\"\n"
     ]
    }
   ],
   "source": [
    "analyse_train = pd.read_csv(TRAIN_DF, delimiter='\\t', header=None)\n",
    "analyse_test = pd.read_csv(TEST_DF, delimiter='\\t', header=None)\n",
    "\n",
    "with pd.option_context('display.max_colwidth', 19):\n",
    "    print(\"Die ersten 5 Zeilen in der Trainingsmenge:\")\n",
    "    print(analyse_train[:5],\"\\n\")\n",
    "    print(\"Die ersten 5 Zeilen in der Testmenge:\")\n",
    "    print(analyse_test[:5], \"\\n\")\n",
    "    \n",
    "print(f\"Nachricht der ersten Zeile der Trainingsmenge:\\n{analyse_train[1].iloc[0]}\\n\")\n",
    "print(f\"Nachricht der ersten Zeile der Testmenge:\\n{analyse_test[1].iloc[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Class Distribution\n",
    "The distribution of classes is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainingsmenge:\n",
      "[['negative' 5228]\n",
      " ['neutral' 14497]\n",
      " ['positive' 1216]]\n",
      "Testmenge:\n",
      "[['negative' 617]\n",
      " ['neutral' 1812]\n",
      " ['positive' 155]]\n"
     ]
    }
   ],
   "source": [
    "unique_train, counts_train = np.unique(analyse_train[3], return_counts=True)\n",
    "unique_test, counts_test = np.unique(analyse_test[3], return_counts=True)\n",
    "\n",
    "print(f\"Trainingsmenge:\\n{np.asarray((unique_train, counts_train)).T}\")\n",
    "print(f\"Testmenge:\\n{np.asarray((unique_test, counts_test)).T}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Cleansing the Data\n",
    "\n",
    "For our task, we only need the text of the posts and the labels, so we only use columns 1 and 3.\n",
    "\n",
    "To simplify mentions, each Twitterusername is replaced by the text 'twitterusername'. @DB_Bahn is specially marked as it is of particular importance here.\n",
    "\n",
    "In addition, punctuation marks are removed and emojis are replaced with fixed words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0         1\n",
      "0  @DB_Bahn ja, weil in Wuppertal Bauarbeiten sin...   neutral\n",
      "1  @nordschaf theoretisch kannste dir überall im ...  positive\n",
      "2  Bahn verspätet sich..gleich kommt noch jemand ...  negative\n",
      "3  Ihre Anfragen brachten uns zu neuen Leistungen...   neutral\n",
      "4  Kann ich mit dem DB Geschenk Ticket den ICE Sp...   neutral\n"
     ]
    }
   ],
   "source": [
    "train_data = pd.read_csv(TRAIN_DF, delimiter='\\t', usecols=[1,3], header=None).rename(columns={1:0,3:1})\n",
    "test_data = pd.read_csv(TEST_DF, delimiter='\\t', usecols=[1,3], header=None).rename(columns={1:0,3:1})\n",
    "\n",
    "print(train_data[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datensätze werden bereinigt, dies kann einige Zeit in Anspruch nehmen...\n",
      "Datensätze bereinigt.\n",
      "                                                   0         1\n",
      "0  dbusername ja weil in wuppertal bauarbeiten si...   neutral\n",
      "1  twitterusername theoretisch kannste dir uebera...  positive\n",
      "2  bahn verspaetet sich annoyeddots gleich kommt ...  negative\n",
      "3  ihre anfragen brachten uns zu neuen leistungen...   neutral\n",
      "4  kann ich mit dem db geschenk ticket den ice sp...   neutral\n"
     ]
    }
   ],
   "source": [
    "print(\"Datensätze werden bereinigt, dies kann einige Zeit in Anspruch nehmen...\")\n",
    "clean_train_data = train_data.apply(data_cleaner.clean_text)\n",
    "clean_test_data = test_data.apply(data_cleaner.clean_text)\n",
    "print(\"Datensätze bereinigt.\")\n",
    "\n",
    "print(clean_train_data[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die bereinigten Datensätze sind bereits gespeichert und können im Folgenden geladen werden.\n",
    "Ein Vergleich der originalen und bereinigten Daten kann hier angesehen werden. Über das Argument in `.iloc[]`kann der Eintrag gewählt werden. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original: Bahn verspätet sich..gleich kommt noch jemand und drückt mir ein riesiges Kreuz in die Hand, das ich rumschleppen muss\n",
      "bereinigt: bahn verspaetet sich annoyeddots gleich kommt noch jemand und drueckt mir ein riesiges kreuz in die hand das ich rumschleppen muss\n"
     ]
    }
   ],
   "source": [
    "print(f\"original: {train_data[0].iloc[2]}\")\n",
    "print(f\"bereinigt: {clean_train_data[0].iloc[2]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load cleaned training and test data.\n",
    "**Use this if you want to skip the cleanup**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_train_data = pd.read_csv(CLEAN_TRAIN_DF, delimiter='\\t', header=None)\n",
    "clean_test_data = pd.read_csv(CLEAN_TEST_DF, delimiter='\\t', header=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Logistic Regression with Gensim\n",
    "The training and test dataset is preprocessed with Gensim to vectorize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainingsdaten:\n",
      "[TaggedDocument(words=['dbusername', 'ja', 'weil', 'in', 'wuppertal', 'bauarbeiten', 'sind', 'soweit', 'bin', 'ich', 'auch', 'aber', 'wieso', 'nur', 'am', 'wochenende', 'und', 'grade', 'jetzt'], tags='neutral')]\n",
      "\n",
      "Testdaten:\n",
      "[TaggedDocument(words=['re', 'emote', 'das', 'erste', 'ich', 'fahre', 'nicht', 'mit', 'der', 'bahn', 'nach', 'erzaehlungen', 'von', 'freunden', 'scheint', 'es', 'ein', 'zentrales', 'problem', 'zu', 'sein', 'es', 'kann', 'nicht', 'sei', 'dass', 'jemand', 'in', 'hannover', 'umsteigen', 'muss', 'um', 'in', 'einen', 'zug', 'auch', 'ohne', 'klimaanlage', 'nach', 'berlin', 'zu', 'fahren', 'es', 'war', 'aber', 'bei', 'dem', 'letzten', 'hitzehoch', 'das', 'liegt', 'am', 'hersteller'], tags='negative')]\n"
     ]
    }
   ],
   "source": [
    "def read_corpus(fname):\n",
    "    for i in range(0,fname.shape[0]):\n",
    "        tokens = gensim.utils.simple_preprocess(str(fname[0].iloc[i]))\n",
    "        yield gensim.models.doc2vec.TaggedDocument(tokens, str(fname[1].iloc[i]))\n",
    "                \n",
    "train_corpus = list(read_corpus(clean_train_data))\n",
    "test_corpus = list(read_corpus(clean_test_data))       \n",
    "\n",
    "print(\"Trainingsdaten:\")\n",
    "print(train_corpus[:1])\n",
    "print(\"\\nTestdaten:\")\n",
    "print(test_corpus[:1])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Create Doc2Vec model\n",
    "Gensim is configured for complete sentences (docs) and the dictionary is created. \"min_count\" ignores all words that occur less often than defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=100, min_count=2, epochs=15)\n",
    "model.build_vocab(train_corpus)\n",
    "model.train(train_corpus, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, text and labels are extracted for easy reuse. Then, the training sets are converted into vectors by the Doc2Vec model and unique numbers are assigned to the training labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'negative': 0, 'neutral': 1, 'positive': 2}\n"
     ]
    }
   ],
   "source": [
    "train_regressors, train_targets = zip(*[(doc.words, doc.tags) for doc in train_corpus])\n",
    "\n",
    "# converting training sentences into vectors\n",
    "train_x = np.asarray([model.infer_vector(regressor) for regressor in train_regressors])\n",
    "\n",
    "# converting training labels to unique numbers \n",
    "target_name_idx = {target:i for i, target in enumerate(np.unique(train_targets))}\n",
    "train_y = np.vectorize(target_name_idx.get)(train_targets)\n",
    "print(f\"{target_name_idx}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Logistic Regression\n",
    "The expected labels are given to the logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(max_iter=2000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(max_iter=2000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(max_iter=2000)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg = LogisticRegression(max_iter=2000)\n",
    "logreg.fit(train_x, train_y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Accuracy of the Test Set\n",
    "Here the test set is analyzed with the model. The summary is displayed by SKLearn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.50      0.32      0.39       617\n",
      "     neutral       0.75      0.87      0.81      1812\n",
      "    positive       0.23      0.10      0.14       155\n",
      "\n",
      "    accuracy                           0.70      2584\n",
      "   macro avg       0.49      0.43      0.45      2584\n",
      "weighted avg       0.66      0.70      0.67      2584\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_regressors, test_targets = zip(*[(doc.words, doc.tags) for doc in test_corpus])\n",
    "\n",
    "# converting test sentences into vectors\n",
    "test_x = np.asarray([model.infer_vector(regressor) for regressor in test_regressors])\n",
    "# converting test labels to unique numbers \n",
    "test_y = np.vectorize(target_name_idx.get)(test_targets)\n",
    "\n",
    "# predict labels of test sentences\n",
    "preds = logreg.predict(test_x)\n",
    "\n",
    "print(classification_report(test_y, preds, target_names=target_name_idx.keys()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 FastText\n",
    "[FastText](https://fasttext.cc/) is a framework from Facebook for classifying text. It is efficient and can even be used on mobile devices.\n",
    "\n",
    "## 4.1 Preprocessing for FastText.\n",
    "First, the comments are given labels. They will be converted from the dataset into this form: <br>.\n",
    "\"\\__label\\__<positive/neutral/negative> >comment<\".\n",
    " \n",
    "The first two entries are displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['__label__neutral dbusername ja weil in wuppertal bauarbeiten sind soweit bin ich auch aber wieso nur am wochenende und grade jetzt', '__label__positive twitterusername theoretisch kannste dir ueberall im koelner stadtbereich was suchen mit der kvb sbahn kommt man ueberall fix hin']\n"
     ]
    }
   ],
   "source": [
    "labeledList_train = [f\"__label__{row[1]} {row[0]}\" for _, row in clean_train_data.iterrows()]\n",
    "labeledList_test = [f\"__label__{row[1]} {row[0]}\" for _, row in clean_test_data.iterrows()]\n",
    "print(labeledList_train[:2])\n",
    "\n",
    "with open(f\"{ROOT_DIR}/fasttext_train.txt\", 'w') as f:\n",
    "      f.write(\"\\n\".join(labeledList_train))\n",
    "\n",
    "with open(f\"{ROOT_DIR}/fasttext_test.txt\", 'w') as f:\n",
    "      f.write(\"\\n\".join(labeledList_test))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Training\n",
    "Then FastText is used to train the text. The number of epochs, learning rate ([0.1,1]) and NGram length ([1,5]) can be adjusted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 1M words\n",
      "Number of words:  96976\n",
      "Number of labels: 3\n",
      "Progress: 100.0% words/sec/thread:  119111 lr:  0.000000 avg.loss:  0.183315 ETA:   0h 0m 0s\n"
     ]
    }
   ],
   "source": [
    "model = fasttext.train_supervised(input=f\"{ROOT_DIR}/fasttext_train.txt\", epoch=20, lr=0.2, wordNgrams=4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Test the Model\n",
    "To test the model, the test dataset is passed.\n",
    "\n",
    "The output is formatted as follows: (Samples, Precision, Recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2584, 0.7801857585139319, 0.7801857585139319)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.test(f\"{ROOT_DIR}/fasttext_test.txt\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Single Inference\n",
    "Here you can test your own sentences with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'dbusername heute mal wieder viel zu spaet sadsmiley'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(('__label__negative',), array([1.00000966]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_predict=\"@DB_Bahn Heute mal wieder viel zu spät :(\"\n",
    "\n",
    "#preprocess input\n",
    "df = pd.DataFrame([to_predict])\n",
    "df = df.apply(data_cleaner.clean_text)\n",
    "to_predict = df[0].item()\n",
    "print(f\"'{to_predict}'\")\n",
    "\n",
    "model.predict(to_predict, k=-1, threshold=0.3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Bert\n",
    "[Bert](https://huggingface.co/transformers/model_doc/bert.html) (Bidirectional Encoder Representations from Transformers) is a model that analyzes not only the words in a text, but also the context.\n",
    "\n",
    "In this notebook PyTorch is used as a backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                   0         1\n",
      "0  dbusername ja weil in wuppertal bauarbeiten si...   neutral\n",
      "1  twitterusername theoretisch kannste dir uebera...  positive\n"
     ]
    }
   ],
   "source": [
    "clean_train_data = pd.read_csv(CLEAN_TRAIN_DF, delimiter='\\t', header=None)\n",
    "clean_test_data = pd.read_csv(CLEAN_TEST_DF, delimiter='\\t', header=None)\n",
    "print(clean_train_data[:2])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Preprocessing\n",
    "The model expects the labels to be ordinal, so they are converted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ordinale labels: {'negative': 0, 'neutral': 1, 'positive': 2}\n"
     ]
    }
   ],
   "source": [
    "# convert labels to ordinals\n",
    "labels_to_ordinal = {target:i for i, target in enumerate(np.unique(clean_train_data[1]))}\n",
    "print(f\"ordinale labels: {labels_to_ordinal}\")\n",
    "        \n",
    "bert_train = pd.DataFrame({'label': np.vectorize(labels_to_ordinal.get)(clean_train_data[1]),\n",
    "                         'text':clean_train_data[0]})\n",
    "\n",
    "bert_test = pd.DataFrame({'label': np.vectorize(labels_to_ordinal.get)(clean_test_data[1]),\n",
    "                         'text':clean_test_data[0]})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text is converted to tokens with start and stop symbols using BERT. Only inputs of 512 characters are supported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc1871db4f36454283885e8d31406c2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67d265dc357c44cfb155920d36d850c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/433 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c66029492e614a3a980cd03cffaf0456",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/249k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d91bb1d06b645839d59dd57b3d72d3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/474k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token werden erstellt, dies kann einige Zeit in anspruch nehmen..\n",
      "Token erstellt!\n",
      "\n",
      "Text: dbusername ja weil in wuppertal bauarbeiten sind soweit bin ich auch aber wieso nur am wochenende und grade jetzt\n",
      "\n",
      "Als Token: [3, 9, 5655, 212, 1431, 3278, 982, 50, 25, 6221, 14779, 3703, 3085, 287, 4133, 4058, 1169, 194, 386, 6724, 26910, 356, 235, 743, 280, 765, 42, 5086, 57, 1868, 4]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-german-cased\", do_lower_case=True)\n",
    "print(\"Token werden erstellt, dies kann einige Zeit in anspruch nehmen..\")\n",
    "train_tokenized = bert_train['text'].astype(str).apply((lambda x: tokenizer.encode(x[:512], add_special_tokens=True)))\n",
    "test_tokenized = bert_test['text'].astype(str).apply((lambda x: tokenizer.encode(x[:512], add_special_tokens=True)))\n",
    "print(\"Token erstellt!\")\n",
    "\n",
    "print(f\"\\nText: {clean_train_data[0].iloc[0]}\")\n",
    "print(f\"\\nAls Token: {train_tokenized[0]}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Padding the Data\n",
    "Since the data should be accessed as performant as possible, they are stored as arrays. The longest character string defines the size of the arrays. All others are padded with 0 at the end. The `Attention Mask` filters out the padded values later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for i in train_tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)\n",
    "        \n",
    "for i in test_tokenized.values:\n",
    "    if len(i) > max_len:\n",
    "        max_len = len(i)        \n",
    "\n",
    "\n",
    "train_padded = np.array([i + [0]*(max_len-len(i)) for i in train_tokenized.values])\n",
    "train_attention_mask = np.where(train_padded != 0, 1, 0)\n",
    "\n",
    "test_padded = np.array([i + [0]*(max_len-len(i)) for i in test_tokenized.values])\n",
    "test_attention_mask = np.where(test_padded != 0, 1, 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Prepare Data for GPU\n",
    "Here datasets are created for training and testing. The token sets, masks and labels are passed. The `Dataloader` loads the datasets with the size `BATCH_SIZE` if needed to the GPU to not load the GPU memory by the dataset. In this case 50 entries are processed simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.tensor(train_padded),\n",
    "                             torch.tensor(train_attention_mask),\n",
    "                             torch.tensor(list(bert_train['label'])))\n",
    "\n",
    "test_dataset = TensorDataset(torch.tensor(test_padded),\n",
    "                            torch.tensor(test_attention_mask),\n",
    "                            torch.tensor(list(bert_test['label'])))\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            sampler = RandomSampler(train_dataset), \n",
    "            batch_size = BATCH_SIZE\n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            test_dataset,\n",
    "            sampler = SequentialSampler(test_dataset),\n",
    "            batch_size = BATCH_SIZE\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.4 Creating the Model\n",
    "A BERT model with 12 layers is created, the number of labels must match the dataset (here \"positive\", \"neutral\", \"negative\").\n",
    "\n",
    "The \"cuda()\" call moves the model to the GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-german-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-german-cased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30000, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-german-cased\",\n",
    "    num_labels = 3,\n",
    "    output_attentions = False,\n",
    "    output_hidden_states = False,\n",
    ")\n",
    "model.cuda()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Optimizer and Learning Rate\n",
    "As optimizer we use Adam, because it converges very fast.\n",
    "To process as much data as possible at the same time, we use `mixed-precision`, i.e. float16 values are used instead of float32 values if possible to save resources. Details about `mixed-precision` with PyTorch: https://pytorch.org/docs/stable/notes/amp_examples.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = Adam(model.parameters(),\n",
    "                  lr = 1e-5, # args.learning_rate - default is 5e-5,\n",
    "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    "                )\n",
    "scaler = GradScaler() # necessary for mixed precision training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.5 Prepare Training\n",
    "The number of epochs can be set via `NUM_EPOCHS`. The learning rate is controlled by a scheduler.\n",
    "The defined functions are used to display the progress in the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5\n",
    "\n",
    "total_steps = len(train_dataloader) * NUM_EPOCHS\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)\n",
    "\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.6 Training the Model\n",
    "The training process consists of the training phase with the training set and validation phase with the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1/5 ========\n",
      "Training...\n",
      "  Batch    40  of    655.    Elapsed: 0:00:07.\n",
      "  Batch    80  of    655.    Elapsed: 0:00:14.\n",
      "  Batch   120  of    655.    Elapsed: 0:00:21.\n",
      "  Batch   160  of    655.    Elapsed: 0:00:28.\n",
      "  Batch   200  of    655.    Elapsed: 0:00:34.\n",
      "  Batch   240  of    655.    Elapsed: 0:00:41.\n",
      "  Batch   280  of    655.    Elapsed: 0:00:48.\n",
      "  Batch   320  of    655.    Elapsed: 0:00:54.\n",
      "  Batch   360  of    655.    Elapsed: 0:01:01.\n",
      "  Batch   400  of    655.    Elapsed: 0:01:08.\n",
      "  Batch   440  of    655.    Elapsed: 0:01:14.\n",
      "  Batch   480  of    655.    Elapsed: 0:01:21.\n",
      "  Batch   520  of    655.    Elapsed: 0:01:27.\n",
      "  Batch   560  of    655.    Elapsed: 0:01:34.\n",
      "  Batch   600  of    655.    Elapsed: 0:01:41.\n",
      "  Batch   640  of    655.    Elapsed: 0:01:47.\n",
      "  Average training loss: 0.59\n",
      "  Training epoch took: 0:01:50\n",
      "\n",
      " Running Validation...\n",
      " Validation Accuracy: 0.80\n",
      "  Validation Loss: 0.50\n",
      "  Validation Time: 0:00:10\n",
      "\n",
      "======== Epoch 2/5 ========\n",
      "Training...\n",
      "  Batch    40  of    655.    Elapsed: 0:00:07.\n",
      "  Batch    80  of    655.    Elapsed: 0:00:13.\n",
      "  Batch   120  of    655.    Elapsed: 0:00:20.\n",
      "  Batch   160  of    655.    Elapsed: 0:00:26.\n",
      "  Batch   200  of    655.    Elapsed: 0:00:33.\n",
      "  Batch   240  of    655.    Elapsed: 0:00:39.\n",
      "  Batch   280  of    655.    Elapsed: 0:00:46.\n",
      "  Batch   320  of    655.    Elapsed: 0:00:52.\n",
      "  Batch   360  of    655.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    655.    Elapsed: 0:01:05.\n",
      "  Batch   440  of    655.    Elapsed: 0:01:12.\n",
      "  Batch   480  of    655.    Elapsed: 0:01:18.\n",
      "  Batch   520  of    655.    Elapsed: 0:01:25.\n",
      "  Batch   560  of    655.    Elapsed: 0:01:32.\n",
      "  Batch   600  of    655.    Elapsed: 0:01:38.\n",
      "  Batch   640  of    655.    Elapsed: 0:01:45.\n",
      "  Average training loss: 0.44\n",
      "  Training epoch took: 0:01:47\n",
      "\n",
      " Running Validation...\n",
      " Validation Accuracy: 0.82\n",
      "  Validation Loss: 0.47\n",
      "  Validation Time: 0:00:10\n",
      "\n",
      "======== Epoch 3/5 ========\n",
      "Training...\n",
      "  Batch    40  of    655.    Elapsed: 0:00:07.\n",
      "  Batch    80  of    655.    Elapsed: 0:00:13.\n",
      "  Batch   120  of    655.    Elapsed: 0:00:20.\n",
      "  Batch   160  of    655.    Elapsed: 0:00:26.\n",
      "  Batch   200  of    655.    Elapsed: 0:00:33.\n",
      "  Batch   240  of    655.    Elapsed: 0:00:39.\n",
      "  Batch   280  of    655.    Elapsed: 0:00:46.\n",
      "  Batch   320  of    655.    Elapsed: 0:00:52.\n",
      "  Batch   360  of    655.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    655.    Elapsed: 0:01:05.\n",
      "  Batch   440  of    655.    Elapsed: 0:01:12.\n",
      "  Batch   480  of    655.    Elapsed: 0:01:18.\n",
      "  Batch   520  of    655.    Elapsed: 0:01:25.\n",
      "  Batch   560  of    655.    Elapsed: 0:01:31.\n",
      "  Batch   600  of    655.    Elapsed: 0:01:38.\n",
      "  Batch   640  of    655.    Elapsed: 0:01:44.\n",
      "  Average training loss: 0.35\n",
      "  Training epoch took: 0:01:47\n",
      "\n",
      " Running Validation...\n",
      " Validation Accuracy: 0.81\n",
      "  Validation Loss: 0.49\n",
      "  Validation Time: 0:00:10\n",
      "\n",
      "======== Epoch 4/5 ========\n",
      "Training...\n",
      "  Batch    40  of    655.    Elapsed: 0:00:07.\n",
      "  Batch    80  of    655.    Elapsed: 0:00:14.\n",
      "  Batch   120  of    655.    Elapsed: 0:00:20.\n",
      "  Batch   160  of    655.    Elapsed: 0:00:27.\n",
      "  Batch   200  of    655.    Elapsed: 0:00:34.\n",
      "  Batch   240  of    655.    Elapsed: 0:00:40.\n",
      "  Batch   280  of    655.    Elapsed: 0:00:47.\n",
      "  Batch   320  of    655.    Elapsed: 0:00:53.\n",
      "  Batch   360  of    655.    Elapsed: 0:01:00.\n",
      "  Batch   400  of    655.    Elapsed: 0:01:06.\n",
      "  Batch   440  of    655.    Elapsed: 0:01:13.\n",
      "  Batch   480  of    655.    Elapsed: 0:01:19.\n",
      "  Batch   520  of    655.    Elapsed: 0:01:26.\n",
      "  Batch   560  of    655.    Elapsed: 0:01:32.\n",
      "  Batch   600  of    655.    Elapsed: 0:01:39.\n",
      "  Batch   640  of    655.    Elapsed: 0:01:45.\n",
      "  Average training loss: 0.29\n",
      "  Training epoch took: 0:01:48\n",
      "\n",
      " Running Validation...\n",
      " Validation Accuracy: 0.81\n",
      "  Validation Loss: 0.52\n",
      "  Validation Time: 0:00:10\n",
      "\n",
      "======== Epoch 5/5 ========\n",
      "Training...\n",
      "  Batch    40  of    655.    Elapsed: 0:00:07.\n",
      "  Batch    80  of    655.    Elapsed: 0:00:13.\n",
      "  Batch   120  of    655.    Elapsed: 0:00:20.\n",
      "  Batch   160  of    655.    Elapsed: 0:00:26.\n",
      "  Batch   200  of    655.    Elapsed: 0:00:33.\n",
      "  Batch   240  of    655.    Elapsed: 0:00:39.\n",
      "  Batch   280  of    655.    Elapsed: 0:00:46.\n",
      "  Batch   320  of    655.    Elapsed: 0:00:53.\n",
      "  Batch   360  of    655.    Elapsed: 0:00:59.\n",
      "  Batch   400  of    655.    Elapsed: 0:01:06.\n",
      "  Batch   440  of    655.    Elapsed: 0:01:12.\n",
      "  Batch   480  of    655.    Elapsed: 0:01:19.\n",
      "  Batch   520  of    655.    Elapsed: 0:01:26.\n",
      "  Batch   560  of    655.    Elapsed: 0:01:32.\n",
      "  Batch   600  of    655.    Elapsed: 0:01:39.\n",
      "  Batch   640  of    655.    Elapsed: 0:01:46.\n",
      "  Average training loss: 0.24\n",
      "  Training epoch took: 0:01:48\n",
      "\n",
      " Running Validation...\n",
      " Validation Accuracy: 0.80\n",
      "  Validation Loss: 0.55\n",
      "  Validation Time: 0:00:10\n",
      "\n",
      "Training complete!\n",
      "Total training took 0:09:51 (h:mm:ss)\n"
     ]
    }
   ],
   "source": [
    "# use cuda device\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# same seed for reproducibility\n",
    "seed_val = 42\n",
    "random.seed(seed_val)\n",
    "np.random.seed(seed_val)\n",
    "torch.manual_seed(seed_val)\n",
    "torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "# training statistics \n",
    "training_stats = []\n",
    "# time measure\n",
    "total_t0 = time.time()\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    # ========================================\n",
    "    # Training phase\n",
    "    # ========================================\n",
    "    print(f'\\n======== Epoch {epoch+1}/{NUM_EPOCHS} ========\\nTraining...')\n",
    "\n",
    "    #training time for epoch\n",
    "    t0 = time.time()\n",
    "\n",
    "    # reset total loss\n",
    "    total_train_loss = 0\n",
    "\n",
    "    # change model mode to training\n",
    "    model.train()\n",
    "\n",
    "    for step, (tokens, att_mask, labels) in enumerate(train_dataloader):\n",
    "        # Progress update every 20 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0) # elapsed time\n",
    "            print(f'  Batch {step:>5,}  of  {len(train_dataloader):>5,}.    Elapsed: {elapsed}.')\n",
    "\n",
    "        tokens = tokens.to(device)\n",
    "        att_mask = att_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # clear calculated gradiends\n",
    "        optimizer.zero_grad()    \n",
    "        \n",
    "        # calculate gradients\n",
    "        with autocast():\n",
    "            # train model with current batch\n",
    "            outputs = model(tokens, token_type_ids=None, attention_mask=att_mask, labels=labels)\n",
    "            loss, logits = outputs.loss, outputs.logits\n",
    "            \n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.unscale_(optimizer)\n",
    "        \n",
    "        # Clip the norm of the gradients to 1.0.\n",
    "        # This is to help prevent the \"exploding gradients\" problem.\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # Update parameters and take a step using the computed gradient\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "        \n",
    "        # training loss of all batches for statistics\n",
    "        total_train_loss += loss.item()\n",
    "        \n",
    "        # Update the learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "    print(f\"  Average training loss: {avg_train_loss:.2f}\\n  Training epoch took: {training_time}\")\n",
    "        \n",
    "        \n",
    "    # ========================================\n",
    "    #  Validation phase\n",
    "    # ========================================\n",
    "    print(\"\\n Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "    # put the model in evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    for (tokens, att_mask, labels) in validation_dataloader:\n",
    "        tokens = tokens.to(device)\n",
    "        att_mask = att_mask.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # constructing the compute graph is only needed for training\n",
    "        with torch.no_grad():        \n",
    "            # the documentation for this model function is here: \n",
    "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
    "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
    "            # values prior to applying an activation function like the softmax.\n",
    "            outputs = model(tokens, token_type_ids=None, attention_mask=att_mask, labels=labels)\n",
    "            loss, logits = outputs.loss, outputs.logits\n",
    "            \n",
    "        # accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        labels = labels.to('cpu').numpy()\n",
    "\n",
    "        # calculate the accuracy \n",
    "        total_eval_accuracy += flat_accuracy(logits, labels)\n",
    "        \n",
    "    # report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    # calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    # measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(f\" Validation Accuracy: {avg_val_accuracy:.2f}\")\n",
    "    print(f\"  Validation Loss: {avg_val_loss:.2f}\")\n",
    "    print(f\"  Validation Time: {validation_time}\")\n",
    "\n",
    "    # record all statistics from this epoch.\n",
    "    training_stats.append({\n",
    "            'epoch': epoch + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\\nTraining complete!\")\n",
    "print(f\"Total training took {format_time(time.time()-total_t0)} (h:mm:ss)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.7 Single Inference\n",
    "Here you can test your own sentences with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vorverarbeiteter Text: 'dbusername heute wieder spaeter'\n",
      "Tokenized: [3, 9, 5655, 212, 1431, 1138, 525, 338, 4173, 60, 4]\n",
      "Ergebnis: negative\n"
     ]
    }
   ],
   "source": [
    "to_predict=\"@DB_Bahn heute wieder später\"\n",
    "ordinal_to_labels = {v: k for k, v in labels_to_ordinal.items()}\n",
    "\n",
    "#preprocess input\n",
    "df=pd.DataFrame([to_predict])\n",
    "df=df.apply(data_cleaner.clean_text)\n",
    "to_predict=(df[0].item())\n",
    "\n",
    "print(f\"Vorverarbeiteter Text: '{to_predict}'\")\n",
    "\n",
    "tokenized=tokenizer.encode(to_predict[:512])\n",
    "print(f\"Tokenized: {tokenized}\")\n",
    "\n",
    "#predict input\n",
    "tokens = torch.tensor([tokenized]).cuda()\n",
    "with torch.no_grad(): \n",
    "    model.eval()\n",
    "    output = model(tokens, token_type_ids=None)\n",
    "    logits = output.logits\n",
    "\n",
    "prediction = np.argmax(logits.detach().cpu().numpy(), axis=1)[0]\n",
    "print(f\"Ergebnis: {ordinal_to_labels[prediction]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AMI",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Mar 28 2022, 06:59:08) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "b009b95a6445f66c226df8b0b8ef690797f5b0a25adbc81fcf2df93f5b343cd3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
