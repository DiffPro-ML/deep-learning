{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bildklassifikation mit dem MURA-Datensatz\n",
    "\n",
    "Bei der Klassifikation geht es darum, einem Bild ein Label aus einer vorgegebenen Menge an möglichen Labels zuzuweisen. Im Vergleich zur Segmentierung und Objekt Detection ist diese Aufgabe die einfachste.\n",
    "\n",
    "In diesem Notebook wird ein Klassifikationsmodell für den MURA-Datensatz mit Keras und Tensorflow als Backend trainiert.\n",
    "\n",
    "## Datensatz\n",
    "\n",
    "Der [MURA](https://stanfordmlgroup.github.io/competitions/mura/) (**mu**sculoskeletal **ra**diographs) Datensatz besteht aus 40.561 Röntgenbildern von Oberkörperextremitäten der Kategorien: Ellbogen, Finger, Unterarm, Hand, Oberarm, Schulter, und Handgelenk. Zusätzlich ist jedem Bild das Label normal oder abnormal zugeordnet.\n",
    "\n",
    "![XR_HAND](http://s1.saviola.de/screens/mura.png)\n",
    "\n",
    "In diesem Jupyter-Notebook soll ein Model trainiert werden welches Bilder des MURA-Datensatzes in eine der sieben Extremitätsklassen einordnen kann.\n",
    "\n",
    "## Jupyter-Notebook\n",
    "\n",
    "Jupyter-Notebooks sind interaktive Python-Scripte, in denen Markdown und sogar Latex zur Dokumentation verwendet werden kann. In diesem Abschnitt sollen einerseits allgemeine Informationen wie Tastenkombinationen und übliche Workflows, andererseits aber auch für diesen Workshop und die verwendete Hardware spezifische Informationen im Umgang mit Python und Jupyter-Notebooks geliefert werden.\n",
    "\n",
    "### Wichtiges auf einen Blick (a.k.a. TL;DR)\n",
    "\n",
    "- **beim Wechseln auf ein anderes Notebook immer den Kernel beenden (Kernel -> Shutdown)**\n",
    "- **Shift + Enter** zum Ausführen der aktiven Zelle\n",
    "- **Strg + Shift + p** zum Öffnen der Kommandopalette\n",
    "- **Shift + o** zum Togglen des Zell-Scrollings\n",
    "- bei Fehlermeldungen im Zweifel **Kernel neustarten (Kernel -> Restart)**\n",
    "- \"Hilfe, ich sehe den Markdown-Code\" -> **Shift + Enter** in der entsprechenden Zelle\n",
    "- \"Hilfe, ich bekommen ResourceExhaustion / OutOfMemory (OOM) Fehler\" -> **Kernel neustarten, Kernel von anderen noch laufenden Notebooks herunterfahren** (oben links auf das Jupyter-Logo klicken, dann auf den Tab \"Running\")\n",
    "- \"Hilfe, mein Notebook ist kaputt\" -> siehe **Notebook \"Wiederherstellung\"**\n",
    "- \"Passiert da noch was?\" -> ist der **Kreis oben rechts neben \"Python 3\", ausgefüllt und dunkel** dann ist der Kernel noch am Arbeiten, ist er nicht gefüllt dann ist der Kernel untätig. Die aktuell laufende Zelle ist die von oben gesehen erste bei der auf der linken Seite \"In[*]\" anstelle von z.B. \"In[5]\" steht. Es kann aber passieren dass sich der Kernel aufhängt, in dem Fall einfach oben auf __Kernel -> Interrupt__ und die Zelle erneut ausführen\n",
    "\n",
    "\n",
    "### Überblick & Workflow\n",
    "\n",
    "Die einzigen beiden Shortcuts die man sich eigentlich nur merken muss sind\n",
    "\n",
    "- **Shift + Enter** zum Ausführen einer Zelle, und\n",
    "- **Strg + Shift + p** zum Öffnen der Kommandopalette, von der aus man dann direkt Zugriff auf alle möglichen Befehle hat, inklusive entsprechender Shortcuts\n",
    "\n",
    "Ein weiterer nützlicher Shortcut ist **Shift + o**, welcher das **Scrolling für Zellenoutput** umschaltet.\n",
    "\n",
    "Zum **Editieren einer Zelle** genügt ein Doppelklick in die Zelle, bei Code-Zellen reicht es zum Beenden des Editiermodus einfach außerhalb der Zelle zu klicken, bei Dokumentationszellen (wie dieser hier) ist eine Ausführung der Zelle nötig um die Code-Ansicht zu verlassen.\n",
    "\n",
    "Während eine Zelle ausgeführt wird, wechselt der Kernel-Indikator oben rechts neben \"Python 3\" von einem hellen Kreis mit dunklem Rand zu einem ausgefüllten dunklen Kreis und springt wieder zurück sobald die Ausführung beendet ist. Wenn mehrere Zellen gleichzeitig ausgeführt wurden, kann man an dem Label in der linken Spalte ablesen, ob die Zelle fertig ausgeführt wurde (**In [ZAHL]:**) oder ob sie gerade ausgeführt wird bzw. auf Ausführung wartet (**In [*]:**). Zusätzlich wird nach Ausführung einer Zelle die Zellenausgabe unterhalb der Zelle angezeigt.\n",
    "\n",
    "Um den Überblick zu behalten kann es manchmal sinnvoll sein, die **Zellenausgabe zu löschen**. Dies kann u.a. auf diesen beiden Wegen erfolgen:\n",
    "\n",
    "- oben auf Cell -> Current Outputs / All outputs -> Clear\n",
    "- oben auf Kernel -> Restart & Clear Output\n",
    "\n",
    "Der Kernel ist für die Ausführung des Python-Codes zuständig und behält den Kontext (also belegte Variablen, definierte Funktionen und belegter Speicher) seit dem letzten Kernel-(Neu)start. Dies kann zu Problemen führen wenn Zellen in anderer Reihenfolge ausgeführt werden oder Zellen übersprungen werden in denen Variablen oder Funktionen definiert werden die im weiteren Verlauf des Scripts benötigt werden, aber auch wenn **ein anderes Jupyter-Notebook gestartet wird**, da dafür ein weiterer Kernel gestartet wird.\n",
    "\n",
    "Deswegen beim **Wechseln auf ein anderes Notebook** immer den **Kernel herunterfahren oder neustarten** (oben Kernel -> Restart/Shutdown)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports und Tensorflow-Setup\n",
    "\n",
    "Neben den benötigten Imports wird wird hier Tensorflow angewiesen, nur maximal 40% des GPU-RAMs zu belegen, damit mehrere Trainingsprozesse gleichzeitig auf jeder GPU ausgeführt werden können. Überspringt man diesen Schritt, dann reserviert Tensorflow den gesamten Grafikspeicher. Ein wichtiger Faktor für den beim Training benötigen Grafikspeicher ist die gewählte Batch-Größe, auf die später im Script noch genauer eingangen wird.\n",
    "\n",
    "Bitte diese Zahl nicht ändern, da mehrere Trainingsprozesse auf einer GPU ausgeführt werden und es zu Crashes kommen kann wenn der GPU-Speicher voll läuft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import PIL.Image as Image\n",
    "import matplotlib.pyplot as plt\n",
    "import logging\n",
    "from scipy.interpolate import make_interp_spline, BSpline\n",
    "\n",
    "# Make sure TF does not print confusing warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "logging.getLogger(\"tensorflow\").setLevel(logging.ERROR)\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.util import deprecation\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "tf.logging.set_verbosity(tf.logging.INFO)\n",
    "\n",
    "from tensorflow import ConfigProto\n",
    "# Tell TF to not use all GPU RAM\n",
    "config = ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = .40\n",
    "session = tf.Session(config=config)\n",
    "print(\"Tensorflow setup done\")\n",
    "\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras import layers\n",
    "from tensorflow.python.keras.callbacks import Callback\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.python.keras.applications.resnet50 import ResNet50\n",
    "\n",
    "from tensorflow.python.keras import backend as K\n",
    "\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Functions for visualization\n",
    "# See https://www.tensorflow.org/tutorials/images/hub_with_keras\n",
    "def image_to_network_input(filename):\n",
    "    im = Image.open(filename)\n",
    "    plt.imshow(np.asarray(im))\n",
    "\n",
    "    # Convert to RGB and resize\n",
    "    im_converted = im.convert(mode='RGB').resize((IMAGE_SIZE,IMAGE_SIZE))\n",
    "\n",
    "    # Convert to array and add batch dimension\n",
    "    X_test = np.array(im_converted)/255.0\n",
    "    X_test = X_test[np.newaxis, ...]\n",
    "    \n",
    "    return X_test\n",
    "\n",
    "def print_prediction_result(result):\n",
    "    print('Predictions:')\n",
    "    prediction_sum = np.sum(result)\n",
    "    i = 0\n",
    "    for x in np.nditer(result):\n",
    "        print('{}: {:.3f}%'.format(CLASSES[i], x/prediction_sum*100))\n",
    "        i+=1\n",
    "        \n",
    "    predicted_class = CLASSES[np.argmax(result[0], axis=-1)]\n",
    "    print('Predicted class: ' + predicted_class)\n",
    "\n",
    "def smooth(values, num_points = 100):\n",
    "    xnew = np.linspace(0,len(values)-1,num_points) #100 represents number of points to make between T.min and T.max\n",
    "    spl = make_interp_spline(list(range(len(values))), values, k=3) #BSpline object\n",
    "    values_smooth = spl(xnew)\n",
    "    \n",
    "    return xnew, values_smooth\n",
    "    \n",
    "class LossHistory(Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.loss = []\n",
    "        self.acc = []\n",
    "\n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.loss.append(logs.get('loss'))\n",
    "        self.acc.append(logs.get('acc'))\n",
    "\n",
    "\n",
    "def random_crop(img, random_crop_size):\n",
    "    # Note: image_data_format is 'channel_last'\n",
    "    assert img.shape[2] == 3\n",
    "    height, width = img.shape[0], img.shape[1]\n",
    "    dy, dx = random_crop_size\n",
    "    x = np.random.randint(0, width - dx + 1)\n",
    "    y = np.random.randint(0, height - dy + 1)\n",
    "    return img[y:(y+dy), x:(x+dx), :]\n",
    "\n",
    "\n",
    "def crop_generator(batches, crop_length):\n",
    "    \"\"\"Take as input a Keras ImageGen (Iterator) and generate random\n",
    "    crops from the image batches generated by the original iterator.\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        batch_x, batch_y = next(batches)\n",
    "        batch_crops = np.zeros((batch_x.shape[0], crop_length, crop_length, 3))\n",
    "        for i in range(batch_x.shape[0]):\n",
    "            batch_crops[i] = random_crop(batch_x[i], (crop_length, crop_length))\n",
    "        yield (batch_crops, batch_y)\n",
    "\n",
    "def find_last(model_directory):\n",
    "        \"\"\"Finds the last checkpoint file of the last trained model in the\n",
    "        model directory.\n",
    "        Returns:\n",
    "            The path of the last checkpoint file\n",
    "        \"\"\"\n",
    "        \n",
    "        checkpoints = next(os.walk(model_directory))[2]\n",
    "        checkpoints = filter(lambda f: f.endswith(\".hdf5\"), checkpoints)\n",
    "        checkpoints = sorted(checkpoints)\n",
    "        if not checkpoints:\n",
    "            import errno\n",
    "            raise FileNotFoundError(\n",
    "                errno.ENOENT, \"Could not find weight files in {}\".format(model_directory))\n",
    "        checkpoint = os.path.join(model_directory, checkpoints[-1])\n",
    "        return checkpoint\n",
    "        \n",
    "        \n",
    "print(\"Imports and setup done\")\n",
    "\n",
    "ROOT_DIR = os.path.abspath(\"/workspace\")\n",
    "DATA_DIR = os.path.abspath(\"/data\")\n",
    "MODEL_DIR = os.path.join(ROOT_DIR, \"models/classification\")\n",
    "PRETRAINED_MODEL_DIR = os.path.abspath(\"/models\")\n",
    "\n",
    "# Solutions\n",
    "#NUM_CLASSES =\n",
    "#LEARN_RATE = \n",
    "#CONV_BASE_TRAINABLE = \n",
    "\n",
    "print(\"Directories:\")\n",
    "print(\"Root directory:\", ROOT_DIR)\n",
    "print(\"Model directory:\", MODEL_DIR)\n",
    "print(\"Pre-trained model directory:\", PRETRAINED_MODEL_DIR)\n",
    "print(\"Datasets directory:\", DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die wichtigsten Ordner sind:\n",
    "\n",
    "- `ROOT_DIR`: Das Hauptverzeichnis in dem sich die Jupyter-Notebooks befinden\n",
    "- `MODEL_DIR`: Verzeichnis in dem trainierte Modelle abgespeichert werden\n",
    "- `PRETRAINED_MODEL_DIR`: Verzeichnis für vortrainierte Modelle (read-only)\n",
    "- `DATA_DIR`: Verzeichnis in dem sich die Datensätze befinden\n",
    "\n",
    "Hier kann mit einfachen Linux-Kommandozeilenbefehlen die Ordnerstruktur untersucht werden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -l /data\n",
    "!ls -l /data/MURA-v1.1\n",
    "!ls -l /data/MURA-v1.1/train\n",
    "!ls -l /workspace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Datensatz\n",
    "\n",
    "Hier wird der Datensatz definiert. Es gibt in Keras und Tensorflow viele verschiedene Wege Daten einzulesen, in diesem Beispiel wird der [ImageDataGenerator](https://keras.io/preprocessing/image/) verwendet, der aufgrund eines Verzeichnis und Vorverarbeitungsoptionen Bilder in der folgenden Ordnerstruktur `/path/to/dataset/split/class/image.png` einliest und vorverarbeitet (Data Augmentation). Die verschiedenen Optionen zur Data Augmentation können unter dem o.g. Link nachgelesen werden und können später zur Optimierung genutzt werden.\n",
    "\n",
    "In unserem Beispiel ist der Name des Datensatz-Ordners `MURA-v1.1` und die möglichen __Splits__ sind `train` und `valid`, die Klassen entsprechen den Ordnernamen innerhalb der Unterverzeichnisse `train/` und `valid/`.\n",
    "\n",
    "Eine wichtige Variable hier ist `BATCH_SIZE`, welche bestimmt wie viele Bilder gleichzeitig in den Grafikspeicher geladen werden müssen. Wählt man diesen Wert zu hoch, so wird der reservierte Grafikspeicher nicht ausreichen, wählt man ihn zu niedrig so wird das Training sehr langsam. Für unser Beispiel soll dieser Wert auf 16 gesetzt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Create dataset\n",
    "##\n",
    "\n",
    "print(\"Start preparing dataset... please wait, this may take a while\")\n",
    "\n",
    "BATCH_SIZE = 16 #8\n",
    "IMAGE_SIZE = 128 #400\n",
    "PRE_CROPPED_SIZE = 150 #450\n",
    "\n",
    "# Dataset directory\n",
    "DATASET_DIR = '/data/MURA-v1.1/'\n",
    "\n",
    "CLASSES = ['XR_ELBOW', 'XR_FINGER', 'XR_FOREARM', 'XR_HAND', 'XR_HUMERUS', 'XR_SHOULDER', 'XR_WRIST']\n",
    "\n",
    "# make sure these numbers are correct, you can count images using something like\n",
    "# find path/to/dataset/split -name \".png\" | wc -l\n",
    "# or you just run the script once and look for the\n",
    "# \"Found X images belonging to Y classes.\" lines\n",
    "NUM_TRAINING_SAMPLES = 36808\n",
    "NUM_VALIDATION_SAMPLES = 3197\n",
    "\n",
    "# Data augmentation\n",
    "# see https://keras.io/preprocessing/image/\n",
    "# see https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        #interpolation_order=2,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "# only rescale validation data\n",
    "validation_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "# this is a generator that will read pictures found in subfolers of '/train',\n",
    "# in the dataset directory and indefinitely generate batches of augmented image\n",
    "# data\n",
    "print(\"Training data:\")\n",
    "train_generator_raw = train_datagen.flow_from_directory(\n",
    "        DATASET_DIR + '/train',  # this is the target directory\n",
    "        target_size=(PRE_CROPPED_SIZE, PRE_CROPPED_SIZE),  # all images will be resized to 128x128\n",
    "        batch_size=BATCH_SIZE,\n",
    "        interpolation=\"lanczos\",\n",
    "        class_mode='categorical')\n",
    "\n",
    "train_generator = crop_generator(train_generator_raw, IMAGE_SIZE)\n",
    "\n",
    "# this is a similar generator, for validation data\n",
    "print(\"Validation data:\")\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        DATASET_DIR + '/valid',\n",
    "        target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "        batch_size=BATCH_SIZE,\n",
    "        interpolation=\"lanczos\",\n",
    "        class_mode='categorical')\n",
    "\n",
    "print(\"Dataset prepared\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. Bilder anzeigen lassen\n",
    "\n",
    "Im Folgedenen werden vier zufällige Bilder aus dem (Trainings-)Datensatz in Originalauflösung extrahiert. Um stattdessen Bilder aus dem Validierungsdatensatz anzuzeigen, einfach das `'/train'` in der ersten Zeile zu `'/valid'` ändern.\n",
    "\n",
    "Dieser Block kann beliebig oft ausgeführt werden, die ausgewählten Bilder werden jedes Mal zufällig neu ausgewählt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_generator = ImageDataGenerator().flow_from_directory(DATASET_DIR + '/train', batch_size=1, class_mode='categorical')\n",
    "\n",
    "NUM_IMAGES = 4\n",
    "i = 1\n",
    "for x_batch, y_batch in example_generator:\n",
    "    plt.figure()\n",
    "    plt.title(CLASSES[np.where(y_batch[0] == 1.)[0][0]])\n",
    "    plt.imshow(x_batch[0].astype(int))\n",
    "    i += 1\n",
    "    \n",
    "    if i > NUM_IMAGES:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='from_scratch'></a>\n",
    "\n",
    "# 3. Modell-Erstellung (from scratch)\n",
    "\n",
    "Falls Sie das Training from Scratch bereits durchgeführt haben, klicken Sie [hier](#pre_trained) um zum nächsten Schritt zu springen.\n",
    "\n",
    "Als nächstes wird das Modell erstellt. In der Variable `MODEL_DIR` ist das Verzeichnis angegeben, in dem das trainierte Modell gespeichert und (beim Fortsetzen des Trainings) geladen werden soll.\n",
    "\n",
    "`conv_base` enthält das Basis-Modell, in diesem Fall ein [ResNet50](https://www.kaggle.com/keras/resnet50), andere Möglichkeiten sind auf der [keras.io](https://keras.io/applications/)-Webseite aufgelistet (dafür müssen aber auch oben die Imports angepasst werden).\n",
    "\n",
    "Auf das Basis-Modell wird eine neue fully-connected Schicht mit 1000 Knoten gesetzt, und darüber eine weitere fully-connected Ausgabeschicht deren Anzahl Knoten der Anzahl Klassen entspricht.\n",
    "\n",
    "Abschließend wird das Modell kompiliert, wobei die Loss-Funktion, ein Optimizer (hier kann die Lernrate angepasst werden, s.u.), und die zu generierenden Metriken übergeben werden.\n",
    "\n",
    "Hier wird direkt das komplette Modell trainiert, da alle Schichten zufällig initialisiert wurden und ein zweiphasiges Training daher nicht sinnvoll ist.\n",
    "\n",
    "### Optimizer und Lernrate\n",
    "\n",
    "Keras unterstützt [eine Reihe](https://keras.io/optimizers/) von Optimizern mit verschiedenen Parametern. Optimizer sind für die Anpassung der Gewichte zuständig. Wir werden für unser Training zwei verschiedene Optimizer nutzen:\n",
    "\n",
    "- [Adam](https://arxiv.org/abs/1412.6980v8)\n",
    "- [Stochastic Gradient Descent (SGD)](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)\n",
    "\n",
    "Die wichtigsten Parameter die von beiden Optimizern unterstützt werden:\n",
    "\n",
    "- `lr`, die Lernrate\n",
    "- `decay`, learning rate decay\n",
    "\n",
    "Für die Lernrate liegen übliche zwischen `0.001` und `0.00001`. Größere Lernrate ermöglichen schnelleres Training, können aber auch zu problematischen Sprüngen im Lösungsraum und Divergenz (Fehler gleichbleibend schlecht) führen. Kleinere Lernraten eignen sich für das Fine-Tuning.\n",
    "\n",
    "Der learning rate decay sorgt dafür, dass die Lernrate im Laufe des Trainings immer geringer wird. Die Idee ist mit einer großen Lernrate zu starten um so schnell in die Nähe eines lokalen Minimums zu gelangen, und dann mit einer geringeren Lernrate diesem lokalen Minimum möglichst nahe zu kommen. Der Wert sollte hier abhängig von der Anzahl Epochen und Batches gewählt werden. Die Formel für die Berechnung der aktuellen Lernrate ist\n",
    "\n",
    "\\begin{equation*}\n",
    "l = l_{init} * \\frac{1}{1 + d * i}\n",
    "\\end{equation*}\n",
    "\n",
    "mit $l$ = Lernrate, $l_{init}$ = initiale Lernrate, $d$ = decay, $i$ = Iterationen (Anzahl verarbeiteter Batches).\n",
    "\n",
    "Hier ein paar Beispiele mit verschiedenen Lernraten und $d = 0.00025$:\n",
    "\n",
    "<style type=\"text/css\" rel=\"stylesheet\">\n",
    ".foo table { width: 300px; }\n",
    "</style>\n",
    "<div class=\"foo\">\n",
    "    \n",
    "| Epoche | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; Formel &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | $l$ bei $l_{init} = 0.001$ | $l$ bei $l_{init} = 0.0005$ |\n",
    "| ---------- | ---------- | ---------- | ---------- |\n",
    "| 0 | $l_{init} * \\frac{1}{1 + 0.00025 * 0*2300}$ | &nbsp; &nbsp; 0.001 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; | &nbsp; &nbsp; &nbsp; 0.0005 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; |\n",
    "| 1 | $l_{init} * \\frac{1}{1 + 0.00025 * 1*2300}$ | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.000634921 | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.00031746 &nbsp; |\n",
    "| 2 | $l_{init} * \\frac{1}{1 + 0.00025 * 2*2300}$ | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.000465116 | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  0.000232558 |\n",
    "| 3 | $l_{init} * \\frac{1}{1 + 0.00025 * 3*2300}$ | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.000366972 | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  0.000183486 |\n",
    "| 4 | $l_{init} * \\frac{1}{1 + 0.00025 * 4*2300}$ | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.000303030 | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  0.000151515 |\n",
    "| 5 | $l_{init} * \\frac{1}{1 + 0.00025 * 5*2300}$ | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 0.000258065 | &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;  0.000129032 |\n",
    "\n",
    "</div>\n",
    "\n",
    "Aufgaben:\n",
    "\n",
    "- Ersetzen Sie die Variable `NUM_CLASSES` durch die Anzahl Klassen\n",
    "- Ersetzen Sie die Variable `LEARN_RATE` durch eine geeignete Lernrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "# Create model\n",
    "##\n",
    "\n",
    "print(\"Creating model\")\n",
    "\n",
    "os.makedirs(MODEL_DIR, 0o777, exist_ok=True)\n",
    "print(\"model_dir: \", os.path.realpath(MODEL_DIR))\n",
    "\n",
    "model_file = os.path.join(MODEL_DIR, \"model_from_scratch.h5\")\n",
    "\n",
    "# Create Keras model\n",
    "# see https://keras.io/applications/\n",
    "conv_base = ResNet50(weights=None,\n",
    "                  include_top=False,\n",
    "                  input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "# Add more layers here if needed\n",
    "model.add(layers.Dense(1000, activation='relu'))\n",
    "# The number of nodes in the last layer should be equal to the number of classes\n",
    "model.add(layers.Dense(NUM_CLASSES, activation='softmax'))\n",
    "# Enable training for the base model\n",
    "conv_base.trainable = True\n",
    "\n",
    "# Here you can adjust the learning rate\n",
    "# see https://keras.io/optimizers/\n",
    "optimizer = tf.keras.optimizers.Adam(lr=LEARN_RATE, decay=0.0025)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print(\"Model successfully created\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Training (from scratch)\n",
    "\n",
    "Das eigentliche Training wird über die Funktion `fit_generator` gestartet, welchem die zuvor erstellten Generatoren übergeben werden.\n",
    "\n",
    "Zusätzlich muss die Anzahl Epochen (in diesem Fall eine Epoche) sowie die Anzahl Schritte pro Epoche (weil aus einem Generator nicht die Anzahl der Trainings- und Validierungsbilder extrahiert werden kann) angegeben werden. Diese entspricht der Anzahl Trainingsbilder geteilt durch die Batch-Größe, in diesem Fall also `36800 / 16 = 2300`.\n",
    "\n",
    "Dies kann einige Zeit dauern. Ob der Kernel im Hintergrund noch aktiv ist lässt sich in dem Kreis oben rechts erkennen – solange der Kreis ausgefüllt ist, läuft das Script noch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Start training from scratch\")\n",
    "history = LossHistory()\n",
    "filepath = os.path.join(MODEL_DIR, \"from_scratch_{epoch:02d}-{val_loss:.2f}.hdf5\")\n",
    "model_checkpoint = ModelCheckpoint(filepath)\n",
    "model.fit_generator(\n",
    "        generator=train_generator,\n",
    "        steps_per_epoch=NUM_TRAINING_SAMPLES // BATCH_SIZE,\n",
    "        epochs=1,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=NUM_VALIDATION_SAMPLES // BATCH_SIZE,\n",
    "        callbacks=[history])\n",
    "models.save_model(model, filepath = model_file)\n",
    "#evaluation = model.evaluate_generator(train_generator, NUM_TRAINING_SAMPLES // BATCH_SIZE)\n",
    "#print(\"Training loss:\", evaluation[0], \"\\nTraining accuracy: {:.2f}%\".format(evaluation[1]*100))\n",
    "#evaluation = model.evaluate_generator(validation_generator, NUM_VALIDATION_SAMPLES // BATCH_SIZE)\n",
    "#print(\"Validation loss:\", evaluation[0], \"\\nValidation accuracy: {:.2f}%\".format(evaluation[1]*100))\n",
    "print(\"Finished training, model saved to: \" + MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Ausgabe des Trainingsscripts enthält folgende Informationen:\n",
    "\n",
    "- `8/2300`: Das aktuelle Minibatch. Wie oben beschrieben ergibt sich die Zahl 30 aus der Anzahl Trainingsbilder durch die Batch-Größe: `36800 / 16 = 2300`\n",
    "- `ETA: 1:55`: Erwartete Restzeit\n",
    "- `413s 180ms/step`: Gesamtdauer für die Trainingsepoche sowie durchschnittliche Dauer eines Minibatches\n",
    "- `loss: 1.3854`: Wert der zu minimierenden Verlustfunction (in diesem Fall [Cross Entropy](https://en.wikipedia.org/wiki/Cross_entropy)) auf dem letzten Minibatch\n",
    "- `acc: 0.4926`: Accuracy auf dem aktuellen Trainings-Minibatch, also prozentualer Anteil korrekt klassifizierter Bilder\n",
    "- `val_loss: 1.5387`: Wert der Verlustfunction auf den gesamten Validierungsdaten, wird nur einmal pro Epoche generiert\n",
    "- `val_acc: 0.5148`: Accuracy auf den gesamten Validierungsdaten, wird nur einmal pro Epoche generiert\n",
    "\n",
    "Wichtig ist hier ein Vergleich zwischen den Werten für die Trainings- und Validierungsdaten. Wenn diese Werte weit auseinander gehen dann liegt Overfitting vor, die Trainingsdaten werden also auswendig gelernt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='visualization'></a>\n",
    "## 3.2 Visualisierung (from scratch)\n",
    "\n",
    "Nach dem Training kann mit der `LossHistory`-Instanz eine Visualisierung der Klassifikationsergebnisse stattfinden.\n",
    "\n",
    "Es werden hier nur die Loss und Accuracy der Trainingsbatches angezeigt, da der Validierungsfehler nur einmal am Ende jeder Epoche (in diesem Fall also insgesmat nur ein mal) berechnet wird."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(*smooth(history.acc, 100))\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('step')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(*smooth(history.loss, 100))\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('step')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Einzelinferenz (Trainingsbilder, from scratch)\n",
    "\n",
    "Spannender hingegen ist die anschließende Einzelinferenz: was genau macht unser Netz aus bestimmten Bildern der Trainingsmenge?\n",
    "\n",
    "Es werden hier vier (Variable `IMAGES_TO_SHOW`) zufällige Bilder aus der Trainingsmenge genommen.\n",
    "\n",
    "Über die Variable `NUM_WRONG` kann festgelegt werden, wie viele falsch klassifizierte Bilder mindestens ausgewählt werden sollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "example_generator = ImageDataGenerator().flow_from_directory(DATASET_DIR + '/train', batch_size=1, target_size=(IMAGE_SIZE,IMAGE_SIZE), class_mode='categorical')\n",
    "\n",
    "IMAGES_TO_SHOW = 4\n",
    "NUM_WRONG = 2\n",
    "i = 0\n",
    "j = 0\n",
    "for x_batch, y_batch in example_generator:\n",
    "    x_input = x_batch[0]/255.\n",
    "    x_input = x_input[np.newaxis, ...]\n",
    "    result = model.predict(x_input)\n",
    "    \n",
    "    predicted_class = CLASSES[np.argmax(result[0], axis=-1)]\n",
    "    correct_class = CLASSES[np.where(y_batch[0] == 1.)[0][0]]\n",
    "    \n",
    "    if j >= IMAGES_TO_SHOW - NUM_WRONG and NUM_WRONG > 0 and predicted_class == correct_class:\n",
    "        continue\n",
    "        \n",
    "    i += 1\n",
    "    \n",
    "    if predicted_class == correct_class:\n",
    "        j += 1\n",
    "    \n",
    "    print(\"\\n\\nImage\", i)\n",
    "    plt.figure()\n",
    "    plt.title(CLASSES[np.where(y_batch[0] == 1.)[0][0]])\n",
    "    plt.imshow(x_batch[0].astype(int))\n",
    "    \n",
    "    \n",
    "    print_prediction_result(result)\n",
    "    print('Correct class:', CLASSES[np.where(y_batch[0] == 1.)[0][0]])\n",
    "    plt.show()\n",
    "    \n",
    "    if i >= IMAGES_TO_SHOW:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier wird das gleiche für Bilder aus dem Validierungsdatensatz durchgeführt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "example_generator = ImageDataGenerator().flow_from_directory(DATASET_DIR + '/valid', batch_size=1, target_size=(IMAGE_SIZE,IMAGE_SIZE), class_mode='categorical')\n",
    "\n",
    "IMAGES_TO_SHOW = 4\n",
    "NUM_WRONG = 0\n",
    "i = 0\n",
    "j = 0\n",
    "for x_batch, y_batch in example_generator:\n",
    "    x_input = x_batch[0]/255.\n",
    "    x_input = x_input[np.newaxis, ...]\n",
    "    result = model.predict(x_input)\n",
    "    \n",
    "    predicted_class = CLASSES[np.argmax(result[0], axis=-1)]\n",
    "    correct_class = CLASSES[np.where(y_batch[0] == 1.)[0][0]]\n",
    "    \n",
    "    if j >= IMAGES_TO_SHOW - NUM_WRONG:\n",
    "        continue\n",
    "    \n",
    "    if predicted_class == correct_class:\n",
    "        j += 1\n",
    "    \n",
    "    print(\"\\n\\nImage\", i+1)\n",
    "    plt.figure()\n",
    "    plt.title(CLASSES[np.where(y_batch[0] == 1.)[0][0]])\n",
    "    plt.imshow(x_batch[0].astype(int))\n",
    "    \n",
    "    \n",
    "    print_prediction_result(result)\n",
    "    print('Correct class:', CLASSES[np.where(y_batch[0] == 1.)[0][0]])\n",
    "    plt.show()\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "    if i > IMAGES_TO_SHOW:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='pre_trained'></a>\n",
    "# 4. Modell-Erstellung (pre-trained)\n",
    "\n",
    "Nachdem wir uns das Training from scratch angesehen haben soll nun ein Training basierend auf einem vortrainierten Modell stattfinden. Die in Keras mitgelieferten Modelle sind mit dem ImageNet-Datensatz vortrainiert. Um ein vortrainiertes Netz zu nutzen muss bei der Erstellung der `conv_base` der `weights` Parameter auf `imagenet` gesetzt werden.\n",
    "\n",
    "Wichtig ist hierbei `conv_base.trainable` zunächst auf `False` zu setzen, wodurch nur die neu hinzugefügten Schichten für eine Epoche trainierten werden.\n",
    "\n",
    "Falls der Validierungsfehler nach dem ersten Trainingsschritt sehr schlecht ist, dann ist das kein Grund zur Sorge – dieser wird spätestens nach dem zweiten Trainingsschritt dem Trainingsfehler sehr ähnlich sein.\n",
    "\n",
    "Falls ein `ResourceExhaustion` bzw. `OOM`-Error auftritt, muss der Kernel neustartet werden `\"restart the kernel (with dialog)\"`. Anschließend alle Zellen einschließlich der Datensatzerstellung (aber nicht das Training from scratch) ausführen und dann hier fortfahren. Klicken Sie dazu [hier](#from_scratch), dann oben auf `Cell -> Run all above`, und anschließend auf den Link zum Überspringen des Schrittes, um wieder hierhin zu gelangen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Creating pre-trained model, this may take a while...')\n",
    "\n",
    "# Clear session memory to avoid OOM\n",
    "K.clear_session()\n",
    "\n",
    "# Create Keras model\n",
    "# see https://keras.io/applications/\n",
    "conv_base = ResNet50(weights='imagenet',\n",
    "                  include_top=False,\n",
    "                  input_shape=(IMAGE_SIZE, IMAGE_SIZE, 3))\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(conv_base)\n",
    "model.add(layers.Flatten())\n",
    "# Add more layers here if needed\n",
    "model.add(layers.Dense(1000, activation='relu'))\n",
    "# The number of nodes in the last layer should be equal to the number of classes\n",
    "model.add(layers.Dense(NUM_CLASSES, activation='softmax'))\n",
    "\n",
    "print(\"Model created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Training neue Schichten (pre-trained)\n",
    "\n",
    "\n",
    "Aufgaben:\n",
    "\n",
    "- Frieren Sie die Gewichte der vortrainierten Schichten über den Parameter `conv_base.trainable` ein, indem Sie die Variable `CONV_BASE_TRAINABLE` durch `True` oder `False` ersetzen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disable training for the pre-trained model\n",
    "conv_base.trainable = CONV_BASE_TRAINABLE\n",
    "\n",
    "# Here you can adjust the learning rate\n",
    "# see https://keras.io/optimizers/\n",
    "optimizer = tf.keras.optimizers.Adam(lr=LEARN_RATE, decay=0.00025, clipnorm=1.)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "print('Training new model')\n",
    "\n",
    "history = LossHistory()\n",
    "filepath = os.path.join(MODEL_DIR, \"pre_trained_heads_{epoch:02d}-{val_loss:.2f}.hdf5\")\n",
    "model_checkpoint = ModelCheckpoint(filepath)\n",
    "\n",
    "print(\"Start training: only new layers\")\n",
    "# First train only top layers\n",
    "model.fit_generator(\n",
    "        train_generator,\n",
    "        steps_per_epoch=NUM_TRAINING_SAMPLES // BATCH_SIZE,\n",
    "        epochs=1,\n",
    "        callbacks=[history, model_checkpoint],\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=NUM_VALIDATION_SAMPLES // BATCH_SIZE)\n",
    "\n",
    "#models.save_model(model, filepath = model_file)\n",
    "#evaluation = model.evaluate_generator(train_generator, NUM_TRAINING_SAMPLES // BATCH_SIZE)\n",
    "#print(\"Training loss:\", evaluation[0], \"\\nTraining accuracy: {:.2f}%\".format(evaluation[1]*100))\n",
    "#evaluation = model.evaluate_generator(validation_generator, NUM_VALIDATION_SAMPLES // BATCH_SIZE)\n",
    "#print(\"Validation loss:\", evaluation[0], \"\\nValidation accuracy: {:.2f}%\".format(evaluation[1]*100))\n",
    "print(\"Finished training, model saved to: \" + MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='visualization'></a>\n",
    "## 4.2 Visualisierung nach Training neuer Schichten (pre-trained)\n",
    "\n",
    "Nach dem Training kann mit der `LossHistory`-Instanz eine Visualisierung der Klassifikationsergebnisse stattfinden.\n",
    "\n",
    "Diese Visualisierung zeigt jederzeit den Verlauf des zuletzt durchgeführten Trainings an, Sie können also nach jedem Training hierhin zurück kehren und diesen Code-Block erneut ausführen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(*smooth(history.acc, 100))\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('step')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(*smooth(history.loss, 100))\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('step')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2.1 Einzelinferenz nach Training neuer Schichten (pre-trained)\n",
    "\n",
    "Spannender hingegen ist die anschließende Einzelinferenz: was genau macht unser Netz aus bestimmten Bildern der Trainingsmenge?\n",
    "\n",
    "Es werden hier vier (Variable `IMAGES_TO_SHOW`) zufällige Bilder aus der Trainingsmenge genommen.\n",
    "\n",
    "Über die Variable `NUM_WRONG` kann festgelegt werden, wie viele falsch klassifizierte Bilder mindestens ausgewählt werden sollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "example_generator = ImageDataGenerator().flow_from_directory(DATASET_DIR + '/train', batch_size=1, target_size=(IMAGE_SIZE,IMAGE_SIZE), class_mode='categorical')\n",
    "\n",
    "IMAGES_TO_SHOW = 4\n",
    "NUM_WRONG = 0\n",
    "i = 0\n",
    "j = 0\n",
    "for x_batch, y_batch in example_generator:\n",
    "    x_input = x_batch[0]/255.\n",
    "    x_input = x_input[np.newaxis, ...]\n",
    "    result = model.predict(x_input)\n",
    "    \n",
    "    predicted_class = CLASSES[np.argmax(result[0], axis=-1)]\n",
    "    correct_class = CLASSES[np.where(y_batch[0] == 1.)[0][0]]\n",
    "    \n",
    "    if j >= IMAGES_TO_SHOW - NUM_WRONG and NUM_WRONG > 0 and predicted_class == correct_class:\n",
    "        continue\n",
    "    \n",
    "    if predicted_class == correct_class:\n",
    "        j += 1\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "    print(\"\\n\\nImage\", i)\n",
    "    plt.figure()\n",
    "    plt.title(CLASSES[np.where(y_batch[0] == 1.)[0][0]])\n",
    "    plt.imshow(x_batch[0].astype(int))\n",
    "    \n",
    "    \n",
    "    print_prediction_result(result)\n",
    "    print('Correct class:', CLASSES[np.where(y_batch[0] == 1.)[0][0]])\n",
    "    plt.show()\n",
    "    \n",
    "    if i >= IMAGES_TO_SHOW:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier wird das gleiche für Bilder aus dem Validierungsdatensatz durchgeführt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "example_generator = ImageDataGenerator().flow_from_directory(DATASET_DIR + '/valid', batch_size=1, target_size=(IMAGE_SIZE,IMAGE_SIZE), class_mode='categorical')\n",
    "\n",
    "IMAGES_TO_SHOW = 4\n",
    "NUM_WRONG = 0\n",
    "i = 0\n",
    "j = 0\n",
    "for x_batch, y_batch in example_generator:\n",
    "    x_input = x_batch[0]/255.\n",
    "    x_input = x_input[np.newaxis, ...]\n",
    "    result = model.predict(x_input)\n",
    "    \n",
    "    predicted_class = CLASSES[np.argmax(result[0], axis=-1)]\n",
    "    correct_class = CLASSES[np.where(y_batch[0] == 1.)[0][0]]\n",
    "    \n",
    "    if j >= IMAGES_TO_SHOW - NUM_WRONG:\n",
    "        continue\n",
    "    \n",
    "    if predicted_class == correct_class:\n",
    "        j += 1\n",
    "    \n",
    "    print(\"\\n\\nImage\", i+1)\n",
    "    plt.figure()\n",
    "    plt.title(CLASSES[np.where(y_batch[0] == 1.)[0][0]])\n",
    "    plt.imshow(x_batch[0].astype(int))\n",
    "    \n",
    "    \n",
    "    print_prediction_result(result)\n",
    "    print('Correct class:', CLASSES[np.where(y_batch[0] == 1.)[0][0]])\n",
    "    plt.show()\n",
    "    \n",
    "    i += 1\n",
    "    \n",
    "    if i > IMAGES_TO_SHOW:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Fine-Tuning (pre-trained)\n",
    "\n",
    "\n",
    "Im zweiten Schritt wird nun das gesamte Modell für eine Epoche trainiert (`conv_base.trainable` wird auf `True` gesetzt). Als Optimizer wird nun SGD verwendet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.trainable = True\n",
    "\n",
    "try:\n",
    "    model.load_weights(find_last(MODEL_DIR))\n",
    "    print(\"Loading weights from {}\".format(MODEL_DIR))\n",
    "except FileNotFoundError:\n",
    "    pass\n",
    "\n",
    "optimizer = tf.keras.optimizers.SGD(lr=LEARN_RATE, decay=0.000025)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "try: history\n",
    "except NameError: history = LossHistory()\n",
    "\n",
    "filepath = os.path.join(MODEL_DIR, \"pre_trained_full_{epoch:02d}-{val_loss:.2f}.hdf5\")\n",
    "model_checkpoint = ModelCheckpoint(filepath)\n",
    "\n",
    "print(\"Start training from pre-trained: fine-tuning all layers\")\n",
    "model.fit_generator(\n",
    "        generator=train_generator,\n",
    "        steps_per_epoch=NUM_TRAINING_SAMPLES // BATCH_SIZE,\n",
    "        epochs=1,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=NUM_VALIDATION_SAMPLES // BATCH_SIZE,\n",
    "        callbacks=[history, model_checkpoint])\n",
    "#evaluation = model.evaluate_generator(train_generator, NUM_TRAINING_SAMPLES // BATCH_SIZE)\n",
    "#print(\"Training loss:\", evaluation[0], \"\\nTraining accuracy: {:.2f}%\".format(evaluation[1]*100))\n",
    "#evaluation = model.evaluate_generator(validation_generator, NUM_VALIDATION_SAMPLES // BATCH_SIZE)\n",
    "#print(\"Validation loss:\", evaluation[0], \"\\nValidation accuracy: {:.2f}%\".format(evaluation[1]*100))\n",
    "print(\"Finished training, model saved to: \" + MODEL_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Visualisierung Fine-Tuning (pre-trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for accuracy\n",
    "plt.plot(*smooth(history.acc, 100))\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('step')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(*smooth(history.loss, 100))\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('step')\n",
    "plt.legend(['train'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Führen Sie nun die finale Einzelinferenz auf den Trainings-/Validierungsdaten selbst durch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight Averaging\n",
    "\n",
    "[Polyak averaging](https://www.researchgate.net/publication/236736759_New_stochastic_approximation_type_procedures) kann auf neuronale Netze angewendet werden, indem ein Durchschnitt der Gewichte verschiedener Modelle gebildet wird. Dies sorgt oft für stabiliere und teilweise sogar bessere Ergebnisse.\n",
    "\n",
    "Code angepasst von: https://machinelearningmastery.com/polyak-neural-network-model-weight-ensemble/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from stat import S_ISREG, ST_MODE, ST_MTIME\n",
    "\n",
    "# Clear session memory to avoid OOM\n",
    "K.clear_session()\n",
    "\n",
    "def get_model_filenames(directory, n_max, prefix):\n",
    "    print(\"Loading a maximum of {} models with filename prefix {} from directory {}\"\n",
    "          .format(n_max, prefix, directory))\n",
    "    \n",
    "    # get all entries in the directory w/ stats\n",
    "    entries = (os.path.join(directory, fn) for fn in os.listdir(directory))\n",
    "    entries = ((os.stat(path), path) for path in entries)\n",
    "\n",
    "    # leave only regular files, insert modification date\n",
    "    entries = ((stat[ST_MTIME], path)\n",
    "               for stat, path in entries if S_ISREG(stat[ST_MODE]))\n",
    "\n",
    "    results = []\n",
    "    i = 0\n",
    "    for cdate, path in sorted(entries, reverse = True):\n",
    "        results.append(os.path.join(directory, path))\n",
    "        i += 1\n",
    "        if i >= n_max:\n",
    "            break\n",
    "    \n",
    "    #results.reverse()\n",
    "    \n",
    "    print(\"Found {} models: {}\".format(len(results), results))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# load models from file\n",
    "def load_models_from_directory(directory, n_max, prefix):\n",
    "    # gather model names from directory\n",
    "    all_models = list()\n",
    "    for filename in get_model_filenames(directory, n_max, prefix):\n",
    "        print('Loading %s...' % filename)\n",
    "        # load model from file\n",
    "        model = models.load_model(filename)\n",
    "        # add to list of members\n",
    "        all_models.append(model)\n",
    "        print('Done loading %s' % filename)\n",
    "    return all_models\n",
    "\n",
    "\n",
    "# create a model from the weights of multiple models\n",
    "def model_weight_ensemble(members, weights):\n",
    "    print(\"Creating averaged model, this can take a while...\")\n",
    "    # determine how many layers need to be averaged\n",
    "    n_layers = len(members[0].get_weights())\n",
    "    # create an set of average model weights\n",
    "    avg_model_weights = list()\n",
    "    for layer in range(n_layers):\n",
    "        print(\"Processing layer {}\".format(layer))\n",
    "        # collect this layer from each model\n",
    "        layer_weights = np.array([model.get_weights()[layer] for model in members])\n",
    "        # weighted average of weights for this layer\n",
    "        avg_layer_weights = np.average(layer_weights, axis=0, weights=weights)\n",
    "        # store average layer weights\n",
    "        avg_model_weights.append(avg_layer_weights)\n",
    "    # create a new model with the same structure\n",
    "    model = models.clone_model(members[0])\n",
    "    # set the weights in the new\n",
    "    model.set_weights(avg_model_weights)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(\"Finished creating averaged model\")\n",
    "    return model\n",
    "\n",
    "PREFIX = \"pre_trained\"\n",
    "NUM_MODELS = 4\n",
    "# weight decay factor\n",
    "ALPHA = 2.0\n",
    "\n",
    "members = load_models_from_directory(MODEL_DIR, NUM_MODELS, PREFIX)\n",
    "#members = [models.load_model('/workspace/models/classification/pre_trained_full_12-0.14.hdf5')]\n",
    "\n",
    "\n",
    "# uncomment for exponential weight decay\n",
    "#weights = [exp(-i/ALPHA) for i in range(1, len(members)+1)]\n",
    "# uncommend for linear decay\n",
    "#weights = [i/n_members for i in range(len(members), 0, -1)]\n",
    "# uncomment for no weight decay\n",
    "weights = [1/len(members) for i in range(1, len(members)+1)]\n",
    "model = model_weight_ensemble(members, weights)\n",
    "\n",
    "print(\"Starting evaluation:\")\n",
    "#evaluation = model.evaluate_generator(train_generator, NUM_TRAINING_SAMPLES // BATCH_SIZE)\n",
    "#print(\"Training loss:\", evaluation[0], \"\\nTraining accuracy: {:.2f}%\".format(evaluation[1]*100))\n",
    "evaluation = model.evaluate_generator(validation_generator, NUM_VALIDATION_SAMPLES // BATCH_SIZE)\n",
    "print(\"Validation loss:\", evaluation[0], \"\\nValidation accuracy: {:.2f}%\".format(evaluation[1]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Out of Class Bilder\n",
    "\n",
    "Zum Test wie unser Modell mit Bildern umgeht die es noch nie gesehen hat, geben wir ihm nun das folgende Schädel-CT-Bild:\n",
    "\n",
    "<img src=\"https://prod-images.static.radiopaedia.org/images/17058746/a60ee77e6b44da2d72a25cea08dfd2_big_gallery.jpeg\" />\n",
    "Quelle: radiopaedia.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_generator = ImageDataGenerator().flow_from_directory(DATA_DIR + '/ooc', batch_size=1, target_size=(128,128), class_mode='categorical')\n",
    "\n",
    "for x_batch, y_batch in example_generator:\n",
    "    x_input = x_batch[0]/255.\n",
    "    x_input = x_input[np.newaxis, ...]\n",
    "    result = model.predict(x_input)\n",
    "    \n",
    "    predicted_class = CLASSES[np.argmax(result[0], axis=-1)]\n",
    "    correct_class = CLASSES[np.where(y_batch[0] == 1.)[0][0]]\n",
    "    \n",
    "    print(\"\\n\\nImage\", i+1)\n",
    "    plt.figure()\n",
    "    plt.imshow(x_batch[0].astype(int))\n",
    "    \n",
    "    \n",
    "    print_prediction_result(result)\n",
    "    plt.show()\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "\n",
    "## Run 1\n",
    "\n",
    "Config:\n",
    "\n",
    "```python\n",
    "# Data augmentation\n",
    "# see https://keras.io/preprocessing/image/\n",
    "# see https://blog.keras.io/building-powerful-image-classification-models-using-very-little-data.html\n",
    "train_datagen = ImageDataGenerator(\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        rescale=1./255,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        #interpolation_order=2\n",
    "        fill_mode='nearest')\n",
    "\n",
    "# only rescale validation data\n",
    "validation_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "# this is a generator that will read pictures found in subfolers of '/train',\n",
    "# in the dataset directory and indefinitely generate batches of augmented image\n",
    "# data\n",
    "print(\"Training data:\")\n",
    "train_generator_raw = train_datagen.flow_from_directory(\n",
    "        DATASET_DIR + '/train',  # this is the target directory\n",
    "        target_size=(150, 150),  # all images will be resized to 128x128\n",
    "        batch_size=BATCH_SIZE,\n",
    "        #interpolation=\"lanczos\",\n",
    "        class_mode='categorical')\n",
    "# […]\n",
    "LEARN_RATE = 0.001\n",
    "# one epoch training of the heads:\n",
    "optimizer = tf.keras.optimizers.Adam(lr=LEARN_RATE, decay=0.0025, clipnorm=1.)\n",
    "# 100 epochs training full network\n",
    "optimizer = tf.keras.optimizers.SGD(lr=LEARN_RATE, decay=0.00025, clipnorm=1.)\n",
    "```\n",
    "\n",
    "Weight averaging mit 5 Modellen:\n",
    "\n",
    "Validation loss: ~0.26  \n",
    "Validation accuracy: ~94%\n",
    "\n",
    "## Run 2\n",
    "\n",
    "Gleiche Konfiguration wie Run 1 außer:\n",
    "\n",
    "- ```interpolation=\"lanczos\"```\n",
    "\n",
    "Weight averaging mit 5 Modellen:\n",
    "\n",
    "Validation loss: 0.2383039555444801  \n",
    "Validation accuracy: 95.26%\n",
    "\n",
    "## Run 3\n",
    "\n",
    "Gleiche Konfiguration wie Run 2 außer:\n",
    "\n",
    "- `Adam` Optimizer für Fine-Tuning anstelle von `SGD`\n",
    "\n",
    "Weight averaging mit 5 Modellen:\n",
    "\n",
    "Validation loss: 0.183909532866773  \n",
    "Validation accuracy: 94.50%\n",
    "\n",
    "## Run 4\n",
    "\n",
    "Gleiche Konfiguration wie Run 2 außer:\n",
    "\n",
    "- Bildgröße auf 400x400 erhöht\n",
    "- Batch-Size halbiert (8)\n",
    "- 80 Epochen training\n",
    "- `clipnorm=1.` beim Optimizer nach 40 Epochen entfernt\n",
    "\n",
    "Weight averaging mit 5 Modellen (nach 80 Epochen Training):\n",
    "\n",
    "Validation loss: 0.15276804335761035  \n",
    "Validation accuracy: 96.99%\n",
    "\n",
    "Single best model:\n",
    "\n",
    "Validation loss: 0.1423978957033721  \n",
    "Validation accuracy: 97.06%\n",
    "\n",
    "\n",
    "# TODO\n",
    "\n",
    "- Kreuzvalidierung\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(MODEL_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls /workspace/models/classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
